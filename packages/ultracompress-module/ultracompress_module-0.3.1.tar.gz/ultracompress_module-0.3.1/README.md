# UltraCompress Module

Compression JSON multi-mode (fast / aggressive / ultra) avec cha√Ænes de transformations r√©versibles (maps, delta, MTF, RLE, internage + BPE, front-coding, packing / bit-packing d'entiers, zigzag, GCD scaling, affine / d√©cimal normalisation, segmentation num√©rique, d√©duplication de sous-arbres, colonnes am√©lior√©es...) et s√©lection automatique du meilleur backend (multi-niveaux Zstandard, Brotli, LZMA) pour obtenir des r√©ductions >95% (souvent >99%) sur des structures r√©p√©titives complexes ‚Äî tout en garantissant un round‚Äëtrip exact et l'ordre original.

## Installation

```bash
pip install ultracompress_module
````

## Modes

| Mode | Objectif | Vitesse | Recherche backend | Transformations explor√©es |
|------|----------|---------|-------------------|---------------------------|
| `fast` (d√©faut) | Bonne compression rapide | ‚ö° | zstd (niveau unique ou choisi), fallback lzma/brotli si meilleur | Sous-ensemble s√ªr (maps, delta, mtf, rle, int pack basique) |
| `aggressive` | Ratio plus √©lev√© | ‚ö°‚ûúüê¢ | zstd multi-niveaux + brotli (9/11) + lzma extr√™me | Ajoute exploration large int packing, colonnes simples, internage cha√Æne |
| `ultra` | Maximum de r√©duction | üê¢ | Large panel zstd (niveaux √©tendus + dict si dispo) + brotli + lzma | Active recherche combinatoire (intern+BPE, front-coding, subtree dedup robuste, colonnes am√©lior√©es, bit/zigzag pack, GCD/affine/decimal/segbit cascade, variantes pipelines >100) |

Tous les pipelines sont strictement **lossless** et l'ordre des √©l√©ments est pr√©serv√©. Le moteur √©value plusieurs variantes et conserve le plus petit r√©sultat final.

## Utilisation basique (fichiers)

```python
from ultracompress_module import compress, decompress

# Compresser un fichier JSON (mode rapide: compresse le texte brut)
compress("input.json", "output.uc", preserve_text=True)

# Mode structur√© (transformations maps/delta/mtf/rle pour meilleur ratio)
compress("input.json", "output_struct.uc", preserve_text=False, preproc_opts={
	'maps': True, 'delta': True, 'mtf': True, 'rle': True
})

"""Fast (d√©faut) vs aggressive vs ultra"""
# Mode agressif (plus de recherche backend + transformations)
compress("input.json", "output_aggr.uc", preserve_text=False, preproc_opts={
    'maps': True, 'delta': True, 'mtf': True, 'rle': True
}, mode='aggressive')

# Mode ultra (exploration maximale, meilleur ratio, plus lent sur gros fichiers)
compress("input.json", "output_ultra.uc", preserve_text=False, preproc_opts={
    'maps': True, 'delta': True, 'mtf': True, 'rle': True
}, mode='ultra')

# D√©compresser
decompress("output.uc", "reconstruit.json", preserve_text=True)
```

## Utilisation en m√©moire (aucun fichier interm√©diaire)

```python
from ultracompress_module import compress_text, decompress_bytes

json_text = '{"hello": "world", "nums": [1,1,1,2,2,3,3,3,3], "nested": {"a":1,"b":1}}'

# Texte brut (fast) -> id√©al si on veut exactement la cha√Æne identique (espaces, ordre etc.)
blob_fast = compress_text(json_text, preserve_text=True)  # bytes .uc
restored_text = decompress_bytes(blob_fast)  # str identique

# Structur√© agressif (objet Python √† la restauration) :
blob_aggr = compress_text(json_text, preserve_text=False, preproc_opts={'maps': True}, mode='aggressive')
obj = decompress_bytes(blob_aggr, preserve_text=False)  # objet Python identique

# Structur√© ultra (max ratio) :
blob_ultra = compress_text(json_text, preserve_text=False, preproc_opts={'maps': True}, mode='ultra')
obj_ultra = decompress_bytes(blob_ultra, preserve_text=False)
```

## CLI

Apr√®s installation, une commande `ultracompress` est disponible :

```bash
ultracompress compress input.json output.uc  # fast par d√©faut
ultracompress compress --mode aggressive input.json output_aggr.uc
ultracompress compress --mode ultra input.json output_ultra.uc

ultracompress decompress output_ultra.uc restored.json --mode ultra  # (mode requis pour hints quand structur√©)

# Compression texte brut (pr√©server format exact)
ultracompress compress --mode fast --preserve-text input.json out_text.uc

# Optimisation automatique (essaie plusieurs entr√©es / pipelines internes)
ultracompress optimize input.json best.uc
```

Sous-commandes additionnelles :

- `build-maps` : g√©n√®re `maps_keys.json` / `maps_values.json` depuis un corpus.
- `train-dict` : entra√Æne un dictionnaire Zstandard (plac√© dans `zstd_dict`).
- `optimize` : essaie plusieurs strat√©gies de pipeline compl√®tes.
- `fast-optimize` : sous-ensemble plus rapide.
- `inspect` : affiche un aper√ßu (taille, hex, base64, tentative de d√©codage structur√©/texte) d'un fichier `.uc`.

Exemple :

```bash
ultracompress inspect sample.uc --max-bytes 96
```

Afficher l'aide :

```bash
ultracompress --help
```

## Bench (exemple indicatif)

Sur un jeu mixte (structures imbriqu√©es, listes d'entiers, cha√Ænes r√©p√©titives) :

| Dataset | Taille JSON | fast | aggressive | ultra | Gain ultra |
|---------|------------:|-----:|-----------:|------:|-----------:|
| nested_medium | 120 KB | 18.2 KB | 9.7 KB | 5.3 KB | 95.6% |
| numeric_arrays | 200 KB | 21.5 KB | 8.4 KB | 3.1 KB | 98.4% |
| repetitive_strings | 85 KB | 9.1 KB | 4.0 KB | 0.6 KB | 99.3% |

(Les chiffres fluctuent selon contenu / machine; lancer vos propres mesures via un script bench.)

## Exemples complets

Voir `examples/example_usage.py` pour un script autonome qui montre :

- Compression en m√©moire (texte vs structur√©) sur les trois modes.
- Compression de fichiers (fast/aggressive/ultra) et restauration.
- Utilisation de `benchmark_ratio` pour comparer rapidement les tailles.

Ex√©cution :

```bash
python examples/example_usage.py
```

### API auto-d√©tection rapide

Pour simplifier selon le type pass√© (texte JSON, objet Python ou chemin de fichier) :

```python
from ultracompress_module import auto_compress, auto_decompress

data = {"users": [{"id": i, "name": f"u{i}"} for i in range(10)]}
meta = auto_compress(data, mode='aggressive')  # retourne AutoCompressed
print(meta.original_len, '->', meta.compressed_len)
restored = auto_decompress(meta.blob, return_text=False)
assert restored == data

json_text = '{"a":1,"b":[1,2,3]}'
meta_text = auto_compress(json_text, mode='fast')
restored_txt = auto_decompress(meta_text.blob, return_text=True)
assert restored_txt == json_text
```

## Garantie d'int√©grit√©

Chaque transformation est enti√®rement r√©versible. Une suite de tests valide le round‚Äëtrip (fast/aggressive/ultra, texte & structur√©). Si une variante ne r√©duit pas la taille ou √©choue √† valider sa reconstruction interm√©diaire, elle est √©cart√©e.

## Limites / Conseils

- Pour de tr√®s petits fichiers (<200 octets) le surco√ªt d'ent√™te peut r√©duire l'int√©r√™t : rester en `fast`.
- `ultra` peut explorer >100 pipelines; sur de gros JSON (multi-MB) pr√©voir plus de temps CPU.
- Fournir un dictionnaire Zstandard entra√Æn√© (`train-dict`) peut encore am√©liorer les ratios.
- `preserve_text=True` d√©sactive les optimisations structurelles (on compresse la cha√Æne brute apr√®s √©ventuel mapping minimal) afin de garantir l'exactitude byte‚Äë√†‚Äëbyte.

## Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.

---
Notes version 0.3.0:

- Nouveau mode `ultra` : exploration combinatoire √©tendue (internage + BPE, front-coding, subtree dedup robuste, colonnes avanc√©es, cascades num√©riques GCD/affine/decimal/segbit, bit/zigzag pack, multi-niveaux zstd √©largis) ‚Üí gains >95% fr√©quents.
- S√©lection dynamique du meilleur pipeline parmi >100 variantes lorsque pertinent.
- Ajout des transforms num√©riques avanc√©es (mise √† l'√©chelle GCD, d√©tection affine, normalisation d√©cimale, bit-packing segment√©) et am√©lioration de la r√©versibilit√©.
- CLI : option `--mode {fast,aggressive,ultra}` pour `compress` & `decompress`.
- Documentation enrichie (table de modes, benchs, conseils).

Notes version 0.2.0:

- Ajout du mode `aggressive` : essaie plusieurs niveaux Zstandard (3,6,9,15,19), Brotli (9/11), LZMA extr√™me et choisit le meilleur ratio.
- Transformation optionnelle d'emballage d'entiers (`int packing`) pour grandes listes d'entiers (>=8) non n√©gatifs.
- D√©compression des conteneurs structur√©s : auto-d√©tection zstd -> lzma -> brotli (si install√©) sans champ de m√©thode stock√© (compat r√©tro).
- API fonctionnelle en m√©moire stable (`compress_text`, `compress_object`, etc.).
