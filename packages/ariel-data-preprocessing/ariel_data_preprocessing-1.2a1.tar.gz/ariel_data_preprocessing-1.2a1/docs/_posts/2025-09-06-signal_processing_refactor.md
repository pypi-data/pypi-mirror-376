---
layout: post
title: "From Notebook to Package: Refactoring and Deploying the Signal Correction Pipeline"
---

Yesterday's challenge was getting the signal correction pipeline to work. Today's challenge? Making it production-ready. Time to refactor the preprocessing code into a proper Python package, add comprehensive testing, and set up automated CI/CD for deployment to PyPI.

TLDR: here is the [PyPi package](https://pypi.org/project/ariel-data-preprocessing)

## The Refactoring Challenge

The original signal correction pipeline worked great in a Jupyter notebook, but notebook code doesn't scale well. Here's what needed to happen:

1. **Extract the logic** from notebook cells into a clean, reusable class
2. **Add proper documentation** with docstrings for every method
3. **Implement comprehensive unit tests** to catch bugs and regressions
4. **Set up CI/CD workflows** for automated testing and deployment
5. **Package for PyPI** so anyone can install with `pip install ariel-data-challenge`

## The SignalCorrection Class

The refactored `SignalCorrection` class in `ariel_data_preprocessing/signal_correction.py` implements the complete 6-step pipeline:

```python
class SignalCorrection:
    '''
    Class to handle signal correction for Ariel Data Challenge.
    
    Implements the complete 6-step preprocessing pipeline:
    1. Analog-to-Digital Conversion (ADC)
    2. Hot/dead pixel masking
    3. Linearity correction
    4. Dark current subtraction
    5. Correlated Double Sampling (CDS)
    6. Flat field correction
    '''
```

Each step is now a private method with clear documentation:
- `_ADC_convert()` - Applies gain and offset corrections
- `_mask_hot_dead()` - Uses sigma clipping to identify hot pixels and masks dead pixels
- `_apply_linear_corr()` - Applies polynomial corrections pixel by pixel
- `_clean_dark()` - Subtracts scaled dark current
- `_get_cds()` - Performs correlated double sampling
- `_correct_flat_field()` - Normalizes pixel sensitivity

The class is configurable with ADC parameters, CPU count for parallel processing, and input/output data paths.

## Comprehensive Unit Testing

Testing a signal processing pipeline requires careful validation of each step. The test suite in `tests/test_preprocessing.py` covers:

- **Shape preservation** - Ensuring array dimensions are maintained through each step
- **Data type handling** - Verifying float64 conversion and masked array creation
- **CDS frame reduction** - Confirming the frame count is halved correctly
- **Integration with real data** - Using actual calibration files and signal data

Each test uses a subset of real Ariel data to ensure the corrections work with actual telescope outputs, not just synthetic test cases.

## Automated CI/CD Pipeline

Three GitHub workflows handle different aspects of the development pipeline:

### 1. Unit Testing (`unittest.yml`)
Triggered on every pull request to main:
- Sets up Python 3.8 environment
- Installs dependencies
- Runs the complete test suite
- Prevents merging if any tests fail

### 2. Test PyPI Release (`test_pypi_release.yml`)
Triggered when pushing tags to the dev branch:
- Builds the package distribution
- Runs unit tests to ensure quality
- Publishes to Test PyPI for validation
- Allows testing the installation process before production release

### 3. Production PyPI Release (`pypi_release.yml`)
Triggered when creating a GitHub release:
- Builds the final distribution
- Runs comprehensive tests
- Publishes to the main PyPI repository
- Makes the package publicly available via `pip install`

## The Benefits

This refactoring effort pays dividends in multiple ways:

### **Reproducibility**
Anyone can now install and use the exact same preprocessing pipeline:
```bash
pip install ariel-data-preprocessing
```

### **Reliability** 
Automated testing catches bugs before they reach production. Every code change is validated against real data.

### **Maintainability**
Clean class structure with documented methods makes the code much easier to understand and modify.

### **Collaboration**
Other researchers can easily build on this work, contribute improvements, or adapt the pipeline for their own projects.

### **Reproducibility**
The Ariel Data Challenge isn't just about building a working solution - it's about creating tools that the broader astronomical community can use and improve.

With the preprocessing pipeline now available as a proper Python package, complete with automated testing and continuous deployment, the foundation is solid for the next phase: building machine learning models to extract exoplanet atmospheric spectra.

## Next Steps

With the infrastructure in place, the focus shifts back to science:

1. **Integrate the package** into the main analysis workflow
2. **Optimize performance** for batch processing of multiple planets
3. **Build the spectral extraction pipeline** using the cleaned data
4. **Develop machine learning models** for atmospheric parameter estimation

The engineering detour is complete - time to get back to hunting for exoplanet atmospheres!
