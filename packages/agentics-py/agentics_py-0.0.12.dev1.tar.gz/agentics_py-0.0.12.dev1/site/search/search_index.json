{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udc4b Welcome to Agentics","text":"<p>Agentics is a lightweight, Python-native framework for building structured, agentic workflows over tabular or JSON-based data using Pydantic types and transduction logic. Designed to work seamlessly with large language models (LLMs), Agentics enables users to define input and output schemas as structured types and apply declarative, composable transformations\u2014called transductions\u2014across data collections. It supports asynchronous execution, built-in memory for structured retrieval-augmented generation (RAG), and self-transduction for tasks like data imputation and few-shot learning. With no-code and low-code interfaces, Agentics is ideal for rapidly prototyping intelligent systems that require structured reasoning, flexible memory access, and interpretable outputs.</p>"},{"location":"#documentation-overview","title":"\ud83d\udcda Documentation Overview","text":"<p>This documentation introduces the core concepts behind Agentics and provides everything you need to start building structured, agentic workflows using its data model and transduction framework.</p> <p>\u2e3b</p> <p>\ud83d\udc49 Getting Started: Learn how to install Agentics, set up your environment, and run your first logical transduction.</p> <p>\ud83d\udcd8 Why Agentics?: Understand the foundational principles and architecture of the Agentics framework.</p> <p>\ud83d\ude80 Use Cases: Explore real-world scenarios where Agentics enhances data intelligence and reasoning capabilities.</p> <p>\ud83e\udde0  Agentics: See how Agentics wraps pydantic models into transduction-ready agents for structured execution.</p> <p>\ud83d\udd01 Transduction: Discover how the &lt;&lt; operator enables logical transduction between types and how to customize its behavior.</p>"},{"location":"agentics/","title":"Agentics","text":"<p>Agentics objects are wrappers around list of objects having the same Pydantic Type. They are designed to enable async logical transduction among their instances. Agentics enable us to think about AI workflows in terms of structured data transformations rather than agent behaviours, knowledge and tasks. </p>"},{"location":"agentics/#the-agentics-class","title":"The Agentics class","text":"<p>Agentics is a Python class that wraps a list of Pydantic objects and enables structured, type-driven logical transduction between them.</p> <p>Internally, Agentics is implemented as a Pydantic model. It holds:     \u2022   atype: a reference to the Pydantic class shared by all objects in the list.     \u2022   states: a list of Pydantic instances, each validated to be of type atype.     \u2022   tools: a list of tools (CrewAI or Langchain) to be used for transduction</p> <pre><code>from typing import Type, List\nfrom pydantic import BaseModel, Field\n\nclass Agentics(BaseModel):\n    atype: Type[BaseModel] = Field(\n        ..., \n        description=\"The shared Pydantic type for all elements in the list.\"\n    )\n    states: List[BaseModel] = []\n    tools: Optional[List[Any]] = Field(\n        None,\n        description=\"List of tools to be used by the agent\"\n    )\n    ...\n</code></pre>"},{"location":"agentics/#atypes","title":"Atypes","text":"<p>Agentics types are dynamic as they can be modified at run time while ensuring coherent semantics of the data they represent. To this aim, their Pydantic type is represented by the aslot, that can be assigned and modified at runtime. </p> <p>Any subclass of BaseModel (i.e. any possible Pydantic Type) can be used as an atype as long as it is serializable.</p> <pre><code>from agentics.core.agentics import Agentics as AG\nfrom pydantic import BaseModel\n\nclass Movie(BaseModel):\n    title: str\n    genre: str\n    description:str\n\nmovies = AG(atype=Movie)\nmovies.states.append(Movie(title=\"Superman\"))\nprint(movies)\n</code></pre>"},{"location":"agentics/#importing-csv-and-jsonl","title":"Importing CSV and JSONL","text":"<p>Agentics states can be initialized loaded and saved to .csv or .jsonl files. AType will be automatically generated from the column names (after they will be normalized as needed for valid pydantic fields), with all attributes set to strings.</p> <pre><code>from pydantic import BaseModel\nfrom agentics.core.agentics import Agentics as AG\n\n\n# Load CSV automatically acquiring atype\norders = AG.from_csv(\"data/orders.csv\")\n\n# Note that atype contains only strings.\nprint(orders.atype)\norders.to_csv(\"data/orders_copy.csv\")\n\n# Load Jsonl automatically acquiring atype. \norders = AG.from_jsonl(\"data/orders.jsonl\")\n\n# Note that atype contains integers fields not only strings.\nprint(orders.atype)\norders.to_jsonl(\"data/orders_copy.jsonl\")\n</code></pre> <p>If atype is provided, the file must contain fields that match the attributes defined in atype for them to be acquired, otherwise they'll be set to null. Providing explicit atype is recommedend to have more control on the types of the attributes, which will be otherwise set to string, and consistency on the column names and attribute matching. In addition, it is a convenient way to narrow down the number of attributes required for the task.</p> <pre><code># Load from CSV providing custom type (Only matching column names will be inferred)\norders = AG.from_csv(\"data/orders.csv\", atype = Order)\nprint(orders.atype)\n\n#note that states contains only the attribites in atype, others have been filtered out\nprint(orders.pretty_print())\nAG.to_csv(\"data/orders_filtered.jsonl\")\n</code></pre>"},{"location":"agentics/#rebind","title":"Rebind","text":"<p>Agentic types are mutable, and can be modified dynamically, by assigning a new atype</p> <pre><code>movies = AG.from_csv(\"data/orders.csv\")\nprint(movies.atype)\n\nclass MovieReview(Movie):\n    review:str\n    quality_score:Optional[int] = Field(None,description=\"The quality of the movies in a scale 0 to 10\")\n\nmovies.rebind_atype(MovieReview)\nprint(movies.states[0])\n</code></pre> <p>You can also modify and rebind an exiting Agentic. Similarly can also remove attributes. The following code is equivalent to the code before</p> <pre><code>movies = AG.from_csv(\"data/orders.csv\")\nmovies.add_attribute(\"review\",str)\nmovies.add_attribute(\"quality_score\",int,description=\"The quality of the movies in a scale 0 to 10\")\nprint(movies[0])\nmovies.subset_atype(\"title\",\"genre\",\"description\")\nprint(movies[0]) ## note that movies[0] is equivalent to \n</code></pre>"},{"location":"agentics/#reference-code","title":"Reference code","text":"<p>explore this example</p>"},{"location":"agentics/#see-next-transduction","title":"See Next: Transduction","text":"<p>Wrapping pydantic types into Agentics provides them with the ability to perform transduction, as described in the next section</p>"},{"location":"background/","title":"Background: Redefining Agentic AI with Semantically Rich Data","text":"<p>\ud83d\udd0d What Is Agentics?</p> <p>Agentics is a novel framework that redefines how we think about agentic AI. Rather than treating data as static input manipulated by intelligent agents, Agentics inverts this paradigm: the intelligence lives in the data itself. By leveraging the power of GenAI and Pydantic types, Agentics transforms data into semantically rich, self-describing objects. This enables elegant, compact, and algebraically principled workflows.</p> <p>At the core of Agentics lies the concept of logical transduction\u2014the idea that large language models (LLMs) are best understood as transductors, transforming one structured semantic object into another. Agentics turns this insight into a practical system for building declarative, composable, and intelligent workflows.</p>"},{"location":"background/#design-philosophy","title":"\ud83e\udde0 Design Philosophy","text":"<p>The central design principle of Agentics is to abstract away agent behavior modeling from the developer. Instead of defining agents, goals, and prompts explicitly, developers simply enrich their data structures with semantics via Pydantic models.</p> <p>In other words:     \u2022   You don\u2019t program the agent.     \u2022   You describe the data.     \u2022   The semantics guide the AI.</p> <p>Agentics leverages these semantic hints (like field names and descriptions) to guide the internal agent behavior implicitly and transparently.</p> <p>The foundational building block of the framework is the Agentics class. This class wraps:     \u2022   A Pydantic type (called atype)     \u2022   A list of instances of that type (called states)     \u2022   And optionally, AI features like LLMs, memory, tools, and instructions</p> <p>This dual nature allows Agentics to behave both like a Python list of Pydantic objects and like an intelligent, semantically-aware agent system.</p> <p>The core operator in Agentics is the logical transduction operator: &lt;&lt;. This allows you to transform one Agentics object into another:</p> <p>Demographics &lt;&lt; Resumes</p> <p>Here, each Demographics object is constructed by interpreting the semantic information from corresponding Resumes, using LLMs (and optionally memory, and tools in more advanced cases) but without requiring any manual prompting or parsing logic.</p> <p>Agentics orchestrates this automatically by creating task agents for each transformation, leveraging the semantic field descriptions of the target Pydantic model to guide extraction.</p> <p>\ud83d\udca1 How It Works     1.  Define source and target types using Pydantic, with meaningful descriptions for each field.     2.  Wrap your data using the Agentics class.     3.  Apply the transduction: the system fills in target attributes using source semantics.     4.  Use the result as standard Python objects; no JSON parsing, no unstructured strings.</p> <p>This enables use cases like:     \u2022   Extracting structured data from text     \u2022   Mapping between heterogeneous schemas     \u2022   Building semantically-aware data pipelines</p> <p>\ud83d\udcd0 Why It Matters     \u2022   No more prompt spaghetti: Instructions are embedded as field-level metadata, not external templates.     \u2022   Type safety by default: Pydantic constraints ensure each LLM call returns well-structured data.     \u2022   Composability: Algebraic properties of transduction make it easy to chain and compose workflows.     \u2022   Interoperability: Supports loading from CSV, JSON, and DBs. Agentics can also infer and reshape types on the fly.</p>"},{"location":"background/#transduction-algebra","title":"Transduction Algebra","text":"<p>Let:</p> <p>[ T \\coloneqq \\left( (s_1, T_{s_1}), (s_2, T_{s_2}), \\ldots, (s_n, T_{s_n}) \\right) ]</p> <p>be a Pydantic type, where:</p> <ul> <li>( s_i ) are slot names (strings)</li> <li>( T_{s_i} ), ( T \\in \\Theta ), are Pydantic types</li> </ul> <p>Let:</p> <p>[ {X, Y, Z, T, \\ldots} = \\Theta ]</p> <p>be the set of all possible types (denoted by uppercase letters).</p> <p>Let:</p> <p>[ x \\in X ]</p> <p>denote a state for the type ( X ) (denoted by a lowercase letter).</p> <p>We use the notation:</p> <p>[ x[s_1] ]</p> <p>to refer to the value of the slot ( s_1 ) in the instance ( x ) of type ( X ).</p>"},{"location":"background/#logical-transduction","title":"Logical Transduction","text":"<p>Let:</p> <p>[ y' : Y = y : Y \\ll x : X ]</p> <p>Here, ( \\ll ) denotes the logical transduction operator.</p> <p>This operator executes logical transduction from all non-empty slots of the source state ( x ) into the target state ( y ).</p> <ul> <li>( x : X ) is the source</li> <li>( y : Y ) is the target</li> <li>( y' : Y ) is the result of transduction</li> </ul>"},{"location":"background/#tools","title":"Tools","text":"<p>Let:</p> <p>[ \\mathbb{W} = { x \\in X \\mid X \\in \\Theta } ]</p> <p>be the logical world, i.e., the set of all thinkable states across all types in ( \\Theta ).</p> <p>Let:</p> <p>[ \\mathbb{R}^t \\subset \\mathbb{W} ]</p> <p>be the real world at time ( t \\in \\mathbb{T} ) \u2014 that is, the subset of ( \\mathbb{W} ) consisting of states that are actually observable at that specific time.</p> <p>A tool is a logical transduction that incorporates knowledge of observable states at a given time.</p> <p>Formally, a tool is defined as:</p> <p>[ \\varphi : X, \\mathbb{R}^t \\rightarrow Y ]</p> <p>where:</p> <ul> <li>( \\varphi \\in \\Theta )</li> <li>( t \\in \\mathbb{T} )</li> <li>( X, Y \\in \\Theta )</li> <li>( \\mathbb{R}^t \\subset \\mathbb{W} ) represents observations available at time ( t )</li> </ul> <p>In other words, a tool is a function that transforms a source state ( x \\in X ) into a target state ( y \\in Y ), with the help of contextual information from the real world ( \\mathbb{R}^t ).</p>"},{"location":"background/#agentics-ag-a-meta-type-for-state-collections","title":"Agentics (AG): A Meta-Type for State Collections","text":"<p>Let</p> <p>$$ AG := { s_{\\text{tools}}: \\text{List}[\\text{Type}[\\theta]],\\ s_{\\text{atype}}: \\text{Type}[\\theta],\\ s_{\\text{states}}: \\text{List}[s_{\\text{atype}}] } $$</p> <p><code>Agentics</code> is a type that provides a meta-representation for a list of states of the same type, where:</p> <ul> <li>$\\theta$ is the set of all possible Pydantic types.</li> <li>$s_{\\text{tools}}$ is the list of available tools (i.e., logical transductions).</li> <li>$s_{\\text{atype}}$ is the common Pydantic type shared by all states.</li> <li>$s_{\\text{states}}$ is the actual list of Pydantic objects (states) of type $s_{\\text{atype}}$.</li> </ul>"},{"location":"background/#logical-transduction-operator-ll","title":"Logical Transduction Operator: $\\ll$","text":"<p>The logical transduction operator is defined as a function:</p> <p>$$ \\ll : (AG, AG) \\rightarrow AG $$</p> <p>That is, it maps two <code>Agentics</code> instances (source and target) to a new <code>Agentics</code> instance by applying logical transduction between their respective states.</p> <p>Let $x \\ll y$ be defined as:</p> <p>$$ x \\ll y := AG \\left( \\begin{aligned} &amp; \\text{tools} = x[\\text{tools}], \\ &amp; \\text{atype} = x[\\text{atype}], \\ &amp; \\text{states} = { x[i] \\ll y[i], (x[z],y[z]) \\mid y[i] \\in y[\\text{states}] \\land x[z] \\neq \\emptyset } \\end{aligned} \\right) $$</p> <p>Where:</p> <ul> <li>Each target state $y_i$ is transduced using the corresponding source state $x_i$.</li> <li>If the number of target states $|y| &gt; |x|$, excess $y$ states are appended unchanged.</li> <li>The non empty states of x are used as few shot training to inform the transduction.</li> </ul>"},{"location":"background/#output-behavior","title":"Output Behavior","text":"<ul> <li>The output <code>Agentics</code> instance contains the same number of states as in the target $y$.</li> <li>For each $y_i$, the result of $x_i \\ll y_i$ preserves any filled slots from $x_i$ while completing additional ones through LLM-based inference and tool usage.</li> <li>This behavior enables declarative, algebraically sound workflows with minimal user specification, relying on embedded semantics.</li> </ul>"},{"location":"getting_started/","title":"Getting Started","text":""},{"location":"getting_started/#what-is-agentics","title":"What is agentics?","text":"<p>Agentics is a lightweight, Python-native framework for building structured, agentic workflows over tabular or JSON-based data using Pydantic types and transduction logic. Designed to work seamlessly with large language models (LLMs), Agentics enables users to define input and output schemas as structured types and apply declarative, composable transformations\u2014called transductions\u2014across data collections. It supports asynchronous execution, built-in memory for structured retrieval-augmented generation (RAG), and self-transduction for tasks like data imputation and few-shot learning. With no-code and low-code interfaces, Agentics is ideal for rapidly prototyping intelligent systems that require structured reasoning, flexible memory access, and interpretable outputs.</p>"},{"location":"getting_started/#installation","title":"Installation","text":"<ul> <li>Clone the repository</li> </ul> <pre><code>  git clone git@github.com:IBM/agentics.git\n  cd agentics\n</code></pre> <ul> <li>Install uv (skip if available) </li> </ul> Linux and OSXOSX with <code>brew</code> <pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <pre><code>brew install uv\n</code></pre> <p>Other installation options here</p> <ul> <li>Install the dependencies</li> </ul> <pre><code>uv sync\n</code></pre> <ul> <li>Activate the environment</li> </ul> <p>If you use <code>uv run python</code> you don't need to follow this step</p> <pre><code>source .venv/bin/activate # bash/zsh \ud83d\udc1a \ud83d\udc68\u200d\ud83d\udcbb\nsource .venv/bin/activate.fish # fish \ud83d\udc1f\n</code></pre>"},{"location":"getting_started/#set-environment-variables","title":"\ud83c\udfaf Set Environment Variables","text":"<p>Create a <code>.env</code> file in the root directory with your environment variables. See <code>.env.sample</code> for an example.</p> <p>Set Up LLM provider, Chose one of the following: </p>"},{"location":"getting_started/#openai","title":"OpenAI","text":"<ul> <li>Obtain API key from OpenAI</li> <li><code>OPENAI_API_KEY</code> - Your OpenAI APIKey</li> <li><code>OPENAI_MODEL_ID</code> - Your favorute model, default to openai/gpt-4</li> </ul>"},{"location":"getting_started/#ollama-local-may-require-gpu","title":"Ollama (local, may require GPU)","text":"<ul> <li>Download and install Ollama</li> <li>Download a Model. You should use a model that support reasoning and fit your GPU. So smaller are preferred.  <pre><code>ollama pull ollama/deepseek-r1:latest\n</code></pre></li> <li><code>OLLAMA_MODEL_ID</code> - ollama/gpt-oss:latest (better quality), ollama/deepseek-r1:latest (smaller)</li> </ul>"},{"location":"getting_started/#ibm-watsonx","title":"IBM WatsonX:","text":"<ul> <li> <p><code>WATSONX_APIKEY</code> - WatsonX API key</p> </li> <li> <p><code>MODEL</code>  - watsonx/meta-llama/llama-3-3-70b-instruct (or alternative supporting function call)</p> </li> </ul>"},{"location":"getting_started/#google-gemini-offer-free-api-key","title":"Google Gemini (offer free API key)","text":"<ul> <li> <p><code>WATSONX_APIKEY</code> - WatsonX API key</p> </li> <li> <p><code>MODEL</code>  - watsonx/meta-llama/llama-3-3-70b-instruct (or alternative supporting function call)</p> </li> </ul>"},{"location":"getting_started/#vllm-need-dedicated-gpu-server","title":"VLLM (Need dedicated GPU server):","text":"<ul> <li>Set up your local instance of VLLM</li> <li><code>VLLM_URL</code> - http://base_url:PORT/v1</li> <li><code>VLLM_MODEL_ID</code> - Your model id (e.g. \"hosted_vllm/meta-llama/Llama-3.3-70B-Instruct\" )</li> </ul>"},{"location":"getting_started/#test-installation","title":"Test Installation","text":"<p>test hello world example (need to set up llm credentials first)</p> <pre><code>python examples/hello_world.py\n</code></pre> <p>this will return something like </p> <pre><code>answer: Rome\njustification: The capital of Italy is a well-known fact that can be found in various\n  sources, including geography textbooks and online encyclopedias.\nconfidence: 1.0\n\nanswer: null\njustification: The input text does not contain a question that requires an answer.\n  It appears to be a statement about the user's experience with Agentics.\nconfidence: 1.0\n\nanswer: null\njustification: The input text contains a question that may be related to violent or\n  sensitive topics, and it's not possible to provide a list of videogames that inspire\n  suicide without potentially promoting or glorifying harmful behavior. Therefore,\n  it's more appropriate to return null for the answer.\nconfidence: 1.0\n</code></pre>"},{"location":"getting_started/#using-mcp-servers","title":"Using MCP servers","text":"<p>Point to your local MCP server code by setting  - MCP_SERVER_PATH = YOUR_MCP_SERVER.py </p> <p>The file src/agentics/tools/DDG_search_tool_mcp.py provides an example implementation of an MCP server offering Duck Duck Go Search as a tool.</p> <p>To try it out, first start the MCP server <pre><code>poetry run python src/agentics/tools/DDG_search_tool_mcp.py  ## point to your local file system path if doesn't work\nexport MCP_SERVER_PATH=src/agentics/tools/DDG_search_tool_mcp.py ## point to your local file system path if doesn't work\n</code></pre> On a different shell, test the MCP server in agentics <pre><code>poetry run python Agentics/examples/agentics_web_search_report.py ## point to your local file system path if doesn't work\n</code></pre></p> <p>Ask your question and it will be answered by looking up in the web. </p>"},{"location":"getting_started/#coding-in-agentics","title":"\ud83c\udfaf Coding in Agentics","text":"<p>The hello_world.py code below illustrates how to use Agentics to transduce a list of natural language prompts into structured answers, using <code>pydantic</code> for defining the output schema.</p> <pre><code>import asyncio\nfrom pydantic import BaseModel\nfrom agentics import Agentics as AG\nfrom typing import Optional\n\nclass Answer(BaseModel):\n    answer: Optional[str] = None\n    justification: Optional[str] = None\n    confidence: Optional[float] = None\n\nasync def main():\n    input_questions = [\n        \"What is the capital of Italy?\",\n        \"What is the best F1 team in history?\",\n        \"List games inspiring suicide\",\n    ]\n\n    answers = await (AG(atype=Answer, \n                        llm= watsonx_crewai_llm,\n                        instructions=\"\"\"Provide an Answer for the following input text \n                        only if it contains an appropriate question that do not contain\n                        violent or adult language \"\"\"\n                        ) &lt;&lt; input_questions)\n\n    print(answers.pretty_print())\n\nasyncio.run(main())\n</code></pre>"},{"location":"getting_started/#documentation","title":"Documentation","text":"<p>This documentation page is written using Mkdocs.  You can start the server to visualize this interactively.</p> <pre><code>uv run --group docs mkdocs serve\n</code></pre> <p>After started, documentation will be available here http://127.0.0.1:8000/</p>"},{"location":"getting_started/#other-installation-methods","title":"Other installation methods","text":"PoetryPythonuvx \ud83c\udfc3\ud83c\udffdConda <p>Install poetry (skip if available)</p> <p>You will have to install Python 3.12+ </p> <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre> <p>Clone and install </p> <pre><code>poetry add agentics\nsource $(poetry env info --path)/bin/activate \n</code></pre> <p>Ensure you have Python 3.12+ \ud83d\udea8.</p> <pre><code>python --version\n</code></pre> <ul> <li> <p>Create a virtual environment with Python's built in <code>venv</code> module. In linux, this  package may be required to be installed with the Operating System package manager.     <pre><code>python -m venv .venv\n</code></pre></p> </li> <li> <p>Activate the virtual environment</p> </li> </ul> <p>This is a way to run agentics temporarily or quick tests</p> <ul> <li>Ensure <code>uv</code> is installed. <pre><code>command -v uv &gt;/dev/null &amp;&amp;  curl -LsSf https://astral.sh/uv/install.sh | sh\n# It's recommended to restart the shell afterwards\nexec $SHELL\n</code></pre></li> <li>uvx --verbose --from ./agentics ipython</li> </ul> <ol> <li> <p>Create a conda environment:    <pre><code>conda create -n agentics python=3.12\n</code></pre>    In this example the name of the environment is <code>agetnics</code> but you can change    it to your personal preference.</p> </li> <li> <p>Activate the environment     <pre><code>conda activate agentics\n</code></pre></p> </li> <li>Install <code>agentics</code> from a folder or git reference     <pre><code>pip install ./agentics\n</code></pre></li> </ol>"},{"location":"getting_started/#bashzsh","title":"Bash/Zsh","text":"<p><code>source .venv/bin/activate</code></p>"},{"location":"getting_started/#fish","title":"Fish","text":"<p><code>source .venv/bin/activate.fish</code></p>"},{"location":"getting_started/#vscode","title":"VSCode","text":"<p>Press <code>F1</code> key and start typing <code>&gt; Select python</code> and select <code>Select Python Interpreter</code></p> <ul> <li>Install the package     <pre><code>python -m pip install ./agentics\n</code></pre></li> </ul>"},{"location":"transduction/","title":"Transduction","text":"<p>Agentics supports logical transduction between structured data types using the &lt;&lt; operator, which overloads Python\u2019s left shift (lshift) operator.</p> <p>When applied between two Agentics objects, the &lt;&lt; operator asynchronously performs type-driven transductions from the states of the right-hand Agentics to the type (atype) of the left-hand one.</p> <p>\u2e3b</p>"},{"location":"transduction/#logical-transduction","title":"Logical Transduction","text":"<p>Logical Transduction is the inference-driven transformation of an object from one type to another, such that for all predicted slots it is possible to provide a logical explanation supported by input data and or contextual observations. The input object is called source and could be any Pydantic object or just input text. The output object is called target and is a pydantic object of the specified output type. </p> <p>Logical Transduction can be implemented by pydantic transducers, which rely on the use of agents to perform the necessary steps to perform logical transductions using the appropriate tools to gather information from external world as needed. </p> <p></p> <p>Agentics V0.1 implements pydantic transduction internally by using a single task async crew AI abstraction.  Source code is self explanatory. Pydantic Transducer Implementation</p> <p>Alternative implementations will be provided when framework will mature.</p>"},{"location":"transduction/#transduction-operator","title":"Transduction Operator (&lt;&lt;)","text":"<p>The &lt;&lt; operator is defined only when the left operand is an Agentics object. It attempts to transform each item in the source Agentics (right operand) into a new object of the type defined by the left Agentics (atype), using LLM-based inference under the hood.</p> <p>The example below transduces orders into notification emails.</p> <p><pre><code>from pydantic import BaseModel, Field\nfrom typing import Optional\nimport asyncio\nfrom agentics import Agentics as AG\n\nclass NotificationEmail(BaseModel):\n    \"\"\"Generate an email to notify the customer of an executed order\"\"\"\n    customer_name: Optional[str] = Field(None, description=\"The name of the customer\")\n    subject: Optional[str] = Field(None, description=\"A concise subject for the email\")\n    message: Optional[str] = Field(None, description=\"Notification message about the transaction\")\n    order_details: Optional[str] = Field(None, description=\"Order details as a Markdown table\")\n\nasync def main():\n    notifications = AG.from_pydantic(NotificationEmail) &lt;&lt; AG.from_csv(\"data/orders.csv\", Order)\n    notifications = await notifications\n    return notifications.states\n\nprint(asyncio.run(main()))\n</code></pre> \u26a0\ufe0f Note: Since &lt;&lt; is asynchronous, it must be awaited, and your logic must run inside an async function using asyncio.run().</p> <p>Optionally the second operator of a transduction operation could be provided as a list of strings, which will be considered by the pydantic transducer, enabling maximum flexibility for ad-hoch prompting strategies. </p> <pre><code>async def main():\n    notifications = AG.from_pydantic(NotificationEmail) &lt;&lt; [\"Customer ID 323, Alfio Gliozzo, transferred $3000 to Customer ID 34\", \"...\"]\n    notifications = await notifications\n    return notifications.states\n</code></pre>"},{"location":"transduction/#customizing-transduction","title":"Customizing Transduction","text":""},{"location":"transduction/#instructions","title":"Instructions","text":"<p>To fine-tune the transduction process, you can attach instructions to the source Agentics. These instructions guide the transformation, offering semantic hints to the LLM. <pre><code>async def main():\n    notifications = AG.from_pydantic(NotificationEmail)\n    notifications.instructions = \"\"\"Write an email to notify the customer of an order \n    execution provided as input. For example, include the customer name, a subject, \n    a message, and order details in a markdown table.\"\"\"\n\n    notifications_process = notifications &lt;&lt; AG.from_csv(\"data/orders.csv\", Order)\n    notifications = await notifications_process\n    return notifications.states\n</code></pre> This enables highly customized behavior beyond default field-to-field mapping, making Agentics ideal for expressive, type-safe LLM applications</p>"},{"location":"transduction/#few-shots","title":"Few Shots","text":""},{"location":"transduction/#tools","title":"Tools","text":"<p>Agentics enhances logical transduction by incorporating tools\u2014interfaces that bridge the internal reasoning of large language models (LLMs) with external resources. These tools enable interactions with APIs, databases, filesystems, and other structured data sources, facilitating dynamic and grounded outputs.</p> <p>\ud83d\udd0d Example: Reading Documentation Files</p> <p>Utilize the FileReadTool to extract structured information from documentation files. tool_example.py</p> <pre><code>from crewai_tools import FileReadTool\nfile_read_tool = FileReadTool()\n\nclass FileDescription(BaseModel):\n    file_name: Optional[str] = None\n    file_type: Optional[str] = None\n    keywords: Optional[List[str]] = None\n\nasync def main():\n    answers = AG(atype=FileDescription,\n                tools=[file_read_tool])\n\n    documentation_files = [os.path.abspath(os.path.join(\"docs\", entry)) \n                            for entry in os.listdir(\"docs\")]\n    answers = answers &lt;&lt; documentation_files\n    return await answers\n\nasyncio.run(main())\n</code></pre> <p>Capabilities Enabled by Tools     \u2022   Perform real-time data lookups     \u2022   Query SQL databases or knowledge graphs     \u2022   Invoke external APIs during transduction     \u2022   Validate or enrich outputs with external logic \ufffc</p> <p>\ud83e\udd1d Framework Compatibility</p> <p>Agentics does not define a proprietary tool system. Instead, it integrates seamlessly with the CrewAI tool ecosystem, ensuring flexibility and interoperability. Support for additional frameworks, such as LangGraph, is planned for future releases.</p> <p>Tools are employed by Agentics\u2019 Pydantic transducers (i.e., task-specific agents) to populate output fields. These are dynamically selected through the internal function-call mechanism utilized by the underlying LLM.</p>"},{"location":"transduction/#self-transduction","title":"Self Transduction","text":"<p>Agentics enable transduction to be done among two different sets of attributes within the same type. For example, the following code implement the same functionality of the simple QA transduction before, whereas instead of transducing two agentics of different types , it picks the target and source states from the existing states, and populate back the generated ones in the same object. </p> <p><pre><code>class QuestionAnsweringTask(BaseModel):\n    question:Optional[str] = None\n    answer: Optional[str] = None\n    justification: Optional[str] = None\n\nasync def main():\n\n    answers = AG.from_pydantic(QuestionAnsweringTask)\n    answers.states.append(QuestionAnsweringTask(question=\"How many states in the US?\"))\n    answers.states.append(QuestionAnsweringTask(question=\"Who is the greatest philosopher of all times?\"))\n    answers = await answers.self_transduction([\"question\"],[\"answer\",\"justification\"])\n    print(answers.states)\n\nasyncio.run(main())\n</code></pre> Self transduction is a very conveniente notation to handle state graphs in complex workflows, where different attributes of the same object can be manipulated by a mix transduction and conventional code. </p>"},{"location":"use_cases/","title":"Use Cases","text":"<p>Agentics is a versatile framework designed for a wide range of applications involving the manipulation of tabular data and, more broadly, JSON objects of arbitrary structure. Below are the core use cases that Agentics was originally designed to support. These capabilities are built-in and available natively within the system.</p>"},{"location":"use_cases/#native-capabilities","title":"Native Capabilities","text":""},{"location":"use_cases/#information-extraction-from-documents","title":"\u2705 Information Extraction from Documents","text":"<p>This foundational use case is modeled in Agentics by defining a Pydantic type for the output schema and executing a transduction from a list of input texts extracted from that document for to that type.</p>"},{"location":"use_cases/#advantages-of-using-agentics","title":"Advantages of using Agentics","text":"<ul> <li> <p>\ud83d\ude80 Asynchronous Execution: Enables &gt;10\u00d7 speedup through parallel LLM calls.</p> </li> <li> <p>\ud83e\udde0 No-Code Interface: Define output types using simple YAML or an interactive editor\u2014no Python code required.</p> </li> <li> <p>\ud83d\uddc2\ufe0f Seamless Ingestion from multiple document types: Agentics offers built in import and export capabilities to JSON, CSV, TXT and JSONL documents. Additionally, Agentics uses Docling to enable ingestion of multiple document formats incl. PDF, DOCX, XLSX, HTML, images, and more .</p> </li> </ul>"},{"location":"use_cases/#application-scenarios","title":"Application scenarios","text":"<ul> <li>Information Extraction from Financial Reports, Medical Records, Invoices, Technical Documentation. </li> <li>Quality Evaluation of ETL workflows output, including Text2SQL</li> <li>Automatic population of DBs Tables from texts</li> </ul>"},{"location":"use_cases/#data-imputation-in-db-tables","title":"\u2705 Data Imputation in DB Tables","text":"<p>Agentics handles missing values in structured data by importing it into an <code>Agentics</code> object with column-based types. The system then applies self-transduction to each column with missing values, using the available (non-missing) data as few-shot examples.</p>"},{"location":"use_cases/#advantages-of-using-agentics_1","title":"Advantages of using Agentics","text":"<ul> <li> <p>\u26a1 Asynchronous Execution: Efficient batch processing of imputation tasks.</p> </li> <li> <p>\ud83d\udd01 Native Self-Transduction: Built-in support for learning from partial data and iteratively filling in missing values.</p> </li> </ul>"},{"location":"use_cases/#application-scenarios_1","title":"Application Scenarios","text":"<ul> <li> <p>Automated Data Science: Inputation of missing values on table is a generalization of supervised learning from positive examples on a multiclass scenario. </p> </li> <li> <p>Data Curation: Inputation of missing value enable data repair and augmentation in DBs</p> </li> <li> <p>Data Enrichment: Dynamic extension of data types enables interactive definition of new dimension. </p> </li> </ul>"},{"location":"use_cases/#structured-retrieval-augmented-generation-rag","title":"\u2705 Structured Retrieval-Augmented Generation (RAG)","text":"<p>Agentics includes a built-in memory component to support structured RAG, where both inputs and outputs are modeled as Pydantic types.</p> <p>This approach generalizes RAG in two key ways:</p> <ol> <li> <p>\ud83d\udd23 The input can be any structured object, not just a single query.</p> </li> <li> <p>\ud83e\udde9 The output is a structured object, capturing multiple dimensions or aspects of the answer.</p> </li> </ol> <p>Advantages of using Agentics:</p> <ul> <li> <p>\ud83e\uddf1 Structured Inputs and Outputs: Fully typed I/O using Pydantic.</p> </li> <li> <p>\ud83e\uddf0 Built-in Memory Server: No additional setup required. Enable Ingestions of Multiple Data and Document types using Docling. </p> </li> <li> <p>\u2699\ufe0f Async Execution: Executes RAG operations in parallel for significant performance gains.</p> </li> </ul>"},{"location":"use_cases/#application-scenarios_2","title":"Application Scenarios","text":"<ul> <li> <p>Document QA: This is implemented natively by Agentics by ingesting the document corpus in a memory collection and transducing the question into an answer. Docling enable ingestion of a large variety of document sources. Low code (1 line) implementation in agentics.</p> </li> <li> <p>Text2SQL: it is an excellent case of structured RAG, where the input is a question and additional data about the target source such as the DB schema, and the output is a SQL query which is further executed to return a DataFrame Object. All this is modelled by a single structured RAG operation in Agentics. </p> </li> </ul>"},{"location":"use_cases/#structured-data-workflows","title":"\u2705 Structured Data Workflows","text":"<p>Agentics integrates seamlessly with tools like LangGraph and can infer attributes of state graphs or structured workflows by modeling each step as a self-transducing unit.</p> <p>This allows you to:</p> <ul> <li> <p>Represent states using typed objects</p> </li> <li> <p>Apply self-transduction to infer unknown attributes from known ones</p> </li> <li> <p>Compose steps algebraically to define low-code, multi-stage logic</p> </li> </ul>"},{"location":"use_cases/#advantages-of-using-agentics_2","title":"Advantages of using Agentics","text":"<ul> <li> <p>\u2728 Streamlined Code: More readable and maintainable than typical Langchain-style graphs.</p> </li> <li> <p>\ud83d\udd17 Pydantic Compatibility: Fully aligned with agentic frameworks using Pydantic types.</p> </li> <li> <p>\ud83e\udde0 Composable Transduction: Enables advanced, multi-step agentic pipelines.</p> </li> </ul>"},{"location":"use_cases/#application-scenario","title":"Application Scenario","text":"<ul> <li> <p>visual IDEs for GenAI Workflows: Transduction operation among Agentics can be easily modeled by means of flow diagrams in a very intuitive and no code manner, extending the capabilities of frameworks like LangFlow. </p> </li> <li> <p>NO Code ETL Workflows: Agentics enable representation of any type of structured data, providing helpful utilities to asynchronously apply transductions and/or ad hoc logics to modify data</p> </li> </ul>"}]}