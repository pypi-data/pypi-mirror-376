{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42505bd5",
   "metadata": {},
   "source": [
    "# Helix LLM & Embedding Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b0ba58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helix.client import Client\n",
    "from helix.mcp import MCPServer, ToolConfig\n",
    "from pydantic import BaseModel\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply() \n",
    "\n",
    "class Person(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "    occupation: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6a27d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helix Client\n",
    "helix_client = Client(local=True)\n",
    "\n",
    "# Helix MCP Server\n",
    "# Enable search_vector tool instead because we will add the embedder later\n",
    "mcp_server = MCPServer(\"helix-mcp\", helix_client, tool_config=ToolConfig(search_vector=True, search_vector_text=False))\n",
    "\n",
    "# Run mcp server in background thread on port 8000\n",
    "# To stop the server, restart the notebook\n",
    "mcp_server.run_bg()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba6b48d",
   "metadata": {},
   "source": [
    "## OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f08ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[08/25/25 15:37:12] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Starting MCP server <span style=\"color: #008000; text-decoration-color: #008000\">'helix-mcp'</span> with transport <span style=\"color: #008000; text-decoration-color: #008000\">'streamable-http'</span> on     <a href=\"file:///Users/gwuwong/Documents/helixdb/helix-py/.venv/lib/python3.13/site-packages/fastmcp/server/server.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">server.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///Users/gwuwong/Documents/helixdb/helix-py/.venv/lib/python3.13/site-packages/fastmcp/server/server.py#1519\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1519</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://127.0.0.1:8000/mcp/</span>                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[08/25/25 15:37:12]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Starting MCP server \u001b[32m'helix-mcp'\u001b[0m with transport \u001b[32m'streamable-http'\u001b[0m on     \u001b]8;id=346993;file:///Users/gwuwong/Documents/helixdb/helix-py/.venv/lib/python3.13/site-packages/fastmcp/server/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=229961;file:///Users/gwuwong/Documents/helixdb/helix-py/.venv/lib/python3.13/site-packages/fastmcp/server/server.py#1519\u001b\\\u001b[2m1519\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttp://127.0.0.1:8000/mcp/\u001b[0m                                              \u001b[2m              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helix.providers.openai_client import OpenAIProvider\n",
    "from helix.embedding.openai_client import OpenAIEmbedder\n",
    "\n",
    "openai_llm = OpenAIProvider(\n",
    "    name=\"openai-llm\",\n",
    "    instructions=\"You are a helpful assistant.\",\n",
    "    model=\"gpt-5-nano\",\n",
    "    reasoning={\n",
    "        \"effort\":\"low\"\n",
    "    },\n",
    "    history=True\n",
    ")\n",
    "\n",
    "openai_embedder = OpenAIEmbedder()\n",
    "\n",
    "# Set embedder for MCP server\n",
    "mcp_server.embedder = openai_embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f34fa7",
   "metadata": {},
   "source": [
    "### Generating LLM Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60850d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi! I’m here and ready to help. How can I assist you today? \n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "Nice to meet you, John. Thanks for sharing—25 and a software engineer. What would you like to dive into today? I can help with coding questions, debugging, project ideas, optimization, interview prep, resume tips, or anything specific you’re working on. If you have a problem or a project, tell me the details and I’ll jump in. \n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "name='John Berger' age=25 occupation='Software Engineer' \n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "I have the MCP traversal tools available. Here’s a quick rundown of what they do and how you can use them:\n",
      "\n",
      "What you can do with the MCP tools\n",
      "- init(): Start a traversal session. Returns a connection_id you’ll use for subsequent calls.\n",
      "- next({ connection_id }): Get the next item in the traversal results for the given session.\n",
      "- collect({ connection_id, range, drop }): Collect items from the traversal. range lets you specify start/end; drop resets the session if true.\n",
      "- reset({ connection_id }): Reset the traversal session (you can start fresh with the same or a new session after).\n",
      "- n_from_type({ connection_id, node_type }): Retrieve the first node of a specified type.\n",
      "- e_from_type({ connection_id, edge_type }): Retrieve the first edge of a specified type.\n",
      "- out_step({ connection_id, edge_label, edge_type }): Traverse outward from current nodes/vectors via an edge_label to target nodes/vectors.\n",
      "- out_e_step({ connection_id, edge_label }): Traverse outward to the edges that originate from current nodes/vectors (via edge_label).\n",
      "- in_step({ connection_id, edge_label, edge_type }): Traverse inward to target nodes/vectors via edge_label.\n",
      "- in_e_step({ connection_id, edge_label }): Traverse inward to the edges that point to current nodes/vectors via edge_label.\n",
      "- filter_items({ connection_id, filter }): Apply a filter to the current traversal state to narrow results.\n",
      "- search_vector_text({ connection_id, query, label }): Perform similarity search on vectors by text query; returns top results.\n",
      "- search_keyword({ connection_id, query, label, limit }): BM25-like keyword search on nodes of a given label; returns top results.\n",
      "\n",
      "Parallel tool usage\n",
      "- You can run multiple tool calls in parallel using multi_tool_use.parallel when they don’t depend on each other. This can speed up traversal setup and queries.\n",
      "\n",
      "If you’d like, tell me what you’re trying to explore in your graph (node/edge types, start node, filters), and I can propose a sequence of MCP tool calls to achieve your goal. \n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generating response from string\n",
    "response = openai_llm.generate(\"Hello, how are you?\")\n",
    "print(response, '\\n' + '-'*100, '\\n')\n",
    "\n",
    "# Generate response from message\n",
    "response = openai_llm.generate([{\"role\": \"user\", \"content\": \"My name is John Berger. I am a software engineer. I am 25 years old.\"}])\n",
    "print(response, '\\n' + '-'*100, '\\n')\n",
    "\n",
    "# Generate structured response\n",
    "response = openai_llm.generate([{\"role\": \"user\", \"content\": \"Who am I?\"}], Person)\n",
    "print(response, '\\n' + '-'*100, '\\n')\n",
    "\n",
    "# Enable MCP tools\n",
    "openai_llm.enable_mcps(\"helix-mcp\")\n",
    "response = openai_llm.generate([{\"role\": \"user\", \"content\": \"What MCP tools do you have?\"}])\n",
    "print(response, '\\n' + '-'*100, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10829f02",
   "metadata": {},
   "source": [
    "### Generating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca8832f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension: 1536\t[0.0037015017587691545, 0.008918856270611286, -0.04589862376451492, 0.0055170003324747086, -0.025452231988310814] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[HELIX]\u001b[0m Embedding: 100%|██████████| 3/3 [00:00<00:00, 48960.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension: 1536\t[0.009038523770868778, 0.024945339187979698, -0.05953341722488403, 0.013305002823472023, -0.06579749286174774]\n",
      "Dimension: 1536\t[0.0016707692993804812, -0.012029538862407207, -0.04344000294804573, -0.017710154876112938, 0.013520379550755024]\n",
      "Dimension: 1536\t[0.021052706986665726, 0.022584712132811546, -0.02384490892291069, 0.00512727303430438, -0.022152289748191833]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Embedding text\n",
    "vector = openai_embedder.embed(\"My name is John Berger. I am a software engineer. I am 25 years old.\")\n",
    "print(f\"Dimension: {len(vector)}\\t\" + str(vector[:5]), \"\\n\")\n",
    "\n",
    "# Embedding batch documents\n",
    "documents = [\"My name is John Berger.\", \"I am a software engineer.\", \"I am 25 years old.\"]\n",
    "for vector in openai_embedder.embed_batch(documents):\n",
    "    print(f\"Dimension: {len(vector)}\\t\" + str(vector[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca7d552",
   "metadata": {},
   "source": [
    "## Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d046d84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helix.providers.gemini_client import GeminiProvider\n",
    "from helix.embedding.gemini_client import GeminiEmbedder\n",
    "\n",
    "gemini_llm = GeminiProvider(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    temperature=0.1,\n",
    "    history=True\n",
    ")\n",
    "\n",
    "gemini_embedder = GeminiEmbedder()\n",
    "\n",
    "# Set embedder for MCP server\n",
    "mcp_server.embedder = gemini_embedder\n",
    "mcp_server.embedder_args = {\"task_type\": \"RETRIEVAL_QUERY\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa3e277",
   "metadata": {},
   "source": [
    "### Generating LLM Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11d86747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am doing well, thank you for asking! As a large language model, I don't experience emotions or feelings like humans do, but I am functioning optimally and ready to assist you. How can I help you today?\n",
      " \n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "Okay, John Berger, 25-year-old software engineer. Nice to meet you! Is there anything specific you'd like to talk about or any way I can help you today? Perhaps you have a coding question, want to brainstorm ideas, or just want to chat? Let me know!\n",
      " \n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "name='John Berger' age=25 occupation='software engineer' \n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "I have access to the following tools for interacting with the MCP:\n",
      "\n",
      "*   `init`: Initializes the MCP traversal connection.\n",
      "*   `next`: Gets the next item in the traversal results.\n",
      "*   `collect`: Collects all items in the traversal results.\n",
      "*   `reset`: Resets the MCP traversal connection.\n",
      "*   `n_from_type`: Retrieves all nodes of a given type.\n",
      "*   `e_from_type`: Retrieves all edges of a given type.\n",
      "*   `out_step`: Traverses out from current nodes or vectors.\n",
      "*   `out_e_step`: Traverses out to edges from current nodes or vectors.\n",
      "*   `in_step`: Traverses in from current nodes or vectors.\n",
      "*   `in_e_step`: Traverses in to edges from current nodes or vectors.\n",
      "*   `filter_items`: Filters the current state of the traversal.\n",
      "*   `search_vector_text`: Similarity searches vectors based on text query.\n",
      "*   `search_keyword`: BM25 searches nodes based on keyword query. \n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generating response from string\n",
    "response = gemini_llm.generate(\"Hello, how are you?\")\n",
    "print(response, '\\n' + '-'*100, '\\n')\n",
    "\n",
    "# Generate response from message\n",
    "response = gemini_llm.generate([{\"role\": \"user\", \"parts\": [{\"text\": \"My name is John Berger. I am a software engineer. I am 25 years old.\"}]}])\n",
    "print(response, '\\n' + '-'*100, '\\n')\n",
    "\n",
    "# Generate structured response\n",
    "response = gemini_llm.generate([{\"role\": \"user\", \"parts\": [{\"text\": \"Who am I?\"}]}], Person)\n",
    "print(response, '\\n' + '-'*100, '\\n')\n",
    "\n",
    "# Enable MCP tools\n",
    "gemini_llm.enable_mcps(\"helix-mcp\")\n",
    "response = gemini_llm.generate([{\"role\": \"user\", \"parts\": [{\"text\": \"What MCP tools do you have?\"}]}])\n",
    "print(response, '\\n' + '-'*100, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f1f518",
   "metadata": {},
   "source": [
    "### Generating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55465fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension: 1536\t[-0.017157292, 0.018253414, 0.017877506, -0.0758527, -0.013305571] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[HELIX]\u001b[0m Embedding: 100%|██████████| 3/3 [00:00<00:00, 43539.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension: 1536\t[-0.010100627, 0.00897918, 0.015125714, -0.085755296, -0.018233819]\n",
      "Dimension: 1536\t[0.002266545, 0.0043500746, 0.0003194056, -0.100754924, -0.0135422405]\n",
      "Dimension: 1536\t[-0.013887029, 0.008958238, 0.0064157364, -0.08546713, -0.004919313]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Embedding text\n",
    "vector = gemini_embedder.embed(\"My name is John Berger. I am a software engineer. I am 25 years old.\", task_type=\"RETRIEVAL_DOCUMENT\")\n",
    "print(f\"Dimension: {len(vector)}\\t\" + str(vector[:5]), \"\\n\")\n",
    "\n",
    "# Embedding batch documents\n",
    "documents = [\"My name is John Berger.\", \"I am a software engineer.\", \"I am 25 years old.\"]\n",
    "for vector in gemini_embedder.embed_batch(documents, task_type=\"RETRIEVAL_DOCUMENT\"):\n",
    "    print(f\"Dimension: {len(vector)}\\t\" + str(vector[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868f4681",
   "metadata": {},
   "source": [
    "## Anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d65c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helix.providers.anthropic_client import AnthropicProvider\n",
    "from helix.embedding.voyageai_client import VoyageAIEmbedder\n",
    "\n",
    "anthropic_llm = AnthropicProvider(\n",
    "    model=\"claude-3-5-haiku-20241022\",\n",
    "    temperature=0.1,\n",
    "    history=True\n",
    ")\n",
    "\n",
    "voyageai_embedder = VoyageAIEmbedder()\n",
    "\n",
    "# Set embedder for MCP server\n",
    "mcp_server.embedder = voyageai_embedder\n",
    "mcp_server.embedder_args = {\"input_type\": \"query\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c3d0df",
   "metadata": {},
   "source": [
    "### Generating LLM Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c802923c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm doing well, thank you for asking! How are you today? Is there anything I can help you with? \n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "Hello John! It's nice to meet you. As a 25-year-old software engineer, I imagine you work on some interesting technical projects. Is there anything specific you'd like to discuss today? \n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n",
      "name='John Berger' age=25 occupation='software engineer' \n",
      "---------------------------------------------------------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generating response from string\n",
    "response = anthropic_llm.generate(\"Hello, how are you?\")\n",
    "print(response, '\\n' + '-'*100, '\\n')\n",
    "\n",
    "# Generate response from message\n",
    "response = anthropic_llm.generate([{\"role\": \"user\", \"content\": \"My name is John Berger. I am a software engineer. I am 25 years old.\"}])\n",
    "print(response, '\\n' + '-'*100, '\\n')\n",
    "\n",
    "# Generate structured response\n",
    "response = anthropic_llm.generate([{\"role\": \"user\", \"content\": \"Who am I?\"}], Person)\n",
    "print(response, '\\n' + '-'*100, '\\n')\n",
    "\n",
    "# Anthropic doesn't currently support local MCP servers via streamable http connection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc83416",
   "metadata": {},
   "source": [
    "### Generating Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98ddb4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension: 1024\t[-0.017457768321037292, 0.04947829991579056, -0.029827343299984932, 0.0845692902803421, 0.04526738077402115] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[HELIX]\u001b[0m Embedding: 100%|██████████| 3/3 [00:00<00:00, 41120.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension: 1024\t[-0.03412482887506485, 0.03371741250157356, -0.04263145104050636, 0.06879756599664688, 0.06010128930211067]\n",
      "Dimension: 1024\t[-0.00386068201623857, 0.05951542779803276, 0.010549462400376797, 0.05377177521586418, 0.02488693594932556]\n",
      "Dimension: 1024\t[-0.007679720874875784, 0.035258837044239044, -0.024098483845591545, 0.06100286915898323, 0.03362797945737839]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Embedding text\n",
    "vector = voyageai_embedder.embed(\"My name is John Berger. I am a software engineer. I am 25 years old.\", input_type=\"document\")\n",
    "print(f\"Dimension: {len(vector)}\\t\" + str(vector[:5]), \"\\n\")\n",
    "\n",
    "# Embedding batch documents\n",
    "documents = [\"My name is John Berger.\", \"I am a software engineer.\", \"I am 25 years old.\"]\n",
    "for vector in voyageai_embedder.embed_batch(documents, input_type=\"document\"):\n",
    "    print(f\"Dimension: {len(vector)}\\t\" + str(vector[:5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
