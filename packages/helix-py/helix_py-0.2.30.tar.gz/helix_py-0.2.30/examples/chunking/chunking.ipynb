{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11b4f5ec",
   "metadata": {},
   "source": [
    "# Helix Chunking\n",
    "\n",
    "Uses Chonkie's chunker under the hood. You can read which Chonkie chunker will work best for your use case at https://docs.chonkie.ai/python-sdk/chunkers/overview\n",
    "\n",
    "`helix.Chunk` returns a list of your chunked text from Chonkie so you can use it like:\n",
    "\n",
    "`chunks = helix.Chunk.token_chunk(massive_text_blob, chunk_size=100)` --> this gives a list of chunked string\n",
    "\n",
    "`db.query(\"endpoint\", {\"chunks\": chunks})` --> this is assuming your QUERY looks something like this:\n",
    "\n",
    "\n",
    "```C++\n",
    "QUERY create_chunk(contents: [String]) =>\n",
    "    FOR content in contents {\n",
    "            AddN<Chunk>({chunk: content})\n",
    "    }\n",
    "    RETURN \"success\" \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b89d611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9848b0bf",
   "metadata": {},
   "source": [
    "### Sample Text for Single Text Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea4ed391",
   "metadata": {},
   "outputs": [],
   "source": [
    "massive_text_blob = \"\"\"\n",
    "This is a massive text blob that we want to chunk into smaller pieces for processing. It contains multiple sentences and paragraphs that need to be divided appropriately to maintain context while fitting within token limits. When working with large documents, it is important to ensure that each chunk maintains enough context for downstream tasks, such as retrieval or summarization. Chunking strategies can vary depending on the use case, but the goal is always to balance context preservation with processing efficiency.\n",
    "\n",
    "The chunker should handle overlaps properly to ensure no important information is lost at chunk boundaries. For example, if a sentence is split between two chunks, the overlap ensures that both chunks retain the full meaning of the text. This is especially important in applications like document question answering, where missing a single sentence could lead to incorrect answers. Additionally, chunkers may need to account for different languages, code blocks, or special formatting, which can add complexity to the chunking process.\n",
    "\n",
    "This example demonstrates how the token chunker works with a realistic text sample that would be common in document processing and RAG (Retrieval-Augmented Generation) applications. The chunks will be created with specified token limits and overlap settings to optimize for both comprehension and processing efficiency. Each chunk will contain metadata about its position in the original text and token count for further processing. By using a robust chunking strategy, we can ensure that downstream models receive high-quality, context-rich input, improving the overall performance of NLP pipelines and applications.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4445216f",
   "metadata": {},
   "source": [
    "### Sample Text List for Batch Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2886928",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"First document to chunk with some content for testing.\",\n",
    "    \"Second document with different content for batch processing.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5cf178",
   "metadata": {},
   "source": [
    "### Sample Text For Code Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc5224a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_sample = \"\"\"\n",
    "def hello_world():\n",
    "    print(\"Hello, Chonkie!\")\n",
    "\n",
    "class MyClass:\n",
    "    def __init__(self):\n",
    "        self.value = 42\n",
    "\"\"\"\n",
    "\n",
    "code_samples = [\n",
    "    \"def func1():\\n    pass\",\n",
    "    \"const x = 10;\\nfunction add(a, b) { return a + b; }\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c01de2",
   "metadata": {},
   "source": [
    "### Token Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eded7b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nThis is a massive text blob that we want to chunk into smaller pieces for processing. It contains m',\n",
       " 't contains multiple sentences and paragraphs that need to be divided appropriately to maintain conte',\n",
       " 'intain context while fitting within token limits. When working with large documents, it is important',\n",
       " 'is important to ensure that each chunk maintains enough context for downstream tasks, such as retrie',\n",
       " 'ch as retrieval or summarization. Chunking strategies can vary depending on the use case, but the go',\n",
       " ', but the goal is always to balance context preservation with processing efficiency.\\n\\nThe chunker sh',\n",
       " 'e chunker should handle overlaps properly to ensure no important information is lost at chunk bounda',\n",
       " 'chunk boundaries. For example, if a sentence is split between two chunks, the overlap ensures that b',\n",
       " 'sures that both chunks retain the full meaning of the text. This is especially important in applicat',\n",
       " ' in applications like document question answering, where missing a single sentence could lead to inc',\n",
       " ' lead to incorrect answers. Additionally, chunkers may need to account for different languages, code',\n",
       " 'guages, code blocks, or special formatting, which can add complexity to the chunking process.\\n\\nThis ',\n",
       " 'cess.\\n\\nThis example demonstrates how the token chunker works with a realistic text sample that would',\n",
       " 'e that would be common in document processing and RAG (Retrieval-Augmented Generation) applications.',\n",
       " 'pplications. The chunks will be created with specified token limits and overlap settings to optimize',\n",
       " ' to optimize for both comprehension and processing efficiency. Each chunk will contain metadata abou',\n",
       " 'etadata about its position in the original text and token count for further processing. By using a r',\n",
       " 'By using a robust chunking strategy, we can ensure that downstream models receive high-quality, cont',\n",
       " 'uality, context-rich input, improving the overall performance of NLP pipelines and applications.\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = helix.Chunk.token_chunk(massive_text_blob, chunk_size=100)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5c5d452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🦛 choooooooooooooooooooonk 100% • 2/2 batches chunked [00:00<00:00, 11618.57batch/s] 🌱\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['First document to chunk with some content for testing.',\n",
       " 'Second document with different content for batch processing.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_chunks = helix.Chunk.token_chunk(texts)\n",
    "batch_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad93b5a2",
   "metadata": {},
   "source": [
    "### Sentence Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cb5afd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nThis is a massive text blob that we want to chunk into smaller pieces for processing. It contains multiple sentences and paragraphs that need to be divided appropriately to maintain context while fitting within token limits. When working with large documents, it is important to ensure that each chunk maintains enough context for downstream tasks, such as retrieval or summarization. Chunking strategies can vary depending on the use case, but the goal is always to balance context preservation with processing efficiency.\\n\\nThe chunker should handle overlaps properly to ensure no important information is lost at chunk boundaries. For example, if a sentence is split between two chunks, the overlap ensures that both chunks retain the full meaning of the text. This is especially important in applications like document question answering, where missing a single sentence could lead to incorrect answers. Additionally, chunkers may need to account for different languages, code blocks, or special formatting, which can add complexity to the chunking process.\\n\\nThis example demonstrates how the token chunker works with a realistic text sample that would be common in document processing and RAG (Retrieval-Augmented Generation) applications. The chunks will be created with specified token limits and overlap settings to optimize for both comprehension and processing efficiency. Each chunk will contain metadata about its position in the original text and token count for further processing. By using a robust chunking strategy, we can ensure that downstream models receive high-quality, context-rich input, improving the overall performance of NLP pipelines and applications.\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = helix.Chunk.sentence_chunk(massive_text_blob)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac704576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🦛 choooooooooooooooooooonk 100% • 2/2 docs chunked [00:00<00:00, 19.72doc/s] 🌱\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['First document to chunk with some content for testing.',\n",
       " 'Second document with different content for batch processing.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_chunks = helix.Chunk.sentence_chunk(texts)\n",
    "batch_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4314d2",
   "metadata": {},
   "source": [
    "### Recursive Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe9f47d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nThis is a massive text blob that we want to chunk into smaller pieces for processing. It contains multiple sentences and paragraphs that need to be divided appropriately to maintain context while fitting within token limits. When working with large documents, it is important to ensure that each chunk maintains enough context for downstream tasks, such as retrieval or summarization. Chunking strategies can vary depending on the use case, but the goal is always to balance context preservation with processing efficiency.\\n\\nThe chunker should handle overlaps properly to ensure no important information is lost at chunk boundaries. For example, if a sentence is split between two chunks, the overlap ensures that both chunks retain the full meaning of the text. This is especially important in applications like document question answering, where missing a single sentence could lead to incorrect answers. Additionally, chunkers may need to account for different languages, code blocks, or special formatting, which can add complexity to the chunking process.\\n\\nThis example demonstrates how the token chunker works with a realistic text sample that would be common in document processing and RAG (Retrieval-Augmented Generation) applications. The chunks will be created with specified token limits and overlap settings to optimize for both comprehension and processing efficiency. Each chunk will contain metadata about its position in the original text and token count for further processing. By using a robust chunking strategy, we can ensure that downstream models receive high-quality, context-rich input, improving the overall performance of NLP pipelines and applications.\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = helix.Chunk.recursive_chunk(massive_text_blob)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4633ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🦛 choooooooooooooooooooonk 100% • 2/2 docs chunked [00:00<00:00, 21.89doc/s] 🌱\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['First document to chunk with some content for testing.',\n",
       " 'Second document with different content for batch processing.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_chunks = helix.Chunk.recursive_chunk(texts)\n",
    "batch_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b328d9",
   "metadata": {},
   "source": [
    "### Code Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09bfcc7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ndef hello_world():\\n    print(\"Hello, Chonkie!\")\\n\\nclass MyClass:\\n    def __init__(self):\\n        self.value = 42\\n']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = helix.Chunk.code_chunk(code_sample, language=\"python\")\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca46a51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🦛 choooooooooooooooooooonk 100% • 2/2 docs chunked [00:00<00:00, 7212.90doc/s] 🌱\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['def func1():\\n    pass',\n",
       " 'const x = 10;\\nfunction add(a, b) { return a + b; }']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_chunks = helix.Chunk.code_chunk(code_samples, language=\"python\")\n",
    "batch_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceca787b",
   "metadata": {},
   "source": [
    "### Semantic Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ceea8c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gwuwong/Documents/helixdb/helix-py/.venv/lib/python3.13/site-packages/chonkie/embeddings/model2vec.py:63: RuntimeWarning: invalid value encountered in divide\n",
      "  return np.divide(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\nThis is a massive text blob that we want to chunk into smaller pieces for processing. It contains multiple sentences and paragraphs that need to be divided appropriately to maintain context while fitting within token limits. When working with large documents, it is important to ensure that each chunk maintains enough context for downstream tasks, such as retrieval or summarization.',\n",
       " ' Chunking strategies can vary depending on the use case, but the goal is always to balance context preservation with processing efficiency.\\n\\nThe chunker should handle overlaps properly to ensure no important information is lost at chunk boundaries. For example, if a sentence is split between two chunks, the overlap ensures that both chunks retain the full meaning of the text.',\n",
       " ' This is especially important in applications like document question answering, where missing a single sentence could lead to incorrect answers.',\n",
       " ' Additionally, chunkers may need to account for different languages, code blocks, or special formatting, which can add complexity to the chunking process.\\n\\nThis example demonstrates how the token chunker works with a realistic text sample that would be common in document processing and RAG (Retrieval-Augmented Generation) applications. The chunks will be created with specified token limits and overlap settings to optimize for both comprehension and processing efficiency. Each chunk will contain metadata about its position in the original text and token count for further processing. By using a robust chunking strategy, we can ensure that downstream models receive high-quality, context-rich input, improving the overall performance of NLP pipelines and applications.\\n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = helix.Chunk.semantic_chunk(massive_text_blob)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "174632c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🦛 choooooooooooooooooooonk 100% • 2/2 docs chunked [00:00<00:00, 10994.24doc/s] 🌱\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['First document to chunk with some content for testing.',\n",
       " 'Second document with different content for batch processing.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_chunks = helix.Chunk.semantic_chunk(texts)\n",
    "batch_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74c8cab",
   "metadata": {},
   "source": [
    "### Late Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad30713e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (300 > 256). Running this sequence through the model will result in indexing errors\n",
      "/Users/gwuwong/Documents/helixdb/helix-py/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\nThis is a massive text blob that we want to chunk into smaller pieces for processing. It contains multiple sentences and paragraphs that need to be divided appropriately to maintain context while fitting within token limits. When working with large documents, it is important to ensure that each chunk maintains enough context for downstream tasks, such as retrieval or summarization. Chunking strategies can vary depending on the use case, but the goal is always to balance context preservation with processing efficiency.\\n\\nThe chunker should handle overlaps properly to ensure no important information is lost at chunk boundaries. For example, if a sentence is split between two chunks, the overlap ensures that both chunks retain the full meaning of the text. This is especially important in applications like document question answering, where missing a single sentence could lead to incorrect answers. Additionally, chunkers may need to account for different languages, code blocks, or special formatting, which can add complexity to the chunking process.\\n\\nThis example demonstrates how the token chunker works with a realistic text sample that would be common in document processing and RAG (Retrieval-Augmented Generation) applications. The chunks will be created with specified token limits and overlap settings to optimize for both comprehension and processing efficiency. Each chunk will contain metadata about its position in the original text and token count for further processing. By using a robust chunking strategy, we can ensure that downstream models receive high-quality, context-rich input, improving the overall performance of NLP pipelines and applications.\\n']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = helix.Chunk.late_chunk(massive_text_blob)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "065c65e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🦛 choooooooooooooooooooonk 100% • 2/2 docs chunked [00:00<00:00, 17.94doc/s] 🌱\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['First document to chunk with some content for testing.',\n",
       " 'Second document with different content for batch processing.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_chunks = helix.Chunk.late_chunk(texts)\n",
    "batch_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2960f2",
   "metadata": {},
   "source": [
    "### Neural Chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76033b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " 'This is a massive text blob that we want to chunk into smaller pieces for processing. It contains multiple sentences and paragraphs that need to be divided appropriately to maintain context while fitting within token limits.',\n",
       " ' When working with large documents, it is important to ensure that each chunk maintains enough context for downstream tasks, such as retrieval or summarization. Chunking strategies can vary depending on the use case, but the goal is always to balance context preservation with processing efficiency.\\n',\n",
       " '\\nThe chunker should handle overlaps properly to ensure no important information is lost at chunk boundaries. For example, if a sentence is split between two chunks, the overlap ensures that both chunks retain the full meaning of the text.',\n",
       " ' This is especially important in applications like document question answering, where missing a single sentence could lead to incorrect answers. Additionally, chunkers may need to account for different languages, code blocks, or special formatting, which can add complexity to the chunking process.\\n\\n',\n",
       " 'This example demonstrates how the token chunker works with a realistic text sample that would be common in document processing and RAG (Retrieval-Augmented Generation) applications.',\n",
       " ' The chunks will be created with specified token limits and overlap settings to optimize for both comprehension and processing efficiency. Each chunk will contain metadata about its position in the original text and token count for further processing',\n",
       " '. By using a robust chunking strategy, we can ensure that downstream models receive high-quality, context-rich input, improving the overall performance of NLP pipelines and applications.\\n']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = helix.Chunk.neural_chunk(massive_text_blob)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bacacfac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "🦛 choooooooooooooooooooonk 100% • 2/2 docs chunked [00:00<00:00, 15.75doc/s] 🌱\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['First document to chunk with some content for testing.',\n",
       " 'Second document with different content for batch processing.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_chunks = helix.Chunk.neural_chunk(texts)\n",
    "batch_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdc0be1",
   "metadata": {},
   "source": [
    "### Slumber Chunker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7505e26b",
   "metadata": {},
   "source": [
    "You need to set an Gemini API key in your env to run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36fdf7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f3fdc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🦛 choooooooooooooooooooonk 100% • 36/36 splits processed [00:24<00:00,  1.48split/s] 🌱\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\nThis is a massive text blob that we want to chunk into smaller pieces for processing. Itcontainsmultiplesentencesandparagraphsthatneedtobedividedappropriatelytomaintaincontextwhilefittingwithintokenlimits.When working with large documents, it is important to ensure that each chunk maintains enough context for downstream tasks, such as retrieval or summarization. Chunking strategies can vary depending on the use case, but the goal is always to balance context preservation with processing efficiency.\\n\\nThe chunker should handle overlaps properly to ensure no important information is lost at chunk boundaries. For example, if a sentence is split between two chunks, the overlap ensures that both chunks retain the full meaning of the text. This is especially important in applications like document question answering, where missing a single sentence could lead to incorrect answers. ',\n",
       " 'Additionally, chunkers may need to account for different languages, code blocks, or special formatting, which can add complexity to the chunking process.\\n',\n",
       " '\\nThisexampledemonstrateshowthetokenchunkerworkswitharealistictextsamplethatwouldbecommonindocumentprocessingandRAG(Retrieval-Augmented Generation) applications. Thechunkswillbecreatedwithspecifiedtokenlimitsandoverlapsettingstooptimizeforbothcomprehensionandprocessingefficiency.Each chunk will contain metadata about its position in the original text and token count for further processing. ',\n",
       " 'By using a robust chunking strategy, we can ensure that downstream models receive high-quality, context-rich input, improving the overall performance of NLP pipelines and applications.\\n']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = helix.Chunk.slumber_chunk(massive_text_blob)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cf1fdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "🦛 choooooooooooooooooooonk 100% • 1/1 splits processed [00:06<00:00,  6.04s/split] 🌱\n",
      "🦛 choooooooooooooooooooonk 100% • 1/1 splits processed [00:05<00:00,  5.64s/split] 🌱\n",
      "🦛 choooooooooooooooooooonk 100% • 2/2 docs chunked [00:11<00:00,  5.84s/doc] 🌱\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['First document to chunk with some content for testing.',\n",
       " 'Second document with different content for batch processing.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_chunks = helix.Chunk.slumber_chunk(texts)\n",
    "batch_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edc2db3",
   "metadata": {},
   "source": [
    "### PDF to Markdown Converter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b715ccbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sample Document\\n\\nThis is a massive text blob that we want to chunk into smaller pieces for processing.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_path = \"sample.pdf\"\n",
    "markdown_text = helix.Chunk.pdf_markdown(pdf_path)\n",
    "markdown_text[:102]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b90a01a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample Document\\n\\nThis is a massive text blob that we want to chunk into smaller pieces for processing. It contains\\n\\nmultiple  sentences  and  paragraphs  that need to be divided appropriately to maintain context\\n\\nwhile ﬁtting within token limits. When working with large documents, it is important to ensure\\n\\nthat  each  chunk  maintains  enough  context  for  downstream  tasks,  such  as  retrieval  or\\n\\nsummarization. Chunking strategies can vary depending on the use case, but the goal is always\\n\\nto balance context preservation with processing eﬃciency.\\n\\nThe  chunker  should  handle  overlaps  properly  to  ensure  no  important  information  is  lost  at\\n\\nchunk boundaries. For example, if a sentence is split between two chunks, the overlap ensures\\n\\nthat both chunks retain the full meaning of the text. This is especially important in applications\\n\\nlike  document  question  answering,  where  missing  a  single  sentence  could  lead  to  incorrect\\n\\nanswers.  Additionally,  chunkers  may  need  to  account  for  diﬀerent languages, code blocks, or\\n\\nspecial formatting, which can add complexity to the chunking process.\\n\\nThis  example  demonstrates  how  the  token  chunker  works  with  a  realistic  text  sample  that\\n\\nwould  be  common  in  document  processing  and  RAG  (Retrieval-Augmented  Generation)\\n\\napplications.  The  chunks  will  be  created  with  speciﬁed  token  limits  and  overlap  settings  to\\n\\noptimize  for both comprehension and processing eﬃciency. Each chunk will contain metadata\\n\\nabout its position in the original text and token count for further processing. By using a robust\\n\\nchunking  strategy,  we  can  ensure  that  downstream  models  receive  high-quality,  context-rich\\n\\ninput, improving the overall performance of NLP pipelines and applications.\\n\\n']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = helix.Chunk.recursive_chunk(markdown_text, recipe=\"markdown\")\n",
    "chunks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
