Metadata-Version: 2.4
Name: langchain-llm7
Version: 2025.9.111718
Summary: A LangChain wrapper for LLM7 API
Home-page: https://github.com/chigwell/langchain_llm7
Author: Eugene Evstafev
Author-email: chigwel@gmail.com
License: Apache-2.0
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Programming Language :: Python :: 3
Classifier: Operating System :: OS Independent
Requires-Python: >=3.10
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: tokeniser>=0.0.3
Requires-Dist: requests==2.32.3
Requires-Dist: pydantic==2.11.3
Requires-Dist: langchain-core==0.3.51
Dynamic: author
Dynamic: author-email
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: license
Dynamic: license-file
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

[![PyPI version](https://badge.fury.io/py/langchain-llm7.svg)](https://badge.fury.io/py/langchain-llm7)
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Downloads](https://static.pepy.tech/badge/langchain-llm7)](https://pepy.tech/project/langchain-llm7)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-blue)](https://www.linkedin.com/in/eugene-evstafev-716669181/)

# LangChain LLM7 Integration

Official LangChain compatibility layer for LLM7 API services.

## Installation

```bash
pip install langchain-llm7
```

## Features

- ðŸš€ Native integration with LangChain's BaseChatModel interface
- âš¡ Support for both streaming and non-streaming responses
- ðŸ”§ Customizable model parameters (temperature, max_tokens, stop sequences)
- ðŸ“Š Token usage metadata tracking
- ðŸ›  Robust error handling and retry mechanisms

## Usage

### Basic Implementation

```python
from langchain_llm7 import ChatLLM7
from langchain_core.messages import HumanMessage

# Initialize with default parameters
llm = ChatLLM7()

# Basic invocation
response = llm.invoke([HumanMessage(content="Hello!")])
print(response.content)
```

### Streaming Responses

```python
# Enable streaming
llm = ChatLLM7(streaming=True)

for chunk in llm.stream([HumanMessage(content="Tell me about quantum computing")]):
    print(chunk.content, end="", flush=True)
```

### Advanced Configuration

```python
# Custom model configuration
llm = ChatLLM7(
    model="llama-3.3-70b-instruct-fp8-fast",
    temperature=0.7,
    max_tokens=500,
    stop=["\n", "Observation:"],
    timeout=45
)
```

## Parameters

| Parameter     | Description                                  | Default                      |
|---------------|----------------------------------------------|------------------------------|
| `model`       | Model version to use                         | "gpt-4o-mini-2024-07-18"     |
| `base_url`    | API endpoint URL                             | "https://api.llm7.io/v1"      |
| `temperature` | Sampling temperature (0.0-2.0)              | 1.0                          |
| `max_tokens`  | Maximum number of tokens to generate         | None                         |
| `timeout`     | Request timeout in seconds                  | 120                          |
| `stop`        | Stop sequences for response generation      | None                         |
| `streaming`   | Enable streaming response mode              | False                        |

## Error Handling

The library provides detailed error messages for:
- API communication failures
- Invalid message formats
- Unsupported message types
- Response parsing errors

```python
try:
    llm.invoke([{"invalid": "message"}])
except ValueError as e:
    print(f"Error: {e}")
```

## Testing

To run the test suite:

```bash
pip install pytest
pytest tests/
```

## Documentation

For complete documentation see:
- [LangChain Core Documentation](https://python.langchain.com)
- [LLM7 API Reference](https://api.llm7.io/)

## Contributing

Contributions are welcome! Please open an issue or submit a PR:
- [GitHub Repository](https://github.com/chigwell/langchain_llm7)

## License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

## Contact

Eugene Evstafev  

[LinkedIn Profile](https://www.linkedin.com/in/eugene-evstafev-716669181/)
