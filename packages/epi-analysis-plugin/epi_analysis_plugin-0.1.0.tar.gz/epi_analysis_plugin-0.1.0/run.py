import random
import tempfile
from datetime import date, timedelta
from pathlib import Path

import polars as pl

from did_demo import run as run_did_demo
from matching_plugin import (
    complete_scd_matching_workflow,
    extract_ethnicity_categories,
    extract_highest_education_level,
    extract_highest_education_level_batched,
    extract_temporal_data,
    extract_temporal_data_batched,
)

print("=" * 80)
print("Epidemiological Analysis Plugin - Temporal Extraction Demo")
print("=" * 80)

# Set random seed for reproducible results
random.seed(42)
base_date = date(1995, 1, 1)
sample_size = 500

# Create sample MFR data (birth registry) with birth_type data
print("Creating sample datasets...")

# Generate birth_type data with realistic distribution
birth_types = []
for _ in range(sample_size):
    # Realistic birth type distribution: ~95% singleton, 4% doubleton, 1% higher multiples
    rand = random.random()
    if rand < 0.95:
        birth_types.append("singleton")
    elif rand < 0.99:
        birth_types.append("doubleton")
    elif rand < 0.997:
        birth_types.append("tripleton")
    elif rand < 0.9995:
        birth_types.append("quadleton")
    else:
        birth_types.append("multiple")

mfr_sample = pl.DataFrame(
    {
        "PNR": [f"person_{i:04d}" for i in range(1, sample_size + 1)],
        "FOEDSELSDATO": [
            base_date + timedelta(days=random.randint(0, 365 * 5))
            for _ in range(sample_size)
        ],
        "CPR_MODER": [f"mother_{i:04d}" for i in range(1, sample_size + 1)],
        "CPR_FADER": [f"father_{i:04d}" for i in range(1, sample_size + 1)],
        "MODER_FOEDSELSDATO": [
            base_date - timedelta(days=random.randint(365 * 20, 365 * 35))
            for _ in range(sample_size)
        ],
        "FADER_FOEDSELSDATO": [
            base_date - timedelta(days=random.randint(365 * 20, 365 * 35))
            for _ in range(sample_size)
        ],
        "PARITET": [random.randint(1, 5) for _ in range(sample_size)],
        "birth_type": birth_types,
    }
)

# Create sample LPR data with SCD cases
early_scd_cases = random.sample(range(sample_size), 25)
late_scd_cases = random.sample(
    [i for i in range(sample_size) if i not in early_scd_cases], 15
)

lpr_sample = pl.DataFrame(
    {
        "PNR": [f"person_{i:04d}" for i in range(1, sample_size + 1)],
        "SCD_STATUS": [
            "SCD"
            if i in early_scd_cases
            else "SCD_LATE"
            if i in late_scd_cases
            else "NO_SCD"
            for i in range(sample_size)
        ],
        "SCD_DATE": [
            base_date + timedelta(days=random.randint(365, 365 * 3))
            if i in early_scd_cases
            else base_date + timedelta(days=random.randint(365 * 4, 365 * 6))
            if i in late_scd_cases
            else None
            for i in range(sample_size)
        ],
        "ICD_CODE": [
            f"D57.{random.randint(0, 9)}"
            if i in early_scd_cases or i in late_scd_cases
            else None
            for i in range(sample_size)
        ],
    }
)

print(f"‚úì Created MFR data: {len(mfr_sample)} records")
print(f"‚úì Created LPR data: {len(lpr_sample)} records")
print(f"  - Early SCD cases: {(lpr_sample['SCD_STATUS'] == 'SCD').sum()}")
print(f"  - Late SCD cases: {(lpr_sample['SCD_STATUS'] == 'SCD_LATE').sum()}")
print(f"  - Controls: {(lpr_sample['SCD_STATUS'] == 'NO_SCD').sum()}")

# Show birth type distribution
birth_type_dist = mfr_sample.group_by("birth_type").len().sort("len", descending=True)
print("\nBirth type distribution:")
for row in birth_type_dist.iter_rows():
    birth_type, count = row
    pct = (count / sample_size) * 100
    print(f"  - {birth_type}: {count} ({pct:.1f}%)")

# Create sample registry files for temporal extraction testing
print("\nCreating sample registry files for temporal extraction...")

with tempfile.TemporaryDirectory() as temp_dir:
    temp_path = Path(temp_dir)

    # Create sample registry files for different years (1995-2002)
    # to match the temporal range of case diagnosis dates
    registry_years = [1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002]

    for year in registry_years:
        # Create sample prescription data for each year
        year_prescriptions = []

        # Random subset of people get prescriptions each year
        # Ensure SCD cases get some prescriptions for testing
        scd_case_pnrs = [f"person_{i:04d}" for i in early_scd_cases + late_scd_cases]
        regular_people = [f"person_{i:04d}" for i in range(1, sample_size + 1)]

        # Always include some SCD cases + random others
        guaranteed_prescriptions = random.sample(
            scd_case_pnrs, min(10, len(scd_case_pnrs))
        )
        additional_prescriptions = random.sample(
            regular_people, random.randint(90, 190)
        )
        people_with_prescriptions = list(
            set(guaranteed_prescriptions + additional_prescriptions)
        )

        for pnr in people_with_prescriptions:
            # Each person can have multiple prescriptions
            num_prescriptions = random.randint(1, 5)
            for _ in range(num_prescriptions):
                atc_codes = [
                    "A02BC01",
                    "A10BA02",
                    "C09AA02",
                    "N02BE01",
                    "R03AC02",
                    "J01CA04",
                ]
                year_prescriptions.append(
                    {
                        "PNR": pnr,
                        "ATC_CODE": random.choice(atc_codes),
                        "STRENGTH": f"{random.randint(5, 500)}mg",
                        "DOSE": random.randint(1, 3),
                        "PRESCRIPTION_DATE": date(
                            year, random.randint(1, 12), random.randint(1, 28)
                        ),
                    }
                )

        # Create DataFrame and save as parquet
        registry_df = pl.DataFrame(year_prescriptions)
        if len(registry_df) > 0:
            registry_file = temp_path / f"prescription_registry_{year}.parquet"
            registry_df.write_parquet(registry_file)
            print(f"  ‚úì Created {registry_file.name}: {len(registry_df)} prescriptions")

            # Debug: Show sample of what's being created for one year
            if year == 1996:  # Show sample for a year that should have matches
                print(f"    Sample registry data for {year}:")
                print(registry_df.head(3))
        else:
            print(f"  ‚ö†Ô∏è  No prescriptions generated for {year}")

    # Create sample BEF registry files (child + parents)
    print("\nCreating sample BEF registry files for ethnicity extraction...")
    bef_years = registry_years
    for year in bef_years:
        rows = []
        for i in range(1, sample_size + 1):
            pnr = f"person_{i:04d}"
            mother = f"mother_{i:04d}"
            father = f"father_{i:04d}"

            # Assign origin codes: "0"=Danish, "1"=Western, "2"=Non-Western
            if i % 5 == 0:
                child_opr = "0"
            elif i % 5 in (1, 2):
                child_opr = "1"
            else:
                child_opr = "2"

            # Immigration status: 1=Danish, 2=Immigrant, 3=Descendant
            if child_opr == "0":
                ie_type = 1
            else:
                ie_type = 2 if year <= 1997 else 3

            # Parents' own origins (vary to produce mixed backgrounds)
            mother_opr = "0" if i % 3 == 0 else child_opr
            father_opr = "0" if i % 4 == 0 else child_opr

            # Child row (BEF needs PNR, ARET, OPR_LAND, IE_TYPE only)
            rows.append(
                {
                    "PNR": pnr,
                    "ARET": year,
                    "OPR_LAND": child_opr,
                    "IE_TYPE": ie_type,
                }
            )

            # Mother row (only her own origin is needed for lookups)
            rows.append(
                {
                    "PNR": mother,
                    "ARET": year,
                    "OPR_LAND": mother_opr,
                    "IE_TYPE": 1,
                }
            )

            # Father row
            rows.append(
                {
                    "PNR": father,
                    "ARET": year,
                    "OPR_LAND": father_opr,
                    "IE_TYPE": 1,
                }
            )

        bef_df = pl.DataFrame(rows)
        bef_file = temp_path / f"bef_registry_{year}.parquet"
        bef_df.write_parquet(bef_file)
        print(f"  ‚úì Created {bef_file.name}: {len(bef_df)} rows (child+parents)")

    # Prepare cohort for ethnicity (ensure non-null index dates)
    ethnicity_input = mfr_sample.select(
        [
            pl.col("PNR"),
            (pl.col("FOEDSELSDATO") + pl.duration(days=365 * 2)).alias("INDEX_DATE"),
            pl.col("CPR_MODER"),
            pl.col("CPR_FADER"),
        ]
    )

    # Run ethnicity extraction using temporal join and parental lookups
    try:
        ethnicity_results = extract_ethnicity_categories(
            df=ethnicity_input,
            identifier_col="PNR",
            index_date_col="INDEX_DATE",
            bef_registry_pattern=str(temp_path / "bef_registry_*.parquet"),
            temporal_range=(-1, 1),
        )
        print("\n‚úì Ethnicity extraction completed!")
        print(ethnicity_results.head(5))
    except Exception as e:
        ethnicity_results = None
        print(f"‚ùå Ethnicity extraction failed: {e}")
        import traceback

        traceback.print_exc()

    print("\n" + "=" * 80)
    print("STEP 1: Testing Case-Control Matching with Birth Type")
    print("=" * 80)

    # Test 1A: Matching WITHOUT birth_type constraint (baseline)
    print("\nTest 1A: Baseline matching without birth_type constraint")
    try:
        matched_results_baseline = complete_scd_matching_workflow(
            mfr_data=mfr_sample,
            lpr_data=lpr_sample,
            matching_ratio=3,
            birth_date_window_days=90,
            match_parent_birth_dates=True,
            parent_birth_date_window_days=730,
            match_parity=True,
            match_birth_type=False,  # Explicitly disable birth_type matching
            algorithm="spatial_index",
        )

        print("\n‚úì Baseline matching completed successfully!")
        print(f"Total matched records: {len(matched_results_baseline)}")
        print(f"Match groups: {matched_results_baseline['MATCH_INDEX'].n_unique()}")
        print(f"Cases: {(matched_results_baseline['ROLE'] == 'case').sum()}")
        print(f"Controls: {(matched_results_baseline['ROLE'] == 'control').sum()}")

        # Show birth_type distribution in matched results (should be mixed)
        combined_baseline = mfr_sample.join(
            matched_results_baseline, on="PNR", how="inner"
        )
        print("\nBirth type distribution in baseline matched results:")
        baseline_birth_dist = (
            combined_baseline.group_by(["ROLE", "birth_type"])
            .len()
            .sort(["ROLE", "len"], descending=[False, True])
        )
        for row in baseline_birth_dist.iter_rows():
            role, birth_type, count = row
            print(f"  - {role} {birth_type}: {count}")

        matched_results = matched_results_baseline  # Keep for temporal extraction test

    except Exception as e:
        print(f"‚ùå Baseline matching failed: {e}")
        import traceback

        traceback.print_exc()
        matched_results_baseline = None
        matched_results = None

    # Test 1B: Matching WITH birth_type constraint (NEW FEATURE)
    print("\nTest 1B: NEW FEATURE - Matching with birth_type constraint")
    try:
        matched_results_birth_type = complete_scd_matching_workflow(
            mfr_data=mfr_sample,
            lpr_data=lpr_sample,
            matching_ratio=3,
            birth_date_window_days=90,
            match_parent_birth_dates=True,
            parent_birth_date_window_days=730,
            match_parity=True,
            match_birth_type=True,  # Enable birth_type matching
            algorithm="spatial_index",
        )

        print("\n‚úì Birth-type constrained matching completed successfully!")
        print(f"Total matched records: {len(matched_results_birth_type)}")
        print(f"Match groups: {matched_results_birth_type['MATCH_INDEX'].n_unique()}")
        print(f"Cases: {(matched_results_birth_type['ROLE'] == 'case').sum()}")
        print(f"Controls: {(matched_results_birth_type['ROLE'] == 'control').sum()}")

        # Verify birth_type matching constraint works correctly
        combined_birth_type = mfr_sample.join(
            matched_results_birth_type, on="PNR", how="inner"
        )
        print("\nüîç Verifying birth_type matching constraint:")

        # Check each match group to ensure cases and controls have same birth_type
        match_groups = (
            combined_birth_type.group_by("MATCH_INDEX")
            .agg(
                [
                    pl.col("birth_type").n_unique().alias("unique_birth_types"),
                    pl.col("birth_type").first().alias("birth_type"),
                    pl.col("ROLE").n_unique().alias("roles_count"),
                ]
            )
            .filter(pl.col("roles_count") > 1)
        )  # Only groups with both cases and controls

        violations = match_groups.filter(pl.col("unique_birth_types") > 1)
        if len(violations) == 0:
            print("  ‚úÖ SUCCESS: All match groups have consistent birth_type!")
        else:
            print(
                f"  ‚ùå VIOLATION: Found {len(violations)} match groups with mixed birth_types"
            )

        print("\nBirth type distribution in birth-type constrained results:")
        birth_type_dist = (
            combined_birth_type.group_by(["ROLE", "birth_type"])
            .len()
            .sort(["ROLE", "len"], descending=[False, True])
        )
        for row in birth_type_dist.iter_rows():
            role, birth_type, count = row
            print(f"  - {role} {birth_type}: {count}")

        # Compare effectiveness
        if matched_results_baseline is not None:
            baseline_cases = (matched_results_baseline["ROLE"] == "case").sum()
            birth_type_cases = (matched_results_birth_type["ROLE"] == "case").sum()
            print("\nüìä Matching effectiveness comparison:")
            print(f"  - Baseline (no birth_type): {baseline_cases} cases matched")
            print(f"  - Birth-type constrained: {birth_type_cases} cases matched")
            if birth_type_cases < baseline_cases:
                reduction = baseline_cases - birth_type_cases
                pct_reduction = (reduction / baseline_cases) * 100
                print(
                    f"  - Impact: {reduction} fewer cases ({pct_reduction:.1f}% reduction) due to birth_type constraint"
                )
                print(
                    "    ‚Üí This shows the constraint is working - fewer matches but higher quality!"
                )
            else:
                print("  - No reduction in matched cases")

    except Exception as e:
        print(f"‚ùå Birth-type constrained matching failed: {e}")
        import traceback

        traceback.print_exc()
        matched_results_birth_type = None

    # Test 1C: Test the advanced partitioned_parallel algorithm with birth_type
    print("\nTest 1C: Advanced algorithm (partitioned_parallel) with birth_type")
    try:
        matched_results_advanced = complete_scd_matching_workflow(
            mfr_data=mfr_sample,
            lpr_data=lpr_sample,
            matching_ratio=3,
            birth_date_window_days=90,
            match_parent_birth_dates=True,
            parent_birth_date_window_days=730,
            match_parity=True,
            match_birth_type=True,
            algorithm="partitioned_parallel",  # Use most advanced algorithm
        )

        print("\n‚úì Advanced algorithm matching completed successfully!")
        print(f"Total matched records: {len(matched_results_advanced)}")
        print(f"Match groups: {matched_results_advanced['MATCH_INDEX'].n_unique()}")
        print(f"Cases: {(matched_results_advanced['ROLE'] == 'case').sum()}")
        print(f"Controls: {(matched_results_advanced['ROLE'] == 'control').sum()}")

        # Verify consistency between algorithms
        if matched_results_birth_type is not None:
            spatial_cases = (matched_results_birth_type["ROLE"] == "case").sum()
            advanced_cases = (matched_results_advanced["ROLE"] == "case").sum()
            print("\nüîÑ Algorithm consistency check:")
            print(f"  - Spatial index: {spatial_cases} cases")
            print(f"  - Partitioned parallel: {advanced_cases} cases")
            if (
                abs(spatial_cases - advanced_cases) <= 1
            ):  # Allow small differences due to algorithmic details
                print("  ‚úÖ Algorithms produce consistent results!")
            else:
                print("  ‚ö†Ô∏è  Significant difference between algorithms")

    except Exception as e:
        print(f"‚ùå Advanced algorithm matching failed: {e}")
        import traceback

        traceback.print_exc()

    print("\n" + "=" * 80)
    print("STEP 2: Testing Temporal Data Extraction")
    print("=" * 80)

    # Create test DataFrame with cases and their diagnosis dates
    cases_df = (
        lpr_sample.filter(pl.col("SCD_STATUS") != "NO_SCD")
        .select(["PNR", "SCD_DATE", "ICD_CODE"])
        .rename({"SCD_DATE": "diagnosis_date"})
    )

    print(f"Testing temporal extraction with {len(cases_df)} cases...")

    # Debug: Show what cases we're trying to extract for
    print("\nSample cases to extract data for:")
    sample_cases = cases_df.head(3)
    print(sample_cases)

    # Debug: Show what years we should be looking for
    case_years = (
        cases_df.with_columns(
            pl.col("diagnosis_date").dt.year().alias("diagnosis_year")
        )
        .select(["PNR", "diagnosis_year"])
        .head(3)
    )
    print("\nExpected years for temporal extraction (¬±1 year):")
    for row in case_years.iter_rows():
        pnr, diag_year = row
        target_years = [diag_year - 1, diag_year, diag_year + 1]
        print(f"  - {pnr}: diagnosis {diag_year} ‚Üí looking for {target_years}")

    # Test 1: Basic temporal extraction
    print("\nTest 1: Basic temporal extraction (¬±1 year around diagnosis)")

    # Debug: Show what registry files were actually created
    registry_pattern = str(temp_path / "prescription_registry_*.parquet")
    print(f"Registry pattern: {registry_pattern}")
    registry_files = list(temp_path.glob("prescription_registry_*.parquet"))
    print(f"Found {len(registry_files)} registry files:")
    for f in sorted(registry_files):
        print(f"  - {f.name}")

    try:
        temporal_data = extract_temporal_data(
            df=cases_df,
            identifier_col="PNR",
            index_date_col="diagnosis_date",
            registry_path_pattern=registry_pattern,
            variable_col="ATC_CODE",
            temporal_range=(-1, 1),  # 1 year before to 1 year after
            additional_cols=["STRENGTH", "DOSE"],
            use_cache=True,
        )

        print("‚úì Basic temporal extraction completed!")
        print(f"  Total extracted records: {len(temporal_data)}")
        print(f"  Unique patients: {temporal_data['PNR'].n_unique()}")
        print(
            f"  Year range: {temporal_data['ARET'].min()} - {temporal_data['ARET'].max()}"
        )
        print(
            f"  Relative years: {sorted(temporal_data['RELATIVE_YEAR'].unique().to_list())}"
        )

        # Show sample results
        print("\nSample temporal extraction results:")
        print(temporal_data.head(8))

        if len(temporal_data) > 0:
            # Show distribution by relative year
            print("\nDistribution by relative year:")
            rel_year_dist = (
                temporal_data.group_by("RELATIVE_YEAR").len().sort("RELATIVE_YEAR")
            )
            print(rel_year_dist)

    except Exception as e:
        print(f"‚ùå Basic temporal extraction failed: {e}")
        import traceback

        traceback.print_exc()
        temporal_data = None

    # Test 2: Extended temporal range
    print("\nTest 2: Extended temporal extraction (¬±2 years around diagnosis)")
    try:
        temporal_data_extended = extract_temporal_data(
            df=cases_df,
            identifier_col="PNR",
            index_date_col="diagnosis_date",
            registry_path_pattern=registry_pattern,
            variable_col="ATC_CODE",
            temporal_range=(-2, 2),  # 2 years before to 2 years after
            additional_cols=["STRENGTH"],
            use_cache=True,  # Should use cached registry files
        )

        print("‚úì Extended temporal extraction completed!")
        print(f"  Total extracted records: {len(temporal_data_extended)}")
        print(
            f"  Year range: {temporal_data_extended['ARET'].min()} - {temporal_data_extended['ARET'].max()}"
        )
        print(
            f"  Relative years: {sorted(temporal_data_extended['RELATIVE_YEAR'].unique().to_list())}"
        )

    except Exception as e:
        print(f"‚ùå Extended temporal extraction failed: {e}")
        import traceback

        traceback.print_exc()

    # Test 3: Batched processing
    print("\nTest 3: Batched temporal extraction")
    if len(cases_df) >= 10:
        try:
            temporal_data_batched = extract_temporal_data_batched(
                df=cases_df,
                batch_size=5,  # Small batch size to test batching
                identifier_col="PNR",
                index_date_col="diagnosis_date",
                registry_path_pattern=registry_pattern,
                variable_col="ATC_CODE",
                temporal_range=(-1, 1),
                use_cache=True,
            )

            print("‚úì Batched temporal extraction completed!")
            print(f"  Total extracted records: {len(temporal_data_batched)}")
            print(f"  Batches processed: {(len(cases_df) + 4) // 5}")

            # Verify results are identical to non-batched
            if temporal_data is not None and len(temporal_data_batched) == len(
                temporal_data
            ):
                print("  ‚úì Batched results match non-batched results")
            else:
                print("  ‚ö†Ô∏è  Batched results differ from non-batched")

        except Exception as e:
            print(f"‚ùå Batched temporal extraction failed: {e}")
            import traceback

            traceback.print_exc()
    else:
        print("  Skipped (insufficient cases for batching test)")

    print("\n" + "=" * 80)
    print("STEP 3: Testing Education Level Extraction")
    print("=" * 80)

    # Create sample UDDF education data
    print("Creating sample UDDF education data...")

    # Generate realistic education data for our sample population
    education_records = []

    # HFAUDD codes based on the categorization file
    education_codes = {
        # Short education (10, 15)
        "short": ["1000", "1006", "1007", "1008", "1009", "1010", "1100", "1500"],
        # Medium education (20, 30, 35) + special middle school codes
        "medium": [
            "2000",
            "2010",
            "3000",
            "3010",
            "3500",
            "1021",
            "1022",
            "1023",
            "1121",
            "1122",
            "1123",
            "1423",
            "1522",
            "1523",
            "1721",
            "1722",
            "1723",
        ],
        # Long education (40, 50, 60, 70, 80)
        "long": [
            "4000",
            "4010",
            "5000",
            "5010",
            "6000",
            "6010",
            "7000",
            "7010",
            "8000",
            "8010",
        ],
        # Unknown/missing (90)
        "unknown": ["9000", "9010"],
    }

    # Education distribution (approximately realistic for Danish population)
    education_distribution = {
        "short": 0.25,  # 25% short education
        "medium": 0.45,  # 45% medium education
        "long": 0.27,  # 27% long education
        "unknown": 0.03,  # 3% unknown/missing
    }

    # Generate education records for each person
    for i in range(1, sample_size + 1):
        pnr = f"person_{i:04d}"

        # Determine education level based on distribution
        rand = random.random()
        cumulative = 0
        education_level = "short"
        for level, prob in education_distribution.items():
            cumulative += prob
            if rand <= cumulative:
                education_level = level
                break

        # Select a random HFAUDD code from the chosen level
        hfaudd_code = random.choice(education_codes[education_level])

        # Generate temporal validity dates
        # Most people complete education between ages 15-30
        birth_date = base_date + timedelta(
            days=random.randint(0, 365 * 5)
        )  # Born 1995-2000
        education_start_age = random.randint(15, 25)  # Start education 15-25
        education_duration = random.randint(1, 8)  # 1-8 years duration

        hf_vfra = birth_date + timedelta(days=education_start_age * 365)
        hf_vtil = hf_vfra + timedelta(days=education_duration * 365)

        education_records.append(
            {
                "PNR": pnr,
                "HFAUDD": hfaudd_code,
                "HF_VFRA": hf_vfra,  # Keep as date objects
                "HF_VTIL": hf_vtil,  # Keep as date objects
            }
        )

        # Some people have multiple education records (e.g., additional qualifications)
        if random.random() < 0.15:  # 15% get additional education
            # Add a higher level education
            if education_level == "short":
                higher_level = "medium"
            elif education_level == "medium":
                higher_level = "long"
            else:
                higher_level = education_level  # Already at highest level

            if higher_level != education_level:
                higher_hfaudd = random.choice(education_codes[higher_level])
                later_start = hf_vtil + timedelta(days=random.randint(365, 365 * 3))
                later_duration = random.randint(2, 5)
                later_end = later_start + timedelta(days=later_duration * 365)

                education_records.append(
                    {
                        "PNR": pnr,
                        "HFAUDD": higher_hfaudd,
                        "HF_VFRA": later_start,  # Keep as date objects
                        "HF_VTIL": later_end,  # Keep as date objects
                    }
                )

    # Create UDDF DataFrame
    uddf_sample = pl.DataFrame(education_records)

    print(f"‚úì Created UDDF education data: {len(uddf_sample)} records")
    print(f"  - Unique individuals: {uddf_sample['PNR'].n_unique()}")
    print(
        f"  - Multiple education records: {len(uddf_sample) - uddf_sample['PNR'].n_unique()}"
    )

    # Show sample of education codes
    print("\nSample education records:")
    sample_uddf = uddf_sample.head(5)
    print(sample_uddf)

    # Show education code distribution
    print("\nEducation code distribution (sample):")
    code_dist = (
        uddf_sample.group_by("HFAUDD").len().sort("len", descending=True).head(10)
    )
    print(code_dist)

    # Save UDDF data to temporary file
    uddf_file = temp_path / "uddf_sample.parquet"
    uddf_sample.write_parquet(uddf_file)
    print(f"‚úì Saved UDDF data to {uddf_file.name}")

    # Test 1: Basic education level extraction
    print("\nTest 1: Basic education level extraction")

    # Create test DataFrame with some individuals and their index dates
    test_individuals = pl.DataFrame(
        {
            "PNR": [f"person_{i:04d}" for i in range(1, 21)],  # First 20 people
            "index_date": [
                base_date + timedelta(days=random.randint(365 * 20, 365 * 25))
                for _ in range(20)
            ],  # Index dates when they're adults
        }
    )

    print(f"Testing education extraction for {len(test_individuals)} individuals...")
    print("Sample test data:")
    print(test_individuals.head(3))

    try:
        education_results = extract_highest_education_level(
            df=test_individuals,
            identifier_col="PNR",
            index_date_col="index_date",
            uddf_file_path=str(uddf_file),
        )

        print("\n‚úì Basic education extraction completed!")
        print(f"  Total results: {len(education_results)}")
        print(f"  Unique individuals: {education_results['PNR'].n_unique()}")

        # Show sample results
        print("\nSample education extraction results:")
        print(education_results.head(8))

        # Show education level distribution
        print("\nEducation level distribution:")
        level_dist = (
            education_results.group_by("highest_education_level")
            .len()
            .sort("len", descending=True)
        )
        for row in level_dist.iter_rows():
            level, count = row
            pct = (count / len(education_results)) * 100
            print(f"  - {level}: {count} ({pct:.1f}%)")

    except Exception as e:
        print(f"‚ùå Basic education extraction failed: {e}")
        import traceback

        traceback.print_exc()
        education_results = None

    # Test 2: Special middle school code testing
    print("\nTest 2: Verifying middle school code handling")

    # Create a test case specifically with middle school codes
    middle_school_test = pl.DataFrame(
        {
            "PNR": ["test_001", "test_002", "test_003", "test_004"],
            "index_date": [base_date + timedelta(days=365 * 20) for _ in range(4)],
        }
    )

    # Create UDDF data with specific middle school codes
    middle_school_uddf = pl.DataFrame(
        {
            "PNR": ["test_001", "test_002", "test_003", "test_004"],
            "HFAUDD": [
                "1021",
                "1122",
                "1523",
                "2000",
            ],  # First 3 are middle school, last is regular medium
            "HF_VFRA": [base_date + timedelta(days=365 * 15)]
            * 4,  # Keep as date objects
            "HF_VTIL": [base_date + timedelta(days=365 * 25)]
            * 4,  # Keep as date objects
        }
    )

    # Save test UDDF
    test_uddf_file = temp_path / "middle_school_test.parquet"
    middle_school_uddf.write_parquet(test_uddf_file)

    try:
        middle_school_results = extract_highest_education_level(
            df=middle_school_test,
            identifier_col="PNR",
            index_date_col="index_date",
            uddf_file_path=str(test_uddf_file),
        )

        print("\n‚úì Middle school code test completed!")
        print("Results (all should be 'medium' due to special middle school handling):")
        print(middle_school_results)

        # Verify all middle school codes are classified as medium
        medium_count = (
            middle_school_results["highest_education_level"] == "medium"
        ).sum()
        if medium_count == 4:
            print(
                "  ‚úÖ SUCCESS: All middle school codes correctly classified as 'medium'"
            )
        else:
            print(
                f"  ‚ùå ISSUE: Only {medium_count}/4 middle school codes classified as 'medium'"
            )

    except Exception as e:
        print(f"‚ùå Middle school code test failed: {e}")
        import traceback

        traceback.print_exc()

    # Test 3: Temporal validity testing
    print("\nTest 3: Temporal validity testing")

    # Create test with index dates outside education validity periods
    temporal_test = pl.DataFrame(
        {
            "PNR": ["temp_001", "temp_002"],
            "index_date": [
                base_date + timedelta(days=365 * 10),  # Before education starts
                base_date + timedelta(days=365 * 35),  # After education ends
            ],
        }
    )

    # Create UDDF with specific validity periods
    temporal_uddf = pl.DataFrame(
        {
            "PNR": ["temp_001", "temp_002"],
            "HFAUDD": ["5000", "6000"],  # Long education codes
            "HF_VFRA": [base_date + timedelta(days=365 * 15)]
            * 2,  # Keep as date objects
            "HF_VTIL": [base_date + timedelta(days=365 * 25)]
            * 2,  # Keep as date objects
        }
    )

    temporal_uddf_file = temp_path / "temporal_test.parquet"
    temporal_uddf.write_parquet(temporal_uddf_file)

    try:
        temporal_results = extract_highest_education_level(
            df=temporal_test,
            identifier_col="PNR",
            index_date_col="index_date",
            uddf_file_path=str(temporal_uddf_file),
        )

        print("\n‚úì Temporal validity test completed!")
        print("Results (should show 'unknown' for dates outside validity period):")
        print(temporal_results)

        unknown_count = (temporal_results["highest_education_level"] == "unknown").sum()
        print(
            f"  - {unknown_count}/2 individuals have 'unknown' education (expected due to temporal invalidity)"
        )

    except Exception as e:
        print(f"‚ùå Temporal validity test failed: {e}")
        import traceback

        traceback.print_exc()

    # Test 4: Batched processing
    print("\nTest 4: Batched education extraction")

    if len(test_individuals) >= 10:
        try:
            education_batched = extract_highest_education_level_batched(
                df=test_individuals,
                batch_size=5,  # Small batch size to test batching
                identifier_col="PNR",
                index_date_col="index_date",
                uddf_file_path=str(uddf_file),
            )

            print("\n‚úì Batched education extraction completed!")
            print(f"  Total results: {len(education_batched)}")
            print(f"  Batches processed: {(len(test_individuals) + 4) // 5}")

            # Verify results match non-batched
            if education_results is not None and len(education_batched) == len(
                education_results
            ):
                print("  ‚úÖ Batched results match non-batched results")
            else:
                print("  ‚ö†Ô∏è  Batched results differ from non-batched")

        except Exception as e:
            print(f"‚ùå Batched education extraction failed: {e}")
            import traceback

            traceback.print_exc()
    else:
        print("  Skipped (insufficient individuals for batching test)")

print("\n" + "=" * 80)
print("PERFORMANCE COMPARISON")
print("=" * 80)

# If we have both matching and temporal data, show integrated analysis potential
if matched_results is not None and temporal_data is not None:
    print("Integration potential:")

    # Get cases from matching results
    matched_cases = matched_results.filter(pl.col("ROLE") == "case")

    # Show how temporal data could be joined with matched results
    if len(matched_cases) > 0 and len(temporal_data) > 0:
        common_cases = set(matched_cases["PNR"].to_list()) & set(
            temporal_data["PNR"].to_list()
        )
        print(f"  Cases with both matching and temporal data: {len(common_cases)}")

        if len(common_cases) > 0:
            print("  ‚úì Ready for integrated epidemiological analysis!")
            print("    - Case-control groups identified")
            print("    - Temporal prescription patterns extracted")
            print("    - Can analyze medication usage before/after diagnosis")

if ethnicity_results is not None:
    print("\nEthnicity integration potential:")
    print("  - Ethnicity categories can be joined with matched cases/controls")
    print("  - Enables stratified analysis by ethnicity (e.g., Danish vs. Immigrant)")
    print("  - Controls for confounding in epidemiological studies")

print("\n" + "=" * 80)
print("SUMMARY")
print("=" * 80)
print("‚úì Epidemiological Analysis Plugin test completed successfully!")
print()
print("üÜï NEW FEATURES TESTED:")
print(
    "  ‚úÖ Birth type matching constraint (singleton, doubleton, tripleton, quadleton, multiple)"
)
print("  ‚úÖ Education level extraction from UDDF register with temporal validity")
print("  ‚úÖ Special handling of Danish middle school codes (realskolen)")
print("  ‚úÖ Highest education level prioritization (short < medium < long)")
print("  ‚úÖ Ethnicity categorization from BEF register with parental data")
print("  ‚úÖ Temporal filtering for latest ethnicity data before index date")
print("  ‚úÖ OPR_LAND mapping with Danish/Western/Non-Western categories")
print("  ‚úÖ Removed primitive 'risk_set' algorithm - keeping only advanced ones")
print("  ‚úÖ Updated default algorithm to 'spatial_index' for better performance")
print()
print("üìà EXISTING FEATURES DEMONSTRATED:")
print("  ‚úì Case-control matching with risk-set sampling")
print("  ‚úì Temporal data extraction with dynamic year ranges")
print("  ‚úì Registry file caching for performance")
print("  ‚úì Batch processing for large datasets")
print("  ‚úì Multiple file format support (.parquet, .ipc)")
print("  ‚úì Flexible temporal windows (¬±1, ¬±2 years)")
print("  ‚úì Additional column extraction")
print(
    "  ‚úì Two advanced matching algorithms: 'spatial_index' and 'partitioned_parallel'"
)
print("  ‚úì Parity matching support")
print("  ‚úì Compile-time HFAUDD categorization mapping for efficiency")
print()
print("üéØ BIRTH TYPE MATCHING BENEFITS:")
print(
    "  ‚Ä¢ Ensures cases and controls have same birth type (singleton with singleton, etc.)"
)
print("  ‚Ä¢ Important for studies where birth type affects outcomes")
print(
    "  ‚Ä¢ Maintains epidemiological validity by controlling for multiple birth effects"
)
print(
    "  ‚Ä¢ Can be combined with existing constraints (birth dates, parity, parent info)"
)
print()
print("üéì EDUCATION EXTRACTION BENEFITS:")
print("  ‚Ä¢ Extracts highest attained education level from Danish UDDF register")
print("  ‚Ä¢ Handles temporal validity (HF_VFRA ‚â§ INDEX_DATE ‚â§ HF_VTIL)")
print(
    "  ‚Ä¢ Special recognition of Danish middle school codes (realskolen) as medium education"
)
print("  ‚Ä¢ Categorizes according to ISCED levels: short/medium/long education")
print("  ‚Ä¢ Compile-time HFAUDD mapping for high performance")
print("  ‚Ä¢ Supports multiple education records per individual with priority selection")
print("  ‚Ä¢ Batch processing for large-scale epidemiological studies")
print()
print("üåç ETHNICITY CATEGORIZATION BENEFITS:")
print(
    "  ‚Ä¢ Extracts SEPLINE-compliant ethnicity categories using individual + parental OPR_LAND"
)
print(
    "  ‚Ä¢ Distinguishes Danish, Mixed, Western Immigrant/Descendant, Non-Western Immigrant/Descendant"
)
print("  ‚Ä¢ Uses latest available BEF data before index date (temporal validity)")
print("  ‚Ä¢ Handles missing/unknown codes appropriately")
print("  ‚Ä¢ Enables ethnicity-stratified epidemiological analysis")
print("  ‚Ä¢ Controls for immigration status via IE_TYPE")
print()
print("The plugin is ready for production epidemiological research!")

print("\n" + "=" * 80)
print("Running DID demo...")
print("=" * 80)
run_did_demo()
print("=" * 80)
