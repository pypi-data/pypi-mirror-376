# SynthGenAI-Package for Generating Synthetic Datasets using LLMs

![header_logo_image](./docs/assets/logo_header.png)

SynthGenAI is a package for generating Synthetic Datasets. The idea is to have a tool which is simple to use and can generate datasets on different topics by utilizing LLMs from different API providers. The package is designed to be modular and can be easily extended to include some different API providers for LLMs and new features.

> [!IMPORTANT]
> The package is still in the early stages of development and some features may not be fully implemented or tested. If you find any issues or have any suggestions, feel free to open an issue or create a pull request.

## Why SynthGenAI now? ğŸ¤”

Interest in synthetic data generation has surged recently, driven by the growing recognition of data as a critical asset in AI development. As [Ilya Sutskever](https://youtu.be/1yvBqasHLZs), one of the most important figures in AI, says: 'Data is the fossil fuel of AI.' The more quality data we have, the better our models can perform. However, access to data is often restricted due to privacy concerns, or it may be prohibitively expensive to collect. Additionally, the vast amount of high-quality data on the internet has already been extensively mined. Synthetic data generation addresses these challenges by allowing us to create diverse and useful datasets using current pre-trained Large Language Models (LLMs). Beyond LLMs, synthetic data also holds immense potential for pre-training and post-training of Small Language Models (SLMs), which are gaining popularity due to their efficiency and suitability for specific, resource-constrained applications. By leveraging synthetic data for both LLMs and SLMs, we can enhance performance across a wide range of use cases while balancing resource efficiency and model effectiveness. This approach enables us to harness the strengths of both synthetic and authentic datasets to achieve optimal outcomes.

## Tools used for building SynthGenAI ğŸ§°

The package is built using Python and the following libraries:

- [uv](https://docs.astral.sh/uv/), An extremely fast Python package and project manager, written in Rust.
- [LiteLLM](https://docs.litellm.ai/docs/), A Python SDK for accessing LLMs from different API providers with standardized OpenAI Format.
- [Langfuse](https://langfuse.com/), LLMOps platform for observability, tracebility and monitoring of LLMs.
- [Pydantic](https://pydantic-docs.helpmanual.io/), Data validation and settings management using Python type annotations.
- [Huggingface Hub](https://huggingface.co/) & [Datasets](https://huggingface.co/docs/datasets/), A Python library for saving generated datasets on Hugging Face Hub.

## Installation ğŸ› ï¸

To install the package, you can use the following command:

```bash
pip install synthgenai
```

or if you want to use uv package manager, you can use the following command:

```bash
uv add synthgenai
```

or you can install the package directly from the source code using the following commands:

```bash
git clone https://github.com/Shekswess/synthgenai.git
uv build
pip install ./dist/synthgenai-{version}-py3-none-any.whl
```

### Requirements ğŸ“‹

To use the package, you need to have the following requirements installed:

- [Python 3.10+](https://www.python.org/downloads/)
- [uv](https://docs.astral.sh/uv/) for building the package directly from the source code
- [Ollama](https://ollama.com/) running on your local machine if you want to use Ollama as an API provider (optional)
- [Langfuse](https://langfuse.com/) running on your local machine or in the cloud if you want to use Langfuse for tracebility (optional)
- [Hugging Face Hub](https://huggingface.co/) account if you want to save the generated datasets on Hugging Face Hub with generated token (optional)
- [Gradio](https://gradio.app/) for using the SynthGenAI UI (optional)

## Quick Start ğŸš€

After installation, get started quickly by using the CLI:

```bash
# 1. See what environment variables you need
synthgenai env-setup

# 2. Set up your API keys (example for OpenAI)
export OPENAI_API_KEY="your-api-key-here"

# 3. # List available dataset types
synthgenai list-types

# 4. Generate your first dataset
synthgenai generate instruction \
  --model "openai/gpt-5" \
  --topic "Python Programming" \
  --domain "Software Development" \
  --entries 100

# 5. See more examples
synthgenai examples
```

### Available Commands

- `synthgenai generate` - Generate synthetic datasets
- `synthgenai list-types` - Show all available dataset types
- `synthgenai examples` - Display example commands
- `synthgenai providers` - List supported LLM providers
- `synthgenai env-setup` - Show environment setup guide
- `synthgenai --help` - Show help information

## Usage ğŸ‘¨â€ğŸ’»

### Supported API Providers ğŸ’ª

- [x] [Groq](https://groq.com/) - more info about Groq models that can be used, can be found [here](https://docs.litellm.ai/docs/providers/groq)
- [x] [Mistral AI](https://mistral.ai/) - more info about Mistral AI models that can be used, can be found [here](https://docs.litellm.ai/docs/providers/mistral-ai)
- [x] [Gemini](https://gemini.google.com/) - more info about Gemini models that can be used, can be found [here](https://docs.litellm.ai/docs/providers/gemini)
- [x] [Bedrock](https://aws.amazon.com/bedrock) - more info about Bedrock models that can be used, can be found [here](https://docs.litellm.ai/docs/providers/bedrock)
- [x] [Anthropic](https://www.anthropic.com/) - more info about Anthropic models that can be used, can be found [here](https://docs.litellm.ai/docs/providers/anthropic)
- [x] [OpenAI](https://openai.com) - more info about OpenAI models that can be used, can be found [here](https://docs.litellm.ai/docs/providers/openai)
- [x] [Hugging Face](https://huggingface.co/) - more info about Hugging Face models that can be used, can be found [here](https://docs.litellm.ai/docs/providers/hugging-face)
- [x] [Ollama](https://ollama.com/) - more info about Ollama models that can be used, can be found [here](https://docs.litellm.ai/docs/providers/ollama)
- [x] [vLLM](https://vllm.ai/) - more info about vLLM models that can be used, can be found [here](https://docs.litellm.ai/docs/providers/vllm)
- [x] [SageMaker](https://aws.amazon.com/sagemaker/) - more info about SageMaker models that can be used, can be found [here](https://docs.litellm.ai/docs/providers/aws_sagemaker)
- [x] [Azure](https://azure.microsoft.com/en-us/services/machine-learning/) - more info about Azure and Azure AI models that can be used, can be found [here](https://docs.litellm.ai/docs/providers/azure) & [here](https://docs.litellm.ai/docs/providers/azure_ai)
- [x] [Vertex AI](https://cloud.google.com/vertex-ai) - more info about Vertex AI models that can be used, can be found [here](https://docs.litellm.ai/docs/providers/vertex)
- [x] [DeepSeek](https://www.deepseek.com/) - more info about DeepSeek models that can be used, can be found [here](https://docs.litellm.ai/docs/providers/deepseek)
- [x] [xAI](https://x.ai/) - more info about xAI models that can be used, can be found [here](https://docs.litellm.ai/docs/providers/xai)
- [x] [OpenRouter](https://openrouter.ai/) - more info about OpenRouter models that can be used, can be found [here](https://docs.litellm.ai/docs/providers/openrouter)

### Environment Setup & Configuration ğŸ”

For detailed information about setting up environment variables for different API providers, observability tools, and dataset management, please refer to the [Installation Guide](./docs/installation/index.md#environment-variables-configuration-).

#### Logging Configuration

You can control the logging verbosity using the `SYNTHGENAI_DETAILED_MODE` environment variable:

```bash
# For detailed logging (shows all debug information)
export SYNTHGENAI_DETAILED_MODE="false"

# For NO logging (default)
export SYNTHGENAI_DETAILED_MODE="true"
```

> [!NOTE]
> By default, `SYNTHGENAI_DETAILED_MODE` is set to `"true"`, which provides NO logging output. Set it to `"false"` to enable detailed debugging information during dataset generation.

### Observability & Saving Datasets ğŸ“Š

For observing the generated datasets, you can use **Langfuse** for tracebility and monitoring of the LLMs.

For handling the datasets and saving them on **Hugging Face Hub**, you can use the **Hugging Face Datasets** library.

Currently there are six types of datasets that can be generated using SynthGenAI:

- **Raw Datasets**
- **Instruction Datasets**
- **Preference Datasets**
- **Sentiment Analysis Datasets**
- **Summarization Datasets**
- **Text Classification Datasets**

The datasets can be generated:

- **Synchronously** - each dataset entry is generated one by one
- **Asynchronously** - batch of dataset entries is generated at once

> [!NOTE]
> Asynchronous generation is faster than synchronous generation, but some of LLM providers can have limitations on the number of tokens that can be generated at once.

#### More Examples ğŸ“–

More examples with different combinations of LLM API providers and dataset configurations can be found in the [examples](./examples) directory.

> [!IMPORTANT]
> Sometimes the generation of the keywords for the dataset and the dataset entries can fail due to the limitation of the LLM to generate JSON Object as output (this is handled by the package). That's why it is recommended to use models that are capable of generating JSON Objects (structured output). List of models that can generate JSON Objects can be found [here](https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json).

## Generated Datasets ğŸ“š

Examples of generated synthetic datasets can be found on the [SynthGenAI Datasets Collection](https://huggingface.co/collections/Shekswess/synthgenai-datasets-6764ad878718b1e567653022) on Hugging Face Hub.

## Contributing ğŸ¤

If you want to contribute to this project and make it better, your help is very welcome. Create a pull request with your changes and I will review it. If you have any questions, open an issue.

## License ğŸ“

This project is licensed under the MIT License - see the LICENSE.md file for details.

## Repo Structure ğŸ“‚

```
.
â”œâ”€â”€ .github/                                                      # GitHub configuration files and workflows
â”‚   â”œâ”€â”€ workflows/                                                # GitHub Actions workflows
â”‚   â”‚   â”œâ”€â”€ build_n_publish.yaml                                  # Build and publish workflow
â”‚   â”‚   â”œâ”€â”€ docs.yaml                                             # Documentation deployment workflow
â”‚   â”‚   â””â”€â”€ uv-ci.yaml                                            # UV package manager CI workflow
â”‚   â””â”€â”€ depandabot.yml                                            # Dependabot configuration for automatic dependency updates
â”œâ”€â”€ docs                                                          # MkDocs documentation source files
â”‚   â”œâ”€â”€ assets                                                    # Static assets for documentation
â”‚   â”‚   â”œâ”€â”€ favicon.png                                           # Website favicon
â”‚   â”‚   â”œâ”€â”€ logo_header.png                                       # Header logo image
â”‚   â”‚   â””â”€â”€ logo.svg                                              # SVG logo for the project
â”‚   â”œâ”€â”€ configurations                                            # Configuration documentation
â”‚   â”‚   â”œâ”€â”€ dataset_configuration.md                              # Dataset configuration guide
â”‚   â”‚   â”œâ”€â”€ dataset_generator_configuration.md                    # Dataset generator configuration guide
â”‚   â”‚   â”œâ”€â”€ index.md                                              # Configuration section index
â”‚   â”‚   â””â”€â”€ llm_configuration.md                                  # LLM configuration guide
â”‚   â”œâ”€â”€ contributing                                              # Contribution guidelines
â”‚   â”‚   â””â”€â”€ index.md                                              # How to contribute to the project
â”‚   â”œâ”€â”€ datasets                                                  # Dataset type documentation
â”‚   â”‚   â”œâ”€â”€ index.md                                              # Dataset types overview
â”‚   â”‚   â”œâ”€â”€ instruction_datasets.md                               # Instruction datasets documentation
â”‚   â”‚   â”œâ”€â”€ preference_datasets.md                                # Preference datasets documentation
â”‚   â”‚   â”œâ”€â”€ raw_datasets.md                                       # Raw datasets documentation
â”‚   â”‚   â”œâ”€â”€ sentiment_analysis_datasets.md                        # Sentiment analysis datasets documentation
â”‚   â”‚   â”œâ”€â”€ summarization_datasets.md                             # Summarization datasets documentation
â”‚   â”‚   â””â”€â”€ text_classification_datasets.md                       # Text classification datasets documentation
â”‚   â”œâ”€â”€ examples                                                  # Examples documentation
â”‚   â”‚   â””â”€â”€ index.md                                              # Code examples and usage patterns
â”‚   â”œâ”€â”€ index.md                                                  # Main documentation homepage
â”‚   â”œâ”€â”€ installation                                              # Installation documentation
â”‚   â”‚   â””â”€â”€ index.md                                              # Installation guide and requirements
â”‚   â”œâ”€â”€ llm_providers                                             # LLM provider documentation
â”‚   â”‚   â””â”€â”€ index.md                                              # Supported LLM providers guide
â”‚   â”œâ”€â”€ quick_start                                               # Quick start guide
â”‚   â”‚   â””â”€â”€ index.md                                              # Getting started tutorial
â”‚   â””â”€â”€ stylesheets                                               # Custom CSS styles for documentation
â”œâ”€â”€ examples                                                      # Python example scripts demonstrating usage
â”‚   â”œâ”€â”€ anthropic_instruction_dataset_example.py                  # Anthropic API instruction dataset example
â”‚   â”œâ”€â”€ azure_ai_preference_dataset_example.py                    # Azure AI preference dataset example
â”‚   â”œâ”€â”€ azure_summarization_dataset_example.py                    # Azure summarization dataset example
â”‚   â”œâ”€â”€ bedrock_raw_dataset_example.py                            # AWS Bedrock raw dataset example
â”‚   â”œâ”€â”€ deepseek_instruction_dataset_example.py                   # DeepSeek instruction dataset example
â”‚   â”œâ”€â”€ gemini_langfuse_raw_dataset_example.py                    # Gemini with Langfuse raw dataset example
â”‚   â”œâ”€â”€ groq_preference_dataset_example.py                        # Groq preference dataset example
â”‚   â”œâ”€â”€ huggingface_instruction_dataset_example.py                # Hugging Face instruction dataset example
â”‚   â”œâ”€â”€ mistral_preference_dataset_example.py                     # Mistral AI preference dataset example
â”‚   â”œâ”€â”€ ollama_preference_dataset_example.py                      # Ollama preference dataset example
â”‚   â”œâ”€â”€ openai_raw_dataset_example.py                             # OpenAI raw dataset example
â”‚   â”œâ”€â”€ openrouter_raw_dataset_example.py                         # OpenRouter raw dataset example
â”‚   â”œâ”€â”€ sagemaker_summarization_dataset_example.py                # AWS SageMaker summarization dataset example
â”‚   â”œâ”€â”€ vertex_ai_text_classification_dataset_example.py          # Google Vertex AI text classification example
â”‚   â”œâ”€â”€ vllm_sentiment_analysis_dataset_example.py                # vLLM sentiment analysis dataset example
â”‚   â””â”€â”€ xai_raw_dataset_example.py                                # xAI raw dataset example
â”œâ”€â”€ synthgenai                                                    # Main package source code
â”‚   â”œâ”€â”€ dataset                                                   # Dataset handling modules
â”‚   â”‚   â”œâ”€â”€ __init__.py                                           # Dataset package initializer
â”‚   â”‚   â”œâ”€â”€ base_dataset.py                                       # Base dataset class and common functionality
â”‚   â”‚   â””â”€â”€ dataset.py                                            # Main dataset implementation
â”‚   â”œâ”€â”€ dataset_genetors                                          # Dataset generation modules
â”‚   â”‚   â”œâ”€â”€ __init__.py                                           # Dataset generators package initializer
â”‚   â”‚   â”œâ”€â”€ classification_dataset_generator.py                   # Text classification dataset generator
â”‚   â”‚   â”œâ”€â”€ dataset_generator.py                                  # Base dataset generator class
â”‚   â”‚   â”œâ”€â”€ instruction_dataset_generator.py                      # Instruction-following dataset generator
â”‚   â”‚   â”œâ”€â”€ preference_dataset_generator.py                       # Preference dataset generator (RLHF)
â”‚   â”‚   â”œâ”€â”€ raw_dataset_generator.py                              # Raw text dataset generator
â”‚   â”‚   â”œâ”€â”€ sentiment_dataset_generator.py                        # Sentiment analysis dataset generator
â”‚   â”‚   â””â”€â”€ summarization_dataset_generator.py                    # Text summarization dataset generator
â”‚   â”œâ”€â”€ llm                                                       # LLM interaction modules
â”‚   â”‚   â”œâ”€â”€ __init__.py                                           # LLM package initializer
â”‚   â”‚   â”œâ”€â”€ base_llm.py                                           # Base LLM class and common functionality
â”‚   â”‚   â””â”€â”€ llm.py                                                # Main LLM implementation with LiteLLM integration
â”‚   â”œâ”€â”€ prompts                                                   # Prompt templates for different dataset types
â”‚   â”‚   â”œâ”€â”€ description_system_prompt                             # System prompt for generating descriptions
â”‚   â”‚   â”œâ”€â”€ description_user_prompt                               # User prompt template for descriptions
â”‚   â”‚   â”œâ”€â”€ entry_classification_system_prompt                    # System prompt for classification entries
â”‚   â”‚   â”œâ”€â”€ entry_instruction_system_prompt                       # System prompt for instruction entries
â”‚   â”‚   â”œâ”€â”€ entry_preference_system_prompt                        # System prompt for preference entries
â”‚   â”‚   â”œâ”€â”€ entry_raw_system_prompt                               # System prompt for raw text entries
â”‚   â”‚   â”œâ”€â”€ entry_sentiment_system_prompt                         # System prompt for sentiment entries
â”‚   â”‚   â”œâ”€â”€ entry_summarization_system_prompt                     # System prompt for summarization entries
â”‚   â”‚   â”œâ”€â”€ entry_user_prompt                                     # User prompt template for dataset entries
â”‚   â”‚   â”œâ”€â”€ keyword_system_prompt                                 # System prompt for keyword generation
â”‚   â”‚   â”œâ”€â”€ keyword_user_prompt                                   # User prompt template for keywords
â”‚   â”‚   â”œâ”€â”€ labels_system_prompt                                  # System prompt for label generation
â”‚   â”‚   â””â”€â”€ labels_user_prompt                                    # User prompt template for labels
â”‚   â”œâ”€â”€ schemas                                                   # Pydantic data models and validation schemas
â”‚   â”‚   â”œâ”€â”€ __init__.py                                           # Schemas package initializer
â”‚   â”‚   â”œâ”€â”€ config.py                                             # Configuration data models
â”‚   â”‚   â”œâ”€â”€ datasets.py                                           # Dataset-related data models
â”‚   â”‚   â”œâ”€â”€ enums.py                                              # Enumeration definitions
â”‚   â”‚   â””â”€â”€ messages.py                                           # Message and response data models
â”‚   â”œâ”€â”€ utils                                                     # Utility functions and helpers
â”‚   |   â”œâ”€â”€ file_utils.py                                         # File I/O operations and utilities
â”‚   |   â”œâ”€â”€ __init__.py                                           # Utils package initializer
â”‚   |   â”œâ”€â”€ json_utils.py                                         # JSON processing utilities
â”‚   |   â”œâ”€â”€ progress_utils.py                                     # Progress tracking and display utilities
â”‚   |   â”œâ”€â”€ prompt_utils.py                                       # Prompt processing and formatting utilities
â”‚   |   â”œâ”€â”€ text_utils.py                                         # Text manipulation and processing utilities
â”‚   |   â””â”€â”€ yaml_utils.py                                         # YAML processing utilities
â”‚   â”œâ”€â”€ __init__.py                                               # Main package initializer and version info
â”‚   â””â”€â”€ cli.py                                                    # Command-line interface implementation
â”œâ”€â”€ tests                                                         # Test suite for the package
â”‚   â”œâ”€â”€ __init__.py                                               # Tests package initializer
â”‚   â”œâ”€â”€ conftest.py                                               # pytest configuration and fixtures
â”‚   â”œâ”€â”€ test_dataset_generator.py                                 # Tests for dataset generators
â”‚   â”œâ”€â”€ test_dataset.py                                           # Tests for dataset functionality
â”‚   â””â”€â”€ test_llm.py                                               # Tests for LLM integration
â”œâ”€â”€ .gitignore                                                    # Git ignore rules for excluded files
â”œâ”€â”€ .pre-commit-config.yaml                                       # Pre-commit hooks configuration
â”œâ”€â”€ .python-version                                               # Python version specification for pyenv
â”œâ”€â”€ LICENCE.txt                                                   # MIT License file
â”œâ”€â”€ mkdocs.yml                                                    # MkDocs documentation configuration
â”œâ”€â”€ pyproject.toml                                                # Python project metadata and dependencies (PEP 518)
â”œâ”€â”€ README.md                                                     # Main project documentation and overview
â””â”€â”€ uv.lock                                                       # UV lockfile for reproducible dependency resolution
```
