name: Build and Deploy to Kubernetes

on:
  push:
    branches:
      - dv-stage # Trigger only on pushes to this specific branch.
      - dv-main
      - test/deployment_cache

env:
  PROJECT_ID: desivocalprod01
  REGION: asia-south1
  REPOSITORY: ${{ github.ref == 'refs/heads/dv-main' && 'ringg-registry-prod' || 'ringg-registry-stage' }}
  IMAGE_NAME: dv-pipecat
  # Determine cluster and secrets based on the target branch (dv-main for prod, others for stage)
  CLUSTER: ${{ github.ref == 'refs/heads/dv-main' && 'desivocal-prod-us-e1-cluster' || 'desivocal-staging-cluster' }}
  CLUSTER_ZONE: ${{ github.ref == 'refs/heads/dv-main' && 'us-east1' || 'asia-south1-a' }}
  GITHUB_SHA: ${{ github.sha }}
  # Assuming secrets are named like PROD_SECRETS_JSON / STAGE_SECRETS_JSON etc.
  # SECRETS_JSON should contain a JSON object like {"VAR1": "value1", "VAR2": "value2"}
  SECRETS_JSON: ${{ github.ref == 'refs/heads/dv-main' && secrets.PROD_SECRETS_JSON || secrets.STAGE_SECRETS_JSON }}
  # CREDS_JSON should contain the raw content of the creds.json file
  CREDS_JSON: ${{ github.ref == 'refs/heads/dv-main' && secrets.PROD_CREDS_JSON || secrets.STAGE_CREDS_JSON }}
  # Helm release name
  HELM_RELEASE_NAME: dv-pipecat

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      id-token: write # Required for Workload Identity Federation

    steps:
    - name: Wait for auto-release to complete
      if: github.ref == 'refs/heads/dv-stage'
      run: |
        echo "‚è≥ Waiting for auto-release workflow to complete..."
        
        # Wait up to 10 minutes for the auto-release workflow
        MAX_WAIT_TIME=600  # 10 minutes in seconds
        WAIT_INTERVAL=30   # Check every 30 seconds
        ELAPSED_TIME=0
        
        while [ $ELAPSED_TIME -lt $MAX_WAIT_TIME ]; do
          # Check if any workflow runs are still in progress for this commit
          RUNNING_WORKFLOWS=$(gh api /repos/${{ github.repository }}/actions/runs \
            --jq '.workflow_runs[] | select(.head_sha == "${{ github.sha }}" and .name == "Smart Auto-Release on PR Merge" and .status == "in_progress") | .id' | wc -l)
          
          if [ "$RUNNING_WORKFLOWS" -eq 0 ]; then
            echo "‚úÖ Auto-release workflow completed for commit ${{ github.sha }}"
            break
          fi
          
          echo "üîÑ Auto-release still running, waiting ${WAIT_INTERVAL}s... (${ELAPSED_TIME}s elapsed)"
          sleep $WAIT_INTERVAL
          ELAPSED_TIME=$((ELAPSED_TIME + WAIT_INTERVAL))
        done
        
        if [ $ELAPSED_TIME -ge $MAX_WAIT_TIME ]; then
          echo "‚ö†Ô∏è  Timeout waiting for auto-release workflow, proceeding anyway..."
        fi
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

    - name: Checkout code (fresh after auto-release)
      uses: actions/checkout@v4
      with:
        ref: ${{ github.ref }}
        fetch-depth: 0

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests packaging

    - name: Verify PyPI package availability
      run: |
        echo "üîç Verifying PyPI package availability..."
        
        # Get version from remote-requirements.txt
        PIPECAT_VERSION=$(grep "dv-pipecat-ai" examples/ringg-chatbot/remote-requirements.txt | sed -n 's/.*==\([^]]*\).*/\1/p')
        echo "üì¶ Required pipecat version: $PIPECAT_VERSION"
        
        # Verify package is available on PyPI with retry logic
        MAX_ATTEMPTS=2
        ATTEMPT=1
        PACKAGE_AVAILABLE=false
        
        while [ $ATTEMPT -le $MAX_ATTEMPTS ]; do
          echo "üîç Attempt $ATTEMPT/$MAX_ATTEMPTS: Checking PyPI for dv-pipecat-ai==$PIPECAT_VERSION"
          
          if python scripts/check-pypi-package.py dv-pipecat-ai "$PIPECAT_VERSION"; then
            PACKAGE_AVAILABLE=true
            echo "‚úÖ PyPI package verification successful!"
            break
          else
            if [ $ATTEMPT -eq $MAX_ATTEMPTS ]; then
              echo "‚ùå FAILED: Package dv-pipecat-ai==$PIPECAT_VERSION not available on PyPI after $MAX_ATTEMPTS attempts"
              echo "üö® This usually means:"
              echo "   1. The auto-release workflow failed to publish"
              echo "   2. PyPI is experiencing delays"
              echo "   3. Version mismatch between requirements.txt and PyPI"
              exit 1
            else
              WAIT_TIME=$((ATTEMPT * 10))
              echo "‚è≥ Package not yet available, waiting ${WAIT_TIME}s before retry..."
              sleep $WAIT_TIME
            fi
          fi
          ATTEMPT=$((ATTEMPT + 1))
        done
        
        echo "PIPECAT_VERSION=$PIPECAT_VERSION" >> $GITHUB_ENV

    - name: Authenticate to GCP using Workload Identity Federation
      id: auth
      uses: google-github-actions/auth@v2
      with:
        workload_identity_provider: projects/623676891410/locations/global/workloadIdentityPools/desivocal-staging-pool/providers/github # TODO: Update for production if needed
        service_account: gke-githubactions-svc-stage@desivocalprod01.iam.gserviceaccount.com # TODO: Update for production if needed

    - name: Configure gcloud
      run: |
        gcloud config set project $PROJECT_ID
        gcloud auth configure-docker $REGION-docker.pkg.dev --quiet

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3

    - name: Build & push image (with GHA cache)
      uses: docker/build-push-action@v6
      with:
        context: ./examples/ringg-chatbot
        file: ./examples/ringg-chatbot/remote-Dockerfile
        platforms: linux/${{ github.ref == 'refs/heads/dv-main' && 'arm64' || 'amd64' }}
        push: true
        tags: ${{ env.REGION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.IMAGE_NAME }}:${{ env.GITHUB_SHA }}
        cache-from: type=gha,scope=${{ github.ref == 'refs/heads/dv-main' && 'arm64' || 'amd64' }}
        cache-to: type=gha,mode=max,scope=${{ github.ref == 'refs/heads/dv-main' && 'arm64' || 'amd64' }}
        provenance: 'false'

    # - name: Build Docker image
    #   run: |
    #     cd examples/ringg-chatbot
    #     docker build -t $REGION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$IMAGE_NAME:$GITHUB_SHA -f remote-Dockerfile .

    # - name: Push Docker image to Artifact Registry
    #   run: |
    #     docker push $REGION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$IMAGE_NAME:$GITHUB_SHA

    - name: Install Helm
      uses: azure/setup-helm@v4
      with:
        version: v3.13.3 # Specify Helm version if needed

    - name: Connect to GKE
      uses: google-github-actions/get-gke-credentials@v2
      with:
        cluster_name: ${{ env.CLUSTER }}
        location: ${{ env.CLUSTER_ZONE }}

    - name: Install KEDA (if not already installed)
      run: |
        echo "üîç Checking if KEDA is already installed..."
        
        # Ensure KEDA Helm repository exists and is up-to-date for both install and upgrade paths
        if ! helm repo list | grep -q '^kedacore\s'; then
          helm repo add kedacore https://kedacore.github.io/charts
        else
          # Force update in case URL changed or local cache is stale
          helm repo add kedacore https://kedacore.github.io/charts --force-update
        fi
        helm repo update
        
        # Install KEDA if needed
        if ! kubectl get deployment -n keda keda-operator >/dev/null 2>&1; then
          echo "üöÄ Installing KEDA core..."
          
          # Install KEDA core (required before HTTP add-on) with 30s HTTP timeout
          helm install keda kedacore/keda --namespace keda --create-namespace --wait --timeout=5m \
            --set operator.extraEnvs[0].name=KEDA_HTTP_DEFAULT_TIMEOUT \
            --set operator.extraEnvs[0].value="30000"
            
          echo "‚úÖ KEDA core installation completed with 30s HTTP timeout"
        else
          echo "‚úÖ KEDA core already installed"
          
          # Update existing KEDA installation to ensure HTTP timeout is set
          echo "üîÑ Updating KEDA configuration to ensure 30s HTTP timeout..."
          helm upgrade keda kedacore/keda --namespace keda --reuse-values \
            --set operator.extraEnvs[0].name=KEDA_HTTP_DEFAULT_TIMEOUT \
            --set operator.extraEnvs[0].value="30000" \
            --wait --timeout=5m
          
          echo "‚úÖ KEDA configuration updated with 30s HTTP timeout"
        fi
        
        # Verify KEDA CRDs are available
        echo "üîç Verifying KEDA CRDs..."
        kubectl get crd | grep keda || {
          echo "‚ùå KEDA CRDs not found, waiting for them to be ready..."
          sleep 30
          kubectl get crd | grep keda || {
            echo "‚ùå KEDA CRDs still not available after waiting"
            exit 1
          }
        }
        
        echo "‚úÖ KEDA is ready"

    - name: Install jq (for parsing JSON secrets)
      run: sudo apt-get update && sudo apt-get install -y jq

    - name: Create/Update GCP Credentials Secret
      run: |
        echo "Creating/Updating Kubernetes secret for GCP credentials..."
        # Create secret with creds.json key from CREDS_JSON content
        echo "$CREDS_JSON" | kubectl create secret generic ${{ env.HELM_RELEASE_NAME }}-gcp-creds --from-file=creds.json=/dev/stdin --dry-run=client -o yaml | kubectl apply -f - --overwrite
      env:
        CREDS_JSON: ${{ env.CREDS_JSON }}

    - name: Create/Update Application Secrets
      run: |
        echo "Creating/Updating Kubernetes secret for application environment variables..."
        # Build the --from-literal arguments dynamically from the JSON object stored in SECRETS_JSON
        # This creates one key in the secret for each key in the JSON, matching the file structure expected by read_secret
        LITERAL_ARGS=$(echo "$SECRETS_JSON" | jq -r 'to_entries | .[] | "--from-literal=\(.key)=\(.value)"' | tr '\n' ' ')
        if [ -z "$LITERAL_ARGS" ]; then
          echo "SECRETS_JSON is empty or not valid JSON. Skipping secret creation."
          # Handle error or create empty secret if absolutely required by envFrom (though optional should be fine)
          # kubectl create secret generic ${{ env.HELM_RELEASE_NAME }}-app-secrets --dry-run=client -o yaml | kubectl apply -f - --overwrite
        else
          kubectl create secret generic ${{ env.HELM_RELEASE_NAME }}-app-secrets $LITERAL_ARGS --dry-run=client -o yaml | kubectl apply -f - --overwrite
        fi
      env:
        SECRETS_JSON: ${{ env.SECRETS_JSON }}

    # Old code
    # - name: Deploy to GKE using Helm
    #   run: |
    #     if [ "${{ github.ref }}" == "refs/heads/dv-main" ]; then
    #       VALUES_FILE="values-prod.yaml"
    #       echo "Using production values: $VALUES_FILE"
    #     else
    #       VALUES_FILE="values-stage.yaml"
    #       echo "Using staging values: $VALUES_FILE"
    #     fi

    #     helm upgrade --install ${{ env.HELM_RELEASE_NAME }} ./k8s/dv-pipecat \
    #         --set image.repository=$REGION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$IMAGE_NAME \
    #         --set image.tag=$GITHUB_SHA \
    #         --namespace default \
    #         -f ./k8s/dv-pipecat/$VALUES_FILE
    # ---------- DEPLOY ----------
    # 1) Deploy stage
    - name: Deploy stage service
      if: github.ref == 'refs/heads/dv-stage'
      run: |
        STAGE_ADMIN_API_KEY=$(echo "$SECRETS_JSON" | jq -r '.ADMIN_API_KEY // empty')
        if [ -z "$STAGE_ADMIN_API_KEY" ]; then
          echo "‚ùå ADMIN_API_KEY not found in SECRETS_JSON"
          exit 1
        fi
        STAGE_CALLING_BACKEND_URL=$(echo "$SECRETS_JSON" | jq -r '.CALLING_BACKEND_URL // empty')
        if [ -z "$STAGE_CALLING_BACKEND_URL" ]; then
          echo "‚ùå CALLING_BACKEND_URL not found in SECRETS_JSON"
          exit 1
        fi
        helm upgrade --install $HELM_RELEASE_NAME ./k8s/dv-pipecat \
          -f ./k8s/dv-pipecat/values-stage.yaml \
          --set image.repository=$REGION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$IMAGE_NAME \
          --set image.tag=$GITHUB_SHA \
          --set ADMIN_API_KEY="${STAGE_ADMIN_API_KEY}" \
          --set CALLING_BACKEND_URL="${STAGE_CALLING_BACKEND_URL}" \
          --namespace default

    # 2) **ONLY** when branch is dv-main: deploy/update the CANARY release
    - name: Deploy canary service
      if: github.ref == 'refs/heads/dv-main'
      run: |
        PROD_ADMIN_API_KEY=$(echo "$SECRETS_JSON" | jq -r '.ADMIN_API_KEY // empty')
        if [ -z "$PROD_ADMIN_API_KEY" ]; then
          echo "‚ùå ADMIN_API_KEY not found in SECRETS_JSON"
          exit 1
        fi
        PROD_CALLING_BACKEND_URL=$(echo "$SECRETS_JSON" | jq -r '.CALLING_BACKEND_URL // empty')
        if [ -z "$PROD_CALLING_BACKEND_URL" ]; then
          echo "‚ùå CALLING_BACKEND_URL not found in SECRETS_JSON"
          exit 1
        fi
        echo "üöÄ Deploying canary service..."
        helm upgrade --install ${HELM_RELEASE_NAME}-canary ./k8s/dv-pipecat \
          -f ./k8s/dv-pipecat/values-prod.yaml \
          -f ./k8s/dv-pipecat/values-canary.yaml \
          --set image.repository=$REGION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$IMAGE_NAME \
          --set image.tag=$GITHUB_SHA \
          --set ADMIN_API_KEY="${PROD_ADMIN_API_KEY}" \
          --set CALLING_BACKEND_URL="${PROD_CALLING_BACKEND_URL}" \
          --namespace default

    # - name: Deployment validation
    #   run: |
    #     echo "üîç Validating deployment..."
        
    #     if [ "${{ github.ref }}" == "refs/heads/dv-stage" ]; then
    #       DEPLOYMENT_NAME="$HELM_RELEASE_NAME"
    #       ENVIRONMENT="staging"
    #     else
    #       DEPLOYMENT_NAME="${HELM_RELEASE_NAME}-canary"
    #       ENVIRONMENT="production-canary"
    #     fi
        
    #     echo "üìä Checking deployment: $DEPLOYMENT_NAME"
        
    #     # Wait for deployment to be ready
    #     kubectl rollout status deployment/$DEPLOYMENT_NAME --namespace=default --timeout=300s
        
    #     # Check pod status
    #     READY_PODS=$(kubectl get deployment $DEPLOYMENT_NAME --namespace=default -o jsonpath='{.status.readyReplicas}')
    #     DESIRED_PODS=$(kubectl get deployment $DEPLOYMENT_NAME --namespace=default -o jsonpath='{.spec.replicas}')
        
    #     echo "üìà Pod Status: $READY_PODS/$DESIRED_PODS ready"
        
    #     if [ "$READY_PODS" = "$DESIRED_PODS" ] && [ "$READY_PODS" -gt "0" ]; then
    #       echo "‚úÖ Deployment validation successful for $ENVIRONMENT"
    #       echo "üéâ All $READY_PODS pods are ready and running"
          
    #       # Show running pods
    #       echo "üîç Running pods:"
    #       kubectl get pods -l app.kubernetes.io/name=dv-pipecat --namespace=default
          
    #     else
    #       echo "‚ùå Deployment validation failed for $ENVIRONMENT"
    #       echo "üö® Expected $DESIRED_PODS pods, but only $READY_PODS are ready"
          
    #       # Show pod details for debugging
    #       echo "üîç Pod details:"
    #       kubectl get pods -l app.kubernetes.io/name=dv-pipecat --namespace=default
    #       kubectl describe deployment/$DEPLOYMENT_NAME --namespace=default
          
    #       exit 1
    #     fi

    # - name: Package version verification
    #   run: |
    #     echo "üîç Verifying deployment readiness..."
        
    #     if [ "${{ github.ref }}" == "refs/heads/dv-stage" ]; then
    #       DEPLOYMENT_NAME="$HELM_RELEASE_NAME"
    #     else
    #       DEPLOYMENT_NAME="${HELM_RELEASE_NAME}-canary"
    #     fi
        
    #     # Get a pod name from the deployment
    #     POD_NAME=$(kubectl get pods -l app.kubernetes.io/name=dv-pipecat,app.kubernetes.io/instance=$DEPLOYMENT_NAME --namespace=default -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
        
    #     if [ -n "$POD_NAME" ]; then
    #       echo "üì¶ Found running pod: $POD_NAME"
    #       echo "üìä Expected pipecat version: $PIPECAT_VERSION"
    #       echo "‚úÖ Deployment verification completed"
    #     else
    #       echo "‚ö†Ô∏è  Could not find running pod for verification"
    #     fi

    # - name: Deployment success notification
    #   run: |
    #     if [ "${{ github.ref }}" == "refs/heads/dv-stage" ]; then
    #       ENVIRONMENT="üîß Staging"
    #       NEXT_STEP="Ready for production deployment to dv-main"
    #     else
    #       ENVIRONMENT="üöÄ Production Canary"
    #       NEXT_STEP="Ready for promotion to stable production"
    #     fi
        
    #     echo "üéâ DEPLOYMENT SUCCESS!"
    #     echo "   Environment: $ENVIRONMENT" 
    #     echo "   Pipecat Version: $PIPECAT_VERSION"
    #     echo "   Image: $REGION-docker.pkg.dev/$PROJECT_ID/$REPOSITORY/$IMAGE_NAME:$GITHUB_SHA"
    #     echo "   Next Step: $NEXT_STEP"

    # - name: Deployment failure notification
    #   if: failure()
    #   run: |
    #     echo "‚ùå DEPLOYMENT FAILED!"
    #     echo "üö® The deployment encountered an error. Check the logs above for details."
        
    #     # Show recent events for debugging
    #     echo "üîç Recent Kubernetes events:"
    #     kubectl get events --namespace=default --sort-by='.metadata.creationTimestamp' --field-selector type!=Normal | tail -10
        
    #     # Show deployment status
    #     if [ "${{ github.ref }}" == "refs/heads/dv-stage" ]; then
    #       kubectl describe deployment/$HELM_RELEASE_NAME --namespace=default || true
    #     else
    #       kubectl describe deployment/${HELM_RELEASE_NAME}-canary --namespace=default || true
    #     fi