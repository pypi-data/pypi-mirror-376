{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a86996a9",
   "metadata": {},
   "source": [
    "# arXiv Paper Recommendations\n",
    "\n",
    "This notebook analyzes the latest papers published on [arXiv](https://arxiv.org/) and surfaces reading recommendations based on your interests.\n",
    "\n",
    "The analysis is a two-stage pipeline:\n",
    "\n",
    "- ([filter](../../api/#semlib.Session.filter)) Filter out irrelevant papers: ones on topics you're not interested in.\n",
    "- ([sort](../../api/#semlib.Session.sort)) Sort the remaining papers according to your research interests and other factors like reputation of authors.\n",
    "\n",
    "This notebook uses the OpenAI API and costs about $2.50 to run. You can reduce costs by sub-sampling the dataset or using cheaper models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a19b8",
   "metadata": {},
   "source": [
    "## Install and configure dependencies\n",
    "\n",
    "In addition to Semlib, this notebook uses [arxiv.py](https://github.com/lukasschwab/arxiv.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f700f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install semlib arxiv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bb781d",
   "metadata": {},
   "source": [
    "We start by initializing a Semlib [Session](../../api/#semlib.Session). A session provides a context for performing Semlib operations. We configure the session to cache LLM responses on disk in `cache.db`.\n",
    "\n",
    "This notebook uses OpenAI models. If your `OPENAI_API_KEY` is not already set in your environment, you can uncomment the line at the bottom of the next cell and set your API key there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f48b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import semlib\n",
    "from semlib import OnDiskCache, Session\n",
    "\n",
    "session = Session(cache=OnDiskCache(\"cache.db\"))\n",
    "\n",
    "# Uncomment the following lines and set your OpenAI API key if not already set in your environment\n",
    "\n",
    "# import os\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969b9a45",
   "metadata": {},
   "source": [
    "## Download and preview data\n",
    "\n",
    "We start by defining a function to fetch arXiv paper metadata given a set of categories along with a date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "804a1813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "import arxiv\n",
    "\n",
    "\n",
    "def get_papers(categories: list[str], start_date: date, end_date: date) -> list[arxiv.Result]:\n",
    "    query_cat = \" OR \".join(f\"cat:{cat}\" for cat in categories)\n",
    "    query_date = f\"submittedDate:[{start_date.strftime('%Y%m%d')} TO {end_date.strftime('%Y%m%d')}]\"\n",
    "    query = f\"({query_cat}) AND {query_date}\"\n",
    "    search = arxiv.Search(query)\n",
    "    client = arxiv.Client()\n",
    "    return list(client.results(search))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d3a64c",
   "metadata": {},
   "source": [
    "Next, we fetch a batch of papers. Feel free to edit the list of [categories](https://arxiv.org/category_taxonomy) to match your interests, or update the date range to get the most recent papers at the time you're running this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17bc37b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers: 907\n",
      "\n",
      "Example title: MyGO: Memory Yielding Generative Offline-consolidation for Lifelong Learning Systems\n",
      "\n",
      "Example abstract: Continual or Lifelong Learning aims to develop models capable of acquiring new knowledge from a sequence of tasks without catastrophically forgetting what has been learned before. Existing approaches often rely on storing samples from previous tasks (experience replay) or employing complex regularization terms to protect learned weights. However, these methods face challenges related to data priva...\n"
     ]
    }
   ],
   "source": [
    "papers = get_papers([\"cs.AI\", \"cs.LG\"], date(2025, 8, 29), date(2025, 9, 4))\n",
    "\n",
    "print(f\"Number of papers: {len(papers)}\\n\")\n",
    "print(f\"Example title: {papers[0].title}\\n\")\n",
    "print(f\"Example abstract: {papers[0].summary[:400].replace('\\n', ' ')}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d045891",
   "metadata": {},
   "source": [
    "## Find most relevant papers\n",
    "\n",
    "### Filter out irrelevant papers\n",
    "\n",
    "We start off by _filtering out_ papers that are irrelevant, given a list of topics you're definitely _not_ interested in. We do this with Semlib's [filter](../../api/#semlib.Session.filter) method. By default, this method _keeps_ items matching a criteria, but sometimes, LLMs perform better on an \"inverse\" binary classification problem, like \"is this paper about any of the following topics?\", rather than \"is this paper NOT about any of the following topics?\", so this method supports a `negate=True` argument where it keeps all items that do _not_ match the criteria given to the LLM.\n",
    "\n",
    "For this rough filtering stage, we use a low-cost model, `gpt-4.1-nano`.\n",
    "\n",
    "Feel free to edit the list of topics in the prompt below to match your preferences.\n",
    "\n",
    "The following cell takes about 30 seconds to run with the default `max_concurrency` level (feel free to change it in the `Session` constructor above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f078a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant = await session.filter(\n",
    "    papers,\n",
    "    template=lambda p: f\"\"\"\n",
    "Your task is to determine if the following academic paper is on any of the following topics.\n",
    "\n",
    "Paper title: {p.title}\n",
    "Paper abstract: {p.summary}\n",
    "\n",
    "It the paper about any of the following topics?\n",
    "\n",
    "- Medicine\n",
    "- Healthcare\n",
    "- Biology\n",
    "- Chemistry\n",
    "- Physics\n",
    "\"\"\".strip(),\n",
    "    model=\"openai/gpt-4.1-nano\",\n",
    "    negate=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2446940",
   "metadata": {},
   "source": [
    "We can see how many papers we managed to filter out, and also take a look at some of the irrelevant papers, to make sure the filter worked well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e2bd517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 228 irrelevant papers, including:\n",
      "- Abex-rat: Synergizing Abstractive Augmentation and Adversarial Training for Classification of Occupational Accident Reports\n",
      "- Deep Self-knowledge Distillation: A hierarchical supervised learning for coronary artery segmentation\n",
      "- Quantum-Enhanced Natural Language Generation: A Multi-Model Framework with Hybrid Quantum-Classical Architectures\n",
      "- Multimodal learning of melt pool dynamics in laser powder bed fusion\n",
      "- Temporally-Aware Diffusion Model for Brain Progression Modelling with Bidirectional Temporal Regularisation\n"
     ]
    }
   ],
   "source": [
    "print(f\"Filtered out {len(papers) - len(relevant)} irrelevant papers, including:\")\n",
    "\n",
    "for paper in list({i.title for i in papers} - {i.title for i in relevant})[:5]:\n",
    "    print(f\"- {paper}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5afb1e6",
   "metadata": {},
   "source": [
    "### Sort papers by relevance\n",
    "\n",
    "Next, we sort the list of papers by relevance using Semlib's [sort](../../api/#semlib.Session.sort) method, which sorts items by using an LLM to perform pairwise comparisons. The API supports framing the comparison [task](../../api/compare/#semlib.compare.Task) in a number of ways. Here, we ask the LLM to choose the better fit between options \"A\" and \"B\".\n",
    "\n",
    "We start by defining the prompt for the LLM. We put the static instructions at the start of the LLM prompt to take advantage of [prompt caching](https://openai.com/index/api-prompt-caching/).\n",
    "\n",
    "Feel free to edit the list of interests to match yours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "518d8741",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPARISON_TEMPLATE = \"\"\"\n",
    "You are a research assistant. Help me pick a research paper to read, based on what is most relevant to my interests and what is most likely to be high-quality work based on the title, authors, and abstract.\n",
    "\n",
    "You will be given context on my interests, and two paper abstracts.\n",
    "\n",
    "My research interests include:\n",
    "\n",
    "- Machine learning and artificial intelligence\n",
    "- Systems\n",
    "- Security\n",
    "- Formal methods\n",
    "\n",
    "Here is paper Option A:\n",
    "\n",
    "<option A>\n",
    "{}\n",
    "</option A>\n",
    "\n",
    "Here is paper Option B:\n",
    "\n",
    "<option B>\n",
    "{}\n",
    "</option B>\n",
    "\n",
    "Choose the option (either A or B) that is more relevant to my interests and likely to be a high-quality work.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d42a4",
   "metadata": {},
   "source": [
    "The [sort](../../api/#semlib.Session.sort) API supports a variety of alternatives for supplying a prompt template, such as providing a callable that takes a pair of items and returns a string. In this notebook, we supply a `to_str` function that converts items to a string representation, and a prompt template that is a format string with two placeholders.\n",
    "\n",
    "Next, we define the `to_str` function, which converts a paper (metadata object) to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8cf581ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_str(paper: arxiv.Result) -> str:\n",
    "    return f\"\"\"\n",
    "Title: {paper.title}\n",
    "Authors: {', '.join(author.name for author in paper.authors)}\n",
    "Abstract: {paper.summary}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f88b16a",
   "metadata": {},
   "source": [
    "Finally, we're ready to call `sort()`. Earlier, we used the `gpt-4.1-nano` model to filter papers because that's an easy task and this model is cheaper. For the following sort operation, we use the `gpt-4.1-mini` model. Semlib lets you choose the model on a per-operation basis to control the cost-quality-latency tradeoff.\n",
    "\n",
    "Here, we use the Quicksort algorithm for an average O(n log n) LLM calls. By default, sort performs O(n^2) LLM calls to achieve a higher-quality result.\n",
    "\n",
    "The following cell takes about 7 minutes to run with the default `max_concurrency` setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c98d631",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_results = await session.sort(\n",
    "    relevant,\n",
    "    to_str=to_str,\n",
    "    template=COMPARISON_TEMPLATE,\n",
    "    algorithm=semlib.sort.QuickSort(randomized=False),\n",
    "    model=\"openai/gpt-4.1-mini\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c62b03",
   "metadata": {},
   "source": [
    "### Cost analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6297dcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$2.53'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"${session.total_cost():.2f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8182c36f",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "`sorted_results` now contains the papers ordered from least aligned to most aligned with your research interests. Let's take a look at some of the top results.\n",
    "\n",
    "### Most aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01a2c700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Enabling Trustworthy Federated Learning via Remote Attestation for Mitigating Byzantine Threats (Chaoyu Zhang, Heng Jin, Shanghao Shi, Hexuan Yu, Sydney Johns, Y. Thomas Hou, Wenjing Lou)\n",
      "   http://arxiv.org/abs/2509.00634v1\n",
      "   Federated Learning (FL) has gained significant attention for its privacy-preserving capabilities, enabling distributed devices to collaboratively train a global model without sharing raw data. However...\n",
      "\n",
      "\n",
      "2. zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via Zero-Knowledge Proofs (Guofu Liao, Taotao Wang, Shengli Zhang, Jiqun Zhang, Shi Long, Dacheng Tao)\n",
      "   http://arxiv.org/abs/2508.21393v1\n",
      "   Fine-tuning large language models (LLMs) is crucial for adapting them to specific tasks, yet it remains computationally demanding and raises concerns about correctness and privacy, particularly in unt...\n",
      "\n",
      "\n",
      "3. An Information-Flow Perspective on Explainability Requirements: Specification and Verification (Bernd Finkbeiner, Hadar Frenkel, Julian Siber)\n",
      "   http://arxiv.org/abs/2509.01479v1\n",
      "   Explainable systems expose information about why certain observed effects are happening to the agents interacting with them. We argue that this constitutes a positive flow of information that needs to...\n",
      "\n",
      "\n",
      "4. Poisoned at Scale: A Scalable Audit Uncovers Hidden Scam Endpoints in Production LLMs (Zhiyang Chen, Tara Saba, Xun Deng, Xujie Si, Fan Long)\n",
      "   http://arxiv.org/abs/2509.02372v1\n",
      "   Large Language Models (LLMs) have become critical to modern software development, but their reliance on internet datasets for training introduces a significant security risk: the absorption and reprod...\n",
      "\n",
      "\n",
      "5. ANNIE: Be Careful of Your Robots (Yiyang Huang, Zixuan Wang, Zishen Wan, Yapeng Tian, Haobo Xu, Yinhe Han, Yiming Gan)\n",
      "   http://arxiv.org/abs/2509.03383v1\n",
      "   The integration of vision-language-action (VLA) models into embodied AI (EAI) robots is rapidly advancing their ability to perform complex, long-horizon tasks in humancentric environments. However, EA...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def format_paper(paper: arxiv.Result) -> str:\n",
    "    return f\"\"\"{paper.title} ({', '.join(author.name for author in paper.authors)})\n",
    "   {paper.entry_id}\n",
    "   {paper.summary[:200].replace('\\n', ' ')}...\"\"\"\n",
    "\n",
    "\n",
    "for i, p in enumerate(reversed(sorted_results[-5:])):\n",
    "    print(f\"{i+1}. {format_paper(p)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb353de",
   "metadata": {},
   "source": [
    "### Least aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f0e6ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Content and Engagement Trends in COVID-19 YouTube Videos: Evidence from the Late Pandemic (Nirmalya Thakur, Madeline D Hartel, Lane Michael Boden, Dallas Enriquez, Boston Joyner Ricks)\n",
      "   http://arxiv.org/abs/2509.01954v1\n",
      "   This work investigated about 10,000 COVID-19-related YouTube videos published between January 2023 and October 2024 to evaluate how temporal, lexical, linguistic, and structural factors influenced eng...\n",
      "\n",
      "\n",
      "2. Generative KI für TA (Wolfgang Eppler, Reinhard Heil)\n",
      "   http://arxiv.org/abs/2509.02053v1\n",
      "   Many scientists use generative AI in their scientific work. People working in technology assessment (TA) are no exception. TA's approach to generative AI is twofold: on the one hand, generative AI is ...\n",
      "\n",
      "\n",
      "3. Why it is worth making an effort with GenAI (Yvonne Rogers)\n",
      "   http://arxiv.org/abs/2509.00852v1\n",
      "   Students routinely use ChatGPT and the like now to help them with their homework, such as writing an essay. It takes less effort to complete and is easier to do than by hand. It can even produce as go...\n",
      "\n",
      "\n",
      "4. Community-Centered Spatial Intelligence for Climate Adaptation at Nova Scotia's Eastern Shore (Gabriel Spadon, Oladapo Oyebode, Camilo M. Botero, Tushar Sharma, Floris Goerlandt, Ronald Pelot)\n",
      "   http://arxiv.org/abs/2509.01845v1\n",
      "   This paper presents an overview of a human-centered initiative aimed at strengthening climate resilience along Nova Scotia's Eastern Shore. This region, a collection of rural villages with deep ties t...\n",
      "\n",
      "\n",
      "5. Quantifying the Social Costs of Power Outages and Restoration Disparities Across Four U.S. Hurricanes (Xiangpeng Li, Junwei Ma, Bo Li, Ali Mostafavi)\n",
      "   http://arxiv.org/abs/2509.02653v1\n",
      "   The multifaceted nature of disaster impact shows that densely populated areas contribute more to aggregate burden, while sparsely populated but heavily affected regions suffer disproportionately at th...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, p in enumerate(sorted_results[:5]):\n",
    "    print(f\"{i+1}. {format_paper(p)}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
