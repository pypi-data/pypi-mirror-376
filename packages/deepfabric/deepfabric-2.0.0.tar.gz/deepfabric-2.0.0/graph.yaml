# Example YAML configuration for basic prompt generation
system_prompt: "You are a helpful assistant. You provide clear and concise answers to user questions."

topic_generator: graph
topic_graph:
  args:
     root_prompt: "The history of artificial intelligence"
     provider: "gemini"  # Optional: overrides the main provider
     model: "gemini-2.5-flash-lite" # Optional: overrides the main model
     graph_degree: 3
     graph_depth: 3

data_engine:
  args:
    instructions: "Please provide training examples with questions and answers based on the given topic."
    system_prompt: "<system_prompt_placeholder>"  # Will be replaced with system_prompt
    provider: "gemini"  # LLM provider
    model: "gemini-2.5-flash-lite"  # Model name
    temperature: 0.9  # Higher temperature for more creative variations
    max_retries: 2  # Retry failed prompts up to 2 times

dataset:
  creation:
    num_steps: 5
    batch_size: 1
    provider: "gemini"  # LLM provider
    model: "gemini-2.5-flash-lite"  # Model name
    sys_msg: true  # Include system message in dataset (default: true)
  save_as: "graph_dataset.jsonl"
