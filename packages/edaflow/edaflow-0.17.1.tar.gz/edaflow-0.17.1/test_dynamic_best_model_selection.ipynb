{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "669aec08",
   "metadata": {},
   "source": [
    "# Testing Dynamic Best Model Selection in edaflow ML Workflow\n",
    "\n",
    "This notebook demonstrates how to test the dynamic selection of the best model in the edaflow ML workflow, ensuring that the code works for any primary metric (e.g., accuracy, f1, roc_auc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a243d7",
   "metadata": {},
   "source": [
    "## Outline & Test Plan\n",
    "\n",
    "1. Import required libraries and edaflow\n",
    "2. Load a sample dataset\n",
    "3. Run the ML workflow with different primary metrics\n",
    "4. Assert that the best model is selected according to the specified metric\n",
    "5. Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d1277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import edaflow.ml as ml\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b1b489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample dataset (breast cancer)\n",
    "data = load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['target'] = data.target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586e4717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_best_model_selection(primary_metric):\n",
    "    # Step 1: Setup ML Experiment\n",
    "    config = ml.setup_ml_experiment(\n",
    "        X=X,\n",
    "        y=y,\n",
    "        test_size=0.2,\n",
    "        val_size=0.15,\n",
    "        experiment_name=f\"test_{primary_metric}\",\n",
    "        random_state=42,\n",
    "        primary_metric=primary_metric\n",
    "    )\n",
    "    \n",
    "    # Step 2: Compare Multiple Models\n",
    "    models = {\n",
    "        'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'gradient_boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "        'logistic_regression': LogisticRegression(random_state=42),\n",
    "        'svm': SVC(probability=True, random_state=42)\n",
    "    }\n",
    "    for name, model in models.items():\n",
    "        model.fit(config['X_train'], config['y_train'])\n",
    "    \n",
    "    # Step 3: Compare Models\n",
    "    comparison_results = ml.compare_models(\n",
    "        models=models,\n",
    "        X_train=config['X_train'],\n",
    "        y_train=config['y_train'],\n",
    "        X_test=config['X_test'],\n",
    "        y_test=config['y_test'],\n",
    "        cv_folds=5,\n",
    "        scoring=['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n",
    "    )\n",
    "    \n",
    "    # Step 4: Rank Models and Select Best Performer (dynamic)\n",
    "    ranked_df = ml.rank_models(comparison_results, primary_metric)\n",
    "    best_model_traditional = ranked_df.iloc[0]['model']\n",
    "    best_model = ml.rank_models(\n",
    "        comparison_results,\n",
    "        primary_metric,\n",
    "        return_format='list'\n",
    "    )[0]['model_name']\n",
    "    \n",
    "    # Assert both methods agree\n",
    "    assert best_model == best_model_traditional, f\"Mismatch: {best_model} vs {best_model_traditional}\"\n",
    "    \n",
    "    # Assert the best model has the highest score for the metric\n",
    "    best_score = ranked_df.iloc[0][primary_metric]\n",
    "    assert np.isclose(best_score, ranked_df[primary_metric].max()), f\"Best score is not max for {primary_metric}\"\n",
    "    \n",
    "    print(f\"âœ… Best model for {primary_metric}: {best_model} (score: {best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb9ce81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for different metrics\n",
    "test_metrics = ['accuracy', 'f1', 'roc_auc']\n",
    "for metric in test_metrics:\n",
    "    test_best_model_selection(metric)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
