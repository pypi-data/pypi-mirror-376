{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeCun et al. 1990, \"Handwritten Digit Recognition: Applications of Neural Net Chips and Automatic Learning\"\n",
    "> The following tries to reproduce the original paper. Note that the digits dataset actually used in the paper could not be found and [MNIST 784](https://www.openml.org/search?type=data&status=active&id=554) is used instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* LeCun et al. 1990, _Handwritten Digit Recognition: Applications of Neural Net Chips and Automatic Learning_, [Neurocomputing](https://link.springer.com/chapter/10.1007/978-3-642-76153-9_35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "specifics in LeCun et al. 1990:\n",
    "\n",
    "* neural net\n",
    "    * weight initialization: uniformly at random $\\in [-2.4 / F_i, 2.4 / F_i]$ with $F_i = $ number of inputs of the unit\n",
    "    * \"tanh activation\": $A \\cdot \\tanh (S \\cdot a)$ with $A = 1.716$, $S = 2/3$ and $a = \\text{weights} \\cdot \\text{input}$\n",
    "    * 256 input (16 x 16 pixel images)\n",
    "    * layer #1: \n",
    "        * convolution with 12 5x5-kernels and stride 2 (output: 8 x 8 x 12 = 786 \"units\")\n",
    "        * tanh activation\n",
    "        * $F_i = 5 \\cdot 5 \\cdot n_\\text{input-channels} = 5 \\cdot 5 \\cdot 1 = 25$\n",
    "    * layer #2: \n",
    "        * convolution with 12 5x5-kernels and stride 2 (output: 4 x 4 x 12 = 192 \"units\")\n",
    "        * tanh activation\n",
    "        * $F_i = 5 \\cdot 5 \\cdot n_\\text{input-channels} = 5 \\cdot 5 \\cdot 12 = 300$\n",
    "    * layer #3:\n",
    "        * dense with 30 neurons\n",
    "        * tanh activation\n",
    "        * $F_i = 4 \\cdot 4 \\cdot 12 = 192$\n",
    "    * layer #4:\n",
    "        * dense output layer with 10 neurons\n",
    "        * tanh activation\n",
    "        * $F_i = 30$\n",
    "* target: vector of 10 values either 1 or -1 (so 9x -1 and 1x 1)\n",
    "* loss: mean squared error between prediction and target (paper reached 1.8e-2 on test and 2.5e-3 on train)\n",
    "* error rates: 0.14% on train, 5% on test\n",
    "* training:\n",
    "    * stochastic gradient descent (1 sample per backpropagation)\n",
    "    * samples always in the same order, no shuffling\n",
    "    * 23 or 30 epochs, paper is ambiguous\n",
    "    * learning rate was set using some not defined 2nd order derivative method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import typing as T\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchinfo\n",
    "import tqdm\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import random_neural_net_models.convolution_lecun1990 as conv_lecun1990\n",
    "import random_neural_net_models.data as rnnm_data\n",
    "import random_neural_net_models.learner as rnnm_learner\n",
    "import random_neural_net_models.losses as rnnm_losses\n",
    "import random_neural_net_models.telemetry as telemetry\n",
    "import random_neural_net_models.utils as utils\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_OVERFITTING_ONLY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml(\"mnist_784\", version=1, cache=True, parser=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.make_deterministic(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = utils.get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting a few images to overfit on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "X0, y0 = X.iloc[:n], y.iloc[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = conv_lecun1990.DigitsDataset(X0, y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = ds[4]\n",
    "plt.imshow(item[0], cmap=\"gray\", origin=\"upper\")\n",
    "plt.title(f\"Label: {item[1]}\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining a dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "dataloader = DataLoader(ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspecting one image of the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0]  # .reshape((28,28))\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "estimating convolution block height / width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_lecun1990.calc_conv_output_dim(\n",
    "    28, 5, 2, 2\n",
    "), conv_lecun1990.calc_conv_output_dim(14, 5, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom 2d convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining a 2d convolution component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kh = kw = 5\n",
    "n_in_channels = 1\n",
    "n_out_channels = 1\n",
    "\n",
    "myconv2d = conv_lecun1990.Conv2d(\n",
    "    edge=28,\n",
    "    n_in_channels=n_in_channels,\n",
    "    n_out_channels=n_out_channels,\n",
    "    kernel_width=kw,\n",
    "    kernel_height=kh,\n",
    "    stride=2,\n",
    "    padding=2,\n",
    "    dilation=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(dataloader))\n",
    "train_features = train_features.unsqueeze(dim=1)\n",
    "\n",
    "print(f\"{train_features.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "applying the convolution to an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_features = myconv2d(train_features)\n",
    "\n",
    "print(f\"{conv_features.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualizing the effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = train_labels[0]\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(5, 5))\n",
    "\n",
    "ax = axs[0]\n",
    "img = train_features[0][0]\n",
    "ax.imshow(img, cmap=\"gray\")\n",
    "ax.axis(\"off\")\n",
    "\n",
    "ax = axs[1]\n",
    "img = conv_features.detach().numpy()[0][0]\n",
    "ax.imshow(img, cmap=\"gray\")\n",
    "ax.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## trying to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using `densify_y` to convert a label to a vector of -1/1, i.e. False/True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[0:3], conv_lecun1990.densify_y(train_labels[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating the model in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = conv_lecun1990.Model(lecun_init=True, lecun_act=True)\n",
    "model = telemetry.ModelTelemetry(\n",
    "    model,\n",
    "    activations_every_n=100,\n",
    "    gradients_every_n=100,\n",
    "    activations_name_patterns=(r\".*act.*\",),\n",
    "    gradients_name_patterns=(r\"conv\\d$\", r\"lin\\d\"),\n",
    "    parameters_name_patterns=(r\"conv\\d$\", r\"lin\\d\"),\n",
    ")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchinfo.summary(model, input_size=(1, 28, 28), dtypes=[torch.double])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.1,  # randomly chosen, not provided in the paper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_func = nn.CrossEntropyLoss()\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "_iter = 0\n",
    "model.train()\n",
    "for epoch in tqdm.tqdm(range(n_epochs), desc=\"Epochs\", total=n_epochs):\n",
    "    for i, (xb, yb) in tqdm.tqdm(\n",
    "        enumerate(dataloader), desc=\"Batches\", total=len(dataloader)\n",
    "    ):\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        yb = conv_lecun1990.densify_y(yb)\n",
    "        loss = loss_func(model(xb), yb)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        model.loss_history_train(loss, _iter)\n",
    "        model.parameter_history(_iter)\n",
    "\n",
    "        _iter += 1\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw_gradient_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw_activation_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw_loss_history_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw_loss_history_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw_parameter_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.clean_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference for a few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspecting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.to(device)\n",
    "pred_probs = model(train_features)\n",
    "pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred_probs.detach().cpu().numpy().argmax(axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].cpu()  # .reshape((28,28))\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}, pred: {y_pred[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspecting the effect of the learned filters on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = model.model.conv1.n_out_channels.item()\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=n_filters // 3, ncols=n_filters // 4, figsize=(12, 12)\n",
    ")\n",
    "with torch.no_grad():\n",
    "    conv_features = model.model.act_conv1(\n",
    "        model.model.conv1(train_features.unsqueeze(1))\n",
    "    ).cpu()\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        ax.imshow(conv_features[0][i], cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"Filter {i+1}\")\n",
    "\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## overfitting with `Learner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = rnnm_data.MNISTDatasetWithLabels(X0, y0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(ds_train.y.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(\n",
    "    ds_train,\n",
    "    batch_size=1,\n",
    "    collate_fn=rnnm_data.collate_mnist_dataset_to_block_with_labels,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dl_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = conv_lecun1990.Model2(\n",
    "    lecun_init=True, lecun_act=True, dtype=torch.float\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "lr = 1e-2\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)  # , momentum=1e-3\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer=optimizer,\n",
    "    max_lr=lr,\n",
    "    epochs=n_epochs,\n",
    "    steps_per_epoch=len(dl_train),\n",
    ")\n",
    "loss = rnnm_losses.MSELossMNIST1HotLabel()\n",
    "save_dir = Path(\"./models\")\n",
    "\n",
    "loss_callback = rnnm_learner.TrainLossCallback()\n",
    "activations_callback = rnnm_learner.TrainActivationsCallback(\n",
    "    every_n=100, max_depth_search=4, name_patterns=(r\".*act.*\",)\n",
    ")\n",
    "gradients_callback = rnnm_learner.TrainGradientsCallback(\n",
    "    every_n=100, max_depth_search=4, name_patterns=(r\"conv\\d$\", r\"lin\\d\")\n",
    ")\n",
    "parameters_callback = rnnm_learner.TrainParametersCallback(\n",
    "    every_n=100, max_depth_search=4, name_patterns=(r\"conv\\d$\", r\"lin\\d\")\n",
    ")\n",
    "\n",
    "\n",
    "scheduler_callback = rnnm_learner.EveryBatchSchedulerCallback(scheduler)\n",
    "callbacks = [\n",
    "    loss_callback,\n",
    "    activations_callback,\n",
    "    gradients_callback,\n",
    "    parameters_callback,\n",
    "]\n",
    "\n",
    "lr_find_callback = rnnm_learner.LRFinderCallback(1e-5, 10, 100)\n",
    "\n",
    "learner = rnnm_learner.Learner(\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss,\n",
    "    callbacks=callbacks,\n",
    "    save_dir=save_dir,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.find_learning_rate(\n",
    "    dl_train, n_epochs=2, lr_find_callback=lr_find_callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find_callback.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(dl_train, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_callback.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_inf = DataLoader(\n",
    "    ds_train,\n",
    "    batch_size=10,\n",
    "    collate_fn=rnnm_data.collate_mnist_dataset_to_block_with_labels,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "y_prob = learner.predict(dl_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_OVERFITTING_ONLY:\n",
    "    raise SystemExit(\"Skipping training beyond overfitting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting ~95% accuracy on 10k digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splitting 10k digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10_000  # bit larger than the number of samples in the paper\n",
    "X0, X2, y0, y2 = train_test_split(\n",
    "    X.iloc[:n], y.iloc[:n], test_size=0.2, random_state=42\n",
    ")  # , stratify=y)\n",
    "X0, X1, y0, y1 = train_test_split(X0, y0, test_size=0.2, random_state=42)\n",
    "len(X0), len(X1), len(X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = conv_lecun1990.DigitsDataset(X0, y0)\n",
    "ds_valid = conv_lecun1990.DigitsDataset(X1, y1)\n",
    "ds_test = conv_lecun1990.DigitsDataset(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "dataloader = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "dataloader_valid = DataLoader(ds_valid, batch_size=500, shuffle=False)\n",
    "dataloader_test = DataLoader(ds_test, batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = conv_lecun1990.Model(lecun_init=True, lecun_act=True)\n",
    "model = telemetry.ModelTelemetry(\n",
    "    model,\n",
    "    parameters_every_n=100,\n",
    "    activations_every_n=100,\n",
    "    gradients_every_n=100,\n",
    "    activations_name_patterns=(\".*act.*\",),\n",
    "    gradients_name_patterns=(r\"conv\\d$\", r\"lin\\d\"),\n",
    "    parameters_name_patterns=(r\"conv\\d$\", r\"lin\\d\"),\n",
    ")\n",
    "model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = SGD(\n",
    "    model.parameters(),\n",
    "    lr=0.01,  # randomly chosen, not provided in the paper\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_func = nn.CrossEntropyLoss()\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "_iter = 0\n",
    "model.train()\n",
    "for epoch in tqdm.tqdm(range(n_epochs), desc=\"Epochs\", total=n_epochs):\n",
    "    for i, (xb, yb) in tqdm.tqdm(\n",
    "        enumerate(dataloader), desc=\"Batches\", total=len(dataloader)\n",
    "    ):\n",
    "        xb = xb.to(device)\n",
    "        yb = yb.to(device)\n",
    "        yb = conv_lecun1990.densify_y(yb)\n",
    "        loss = loss_func(model(xb), yb)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        model.loss_history_train(loss, _iter)\n",
    "        model.parameter_history(_iter)\n",
    "\n",
    "        _iter += 1\n",
    "\n",
    "    # compute validation loss\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        ys_pred, ys_true = [], []\n",
    "        for xb, yb in dataloader_valid:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            yb = conv_lecun1990.densify_y(yb)\n",
    "            yp = model(xb)\n",
    "            ys_pred.append(yp)\n",
    "            ys_true.append(yb)\n",
    "        y_pred = torch.cat(ys_pred, dim=0)\n",
    "        y_true = torch.cat(ys_true, dim=0)\n",
    "        loss_test = loss_func(y_pred, y_true)\n",
    "        model.loss_history_test(loss_test, _iter)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw_gradient_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw_activation_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw_loss_history_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw_loss_history_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drawing histograms of the weights and biases across training iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw_parameter_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.clean_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference for a few samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspecting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.to(device)\n",
    "pred_probs = model(train_features)\n",
    "pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pred_probs.cpu().detach().numpy().argmax(axis=1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].cpu()  # .reshape((28,28))\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "print(f\"Label: {label}, pred: {y_pred[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inspecting the effect of the learned filters on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_filters = model.model.conv1.n_out_channels.item()\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=n_filters // 3, ncols=n_filters // 4, figsize=(12, 12)\n",
    ")\n",
    "with torch.no_grad():\n",
    "    conv_features = model.model.act_conv1(\n",
    "        model.model.conv1(train_features.unsqueeze(1))\n",
    "    ).cpu()\n",
    "    for i, ax in enumerate(axs.flatten()):\n",
    "        ax.imshow(conv_features[0][i], cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "        ax.set_title(f\"Filter {i+1}\")\n",
    "\n",
    "plt.show()\n",
    "print(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "computing test set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_pred = []\n",
    "ys_true = []\n",
    "for test_features, test_labels in dataloader_test:\n",
    "    test_features = test_features.to(device)\n",
    "    pred_probs = model(test_features)\n",
    "\n",
    "    y_pred = pred_probs.detach().cpu().numpy().argmax(axis=1)\n",
    "\n",
    "    ys_true.append(test_labels.numpy())\n",
    "    ys_pred.append(y_pred)\n",
    "\n",
    "\n",
    "ys_true = np.concatenate(ys_true)\n",
    "ys_pred = np.concatenate(ys_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_true, ys_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(ys_true, ys_pred)\n",
    "error_rate = 1 - accuracy\n",
    "print(f\"* Accuracy: {accuracy:.2%}\")\n",
    "print(f\"* Error rate: {error_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accuracy: 95.75%\n",
    "* Error rate: 4.25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = metrics.ConfusionMatrixDisplay.from_predictions(ys_true, ys_pred)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using `Learner`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = rnnm_data.MNISTDatasetWithLabels(X0, y0)\n",
    "ds_valid = rnnm_data.MNISTDatasetWithLabels(X1, y1)\n",
    "ds_test = rnnm_data.MNISTDatasetWithLabels(X2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(\n",
    "    ds_train,\n",
    "    batch_size=1,\n",
    "    collate_fn=rnnm_data.collate_mnist_dataset_to_block_with_labels,\n",
    "    shuffle=True,\n",
    ")\n",
    "dl_valid = DataLoader(\n",
    "    ds_valid,\n",
    "    batch_size=200,\n",
    "    collate_fn=rnnm_data.collate_mnist_dataset_to_block_with_labels,\n",
    "    shuffle=False,\n",
    ")\n",
    "dl_test = DataLoader(\n",
    "    ds_test,\n",
    "    batch_size=200,\n",
    "    collate_fn=rnnm_data.collate_mnist_dataset_to_block_with_labels,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(dl_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = conv_lecun1990.Model2(\n",
    "    lecun_init=True, lecun_act=True, dtype=torch.float\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "lr = 1e-3\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr)  # , momentum=1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "# scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "#     optimizer=optimizer,\n",
    "#     max_lr=lr,\n",
    "#     epochs=n_epochs,\n",
    "#     steps_per_epoch=len(dl_train),\n",
    "# )\n",
    "loss = rnnm_losses.MSELossMNIST1HotLabel()\n",
    "save_dir = Path(\"./models\")\n",
    "\n",
    "loss_callback = rnnm_learner.TrainLossCallback()\n",
    "activations_callback = rnnm_learner.TrainActivationsCallback(\n",
    "    every_n=100, max_depth_search=4, name_patterns=(r\".*act.*\",)\n",
    ")\n",
    "gradients_callback = rnnm_learner.TrainGradientsCallback(\n",
    "    every_n=100, max_depth_search=4, name_patterns=(r\"conv\\d$\", r\"lin\\d\")\n",
    ")\n",
    "parameters_callback = rnnm_learner.TrainParametersCallback(\n",
    "    every_n=100, max_depth_search=4, name_patterns=(r\"conv\\d$\", r\"lin\\d\")\n",
    ")\n",
    "early_stopping_callback = rnnm_learner.EarlyStoppingCallback(patience=3)\n",
    "\n",
    "# scheduler_callback = rnnm_learner.EveryBatchSchedulerCallback(scheduler)\n",
    "callbacks = [\n",
    "    loss_callback,\n",
    "    activations_callback,\n",
    "    gradients_callback,\n",
    "    parameters_callback,\n",
    "    early_stopping_callback,\n",
    "    # scheduler_callback\n",
    "]\n",
    "\n",
    "lr_find_callback = rnnm_learner.LRFinderCallback(1e-5, 10, 100)\n",
    "\n",
    "learner = rnnm_learner.Learner(\n",
    "    model, optimizer, loss, callbacks=callbacks, save_dir=save_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.find_learning_rate(\n",
    "    dl_train, n_epochs=2, lr_find_callback=lr_find_callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find_callback.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(dl_train, n_epochs=n_epochs, dataloader_valid=dl_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_callback.plot(window=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_valid = loss_callback.get_losses_valid()\n",
    "losses_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_callback.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients_callback.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_callback.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "computing test set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = learner.predict(dl_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_pred = y_prob.argmax(dim=1).numpy()\n",
    "ys_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_true = np.array([int(v) for v in y2.values])\n",
    "ys_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.accuracy_score(ys_true, ys_pred)\n",
    "error_rate = 1 - accuracy\n",
    "print(f\"* Accuracy: {accuracy:.2%}\")\n",
    "print(f\"* Error rate: {error_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Accuracy: 95.15%\n",
    "* Error rate: 4.85%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = metrics.ConfusionMatrixDisplay.from_predictions(ys_true, ys_pred)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
