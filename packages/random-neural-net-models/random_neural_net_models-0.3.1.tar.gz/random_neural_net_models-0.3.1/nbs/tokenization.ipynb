{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "following along:\n",
    "* https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing\n",
    "* https://www.youtube.com/watch?v=zduSFxRajkE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tom lehrer's songs: https://tomlehrersongs.com/\n",
    "\n",
    "the elements song: https://tomlehrersongs.com/wp-content/uploads/2018/12/the-elements.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = \"\"\"THE ELEMENTS\n",
    "\n",
    "There's antimony, arsenic, aluminum, selenium,\n",
    "And hydrogen and oxygen and nitrogen and rhenium,\n",
    "And nickel, neodymium, neptunium, germanium,\n",
    "And iron, americium, ruthenium, uranium,\n",
    "\n",
    "Europium, zirconium, lutetium, vanadium,\n",
    "And lanthanum and osmium and astatine and radium,\n",
    "And gold and protactinium and indium and gallium,\n",
    "And iodine and thorium and thulium and thallium.\n",
    "\n",
    "There's yttrium, ytterbium, actinium, rubidium,\n",
    "And boron, gadolinium, niobium, iridium,\n",
    "And strontium and silicon and silver and samarium,\n",
    "And bismuth, bromine, lithium, beryllium, and barium.\n",
    "\n",
    "There's holmium and helium and hafnium and erbium,\n",
    "And phosphorus and francium and fluorine and terbium,\n",
    "And manganese and mercury, molybdenum, magnesium,\n",
    "Dysprosium and scandium and cerium and cesium.\n",
    "\n",
    "And lead, praseodymium and platinum, plutonium,\n",
    "Palladium, promethium, potassium, polonium,\n",
    "And tantalum, technetium, titanium, tellurium,\n",
    "And cadmium and calcium and chromium and curium.\n",
    "\n",
    "There's sulfur, californium and fermium, berkelium,\n",
    "And also mendelevium, einsteinium, nobelium,\n",
    "And argon, krypton, neon, radon, xenon, zinc and rhodium,\n",
    "And chlorine, carbon, cobalt, copper, tungsten, tin and sodium.\n",
    "\n",
    "These are the only ones o_f which the news has come to Ha'vard,\n",
    "And there may be many others but they haven't been discavard.\"\"\"\n",
    "import abc\n",
    "import string\n",
    "import typing as T\n",
    "from collections import Counter\n",
    "\n",
    "import regex\n",
    "import tqdm\n",
    "\n",
    "import random_neural_net_models.tokenization as rnnm_tok\n",
    "import random_neural_net_models.utils as utils\n",
    "\n",
    "phrase = \"From adolescence to senility, bypassing maturity.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic byte pair encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = rnnm_tok.text_to_ids(phrase)\n",
    "tokens[:5], tokens[-5:], len(tokens), len(set(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = rnnm_tok.get_stats(tokens)\n",
    "stats.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnm_tok.merge_token_ids(rnnm_tok.TokenIDs(ids=[5, 6, 6, 7, 9, 1]), (6, 7), 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_symbols = string.ascii_letters + string.digits\n",
    "base_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_ids = rnnm_tok.text_to_ids(base_symbols)\n",
    "base_ids[:5], base_ids[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_id = max(tokens + base_ids) + 1\n",
    "replacement_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_to_replace = stats.most_common()[0][0]\n",
    "pair_to_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens2 = rnnm_tok.merge_token_ids(tokens, pair_to_replace, replacement_id)\n",
    "tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(tokens2), max(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens), len(tokens2), len(set(tokens)), len(set(tokens2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(set(tokens)) + 20\n",
    "pair_map, tokens3 = rnnm_tok.repeated_merge(\n",
    "    tokens, vocab_size, return_new_ids=True, show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_map.map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pair_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokens), len(tokens3), len(set(tokens)), len(set(tokens3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in set(tokens + base_ids)}\n",
    "for (token0, token1), idx in pair_map.items():\n",
    "    vocab[idx] = vocab[token0] + vocab[token1]\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnm_tok.decode(tokens3, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bpe_token_ids = rnnm_tok.encode(\"bla bla and bla\", pair_map)\n",
    "test_bpe_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnm_tok.decode(test_bpe_token_ids, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/openai/gpt-2\n",
    "\n",
    "https://github.com/openai/tiktoken\n",
    "\n",
    "https://github.com/google/sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = rnnm_tok.TokenizerSimple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 60\n",
    "tokenizer.fit(lyrics, vocab_size, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_token_ids = tokenizer.encode(phrase)\n",
    "simple_token_ids[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(simple_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT4_SPLIT_PATTERN = r\"\"\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "pattern = regex.compile(GPT4_SPLIT_PATTERN)\n",
    "\n",
    "pattern.findall(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = Counter([1, 1, 1])\n",
    "c1 = Counter([1, 1, 2])\n",
    "c2 = Counter()\n",
    "c2.update(c0)\n",
    "c2.update(c1)\n",
    "c0, c1, c2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.lesswrong.com/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = rnnm_tok.GPT4_SPLIT_PATTERN\n",
    "tokenizer = rnnm_tok.TokenizerRegex()\n",
    "tokenizer.fit(lyrics, vocab_size, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_token_ids = tokenizer.encode(phrase)\n",
    "regex_token_ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(regex_token_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regex + special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_strings = \"\"\"\n",
    "<|endoftext|>Hello world this is one document\n",
    "<|endoftext|>And this is another document\n",
    "<|endoftext|><|fim_prefix|>And this one has<|fim_suffix|> tokens.<|fim_middle|> FIM\n",
    "<|endoftext|>Last document!!! ðŸ‘‹<|endofprompt|>\n",
    "\"\"\".strip()\n",
    "print(special_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token2id_map = {\n",
    "    \"<|endoftext|>\": 100257,\n",
    "    \"<|fim_prefix|>\": 100258,\n",
    "    \"<|fim_middle|>\": 100259,\n",
    "    \"<|fim_suffix|>\": 100260,\n",
    "    \"<|endofprompt|>\": 100276,\n",
    "}\n",
    "vocab_size = 200\n",
    "tokenizer = rnnm_tok.TokenizerRegex()\n",
    "tokenizer.fit(\n",
    "    lyrics, vocab_size=vocab_size, pattern=rnnm_tok.GPT4_SPLIT_PATTERN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.register_special_tokens(special_token2id_map)\n",
    "tokenizer.special_token2id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_ids = tokenizer.encode(special_strings)\n",
    "encoded_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = tokenizer.decode(encoded_ids)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(special_strings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
