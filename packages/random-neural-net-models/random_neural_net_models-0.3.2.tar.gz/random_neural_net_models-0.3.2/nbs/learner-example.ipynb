{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Learner` example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to do two things:\n",
    "\n",
    "1. show how to use [tensordict's](https://github.com/pytorch/tensordict) [dataclasses](https://github.com/pytorch/tensordict?tab=readme-ov-file#tensorclass) to simplify torch.Tensor handling amd\n",
    "2. show how to use a `Learner` class written to work with tensordict dataclasses and fastai style callbacks\n",
    "\n",
    "To keep the notebook mostly self-contained, datasets, datablocks, collation functions, the loss and the model are defined within this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as T\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn.datasets as sk_datasets\n",
    "import sklearn.model_selection as model_selection\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.modules.loss as torch_loss\n",
    "import torch.optim as optim\n",
    "from einops import rearrange\n",
    "from tensordict import tensorclass\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import random_neural_net_models.learner as rnnm_learner\n",
    "import random_neural_net_models.utils as utils\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and split dummy data for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sk_datasets.make_blobs(\n",
    "    n_samples=1_000,\n",
    "    n_features=2,\n",
    "    centers=2,\n",
    "    random_state=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0, X1, y0, y1 = model_selection.train_test_split(\n",
    "    X, y, test_size=0.2, random_state=SEED, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get device to be used by torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = utils.get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: training & validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define datasets for train and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyTrainingDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.n = len(X)\n",
    "        if X.shape[0] != y.shape[0]:\n",
    "            raise ValueError(\n",
    "                f\"X and y must have same length, got {X.shape[0]} and {y.shape[0]}\"\n",
    "            )\n",
    "        if y is not None and y.ndim > 1:\n",
    "            raise ValueError(f\"y must be 1-dimensional, got {y.ndim}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx: int) -> T.Tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = torch.from_numpy(self.X[[idx], :]).float()\n",
    "        y = torch.tensor([self.y[idx]])\n",
    "        y = rearrange(y, \"n -> n 1\")\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = NumpyTrainingDataset(X0, y0)\n",
    "ds_val = NumpyTrainingDataset(X1, y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a collation function to be used by torch `DataLoader` containing `X` and `y` within the tensordict dataclass `XyBlock` as well as a separate dataclass `XBlock`, only to be used for inference when `y` is unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tensorclass\n",
    "class XyBlock:\n",
    "    x: torch.Tensor\n",
    "    y: torch.Tensor\n",
    "\n",
    "\n",
    "def collate_numpy_dataset_to_xyblock(\n",
    "    input: T.Tuple[torch.Tensor, torch.Tensor]\n",
    ") -> XyBlock:\n",
    "    x = torch.concat([v[0] for v in input]).float()\n",
    "    y = torch.concat([v[1] for v in input]).float()\n",
    "    return XyBlock(x=x, y=y, batch_size=[x.shape[0]])\n",
    "\n",
    "\n",
    "@tensorclass\n",
    "class XBlock:\n",
    "    x: torch.Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_train = DataLoader(\n",
    "    ds_train,\n",
    "    batch_size=10,\n",
    "    collate_fn=collate_numpy_dataset_to_xyblock,\n",
    "    shuffle=True,\n",
    ")\n",
    "dl_val = DataLoader(\n",
    "    ds_val,\n",
    "    batch_size=10,\n",
    "    collate_fn=collate_numpy_dataset_to_xyblock,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a dense model that uses the `nn.Linear` projection and non-linearity `nn.Sigmoid` in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    def __init__(self, n_in: int, n_out: int):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(n_in, n_out)\n",
    "        gain = nn.init.calculate_gain(\"sigmoid\")\n",
    "        nn.init.xavier_normal_(self.lin.weight, gain=gain)\n",
    "        self.act = nn.Sigmoid()\n",
    "        self.net = nn.Sequential(self.lin, self.act)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_hidden: T.Tuple[int] = (10, 5, 1),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "\n",
    "        components = [\n",
    "            Layer(n_in, n_out)\n",
    "            for (n_in, n_out) in zip(n_hidden[:-1], n_hidden[1:])\n",
    "        ]\n",
    "\n",
    "        self.net = nn.Sequential(*components)\n",
    "\n",
    "    def forward(self, input: T.Union[XyBlock, XBlock]) -> torch.Tensor:\n",
    "        return self.net(input.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet(n_hidden=(2, 10, 5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customize the torch `BCELoss` so it takes as input our `XyBlock` dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCELoss(torch_loss.BCELoss):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, inference: torch.Tensor, input: XyBlock) -> torch.Tensor:\n",
    "        return super().forward(inference, input.y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate objets for basic functionality, like the optimizer, and optionally tracking of activations, gradients and parameters via callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2\n",
    "learning_rate = 0.1\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=1e-3)\n",
    "loss = BCELoss()\n",
    "loss_callback = rnnm_learner.TrainLossCallback()\n",
    "\n",
    "save_dir = Path(\n",
    "    \"./models\"\n",
    ")  # location used by learner.find_learning_rate to store the model before the search\n",
    "\n",
    "use_callbacks = True\n",
    "# the following callbacks are not strictly necessary for learning rate search and\n",
    "# training, but may make debugging of slow / unexpected training easier\n",
    "\n",
    "if use_callbacks:\n",
    "\n",
    "    # the name_patterns used below work only because of how DenseNet and Layer are defined, you may have to use different patterns\n",
    "    activations_callback = rnnm_learner.TrainActivationsCallback(\n",
    "        every_n=10, max_depth_search=4, name_patterns=(\".*act\",)\n",
    "    )\n",
    "    gradients_callback = rnnm_learner.TrainGradientsCallback(\n",
    "        every_n=10, max_depth_search=4, name_patterns=(\".*lin\",)\n",
    "    )\n",
    "    parameters_callback = rnnm_learner.TrainParametersCallback(\n",
    "        every_n=10, max_depth_search=4, name_patterns=(\".*lin\",)\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        loss_callback,\n",
    "        activations_callback,\n",
    "        gradients_callback,\n",
    "        parameters_callback,\n",
    "    ]\n",
    "else:\n",
    "    callbacks = [loss_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the learner object to handle training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = rnnm_learner.Learner(\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss,\n",
    "    callbacks=callbacks,\n",
    "    save_dir=save_dir,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find_callback = rnnm_learner.LRFinderCallback(1e-5, 100, 100)\n",
    "\n",
    "learner.find_learning_rate(\n",
    "    dl_train, n_epochs=2, lr_find_callback=lr_find_callback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal learning rate is usually where the loss has dipped a bit already. If this is done go up to the code cell where `learning_rate` is defined and set the new value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_find_callback.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e1\n",
    "\n",
    "if use_callbacks:\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer=optimizer,\n",
    "        max_lr=learning_rate,\n",
    "        epochs=n_epochs,\n",
    "        steps_per_epoch=len(dl_train),\n",
    "    )\n",
    "    scheduler_callback = rnnm_learner.EveryBatchSchedulerCallback(scheduler)\n",
    "    learner.update_callback(scheduler_callback)\n",
    "\n",
    "else:\n",
    "    rnnm_learner.set_optimizer_hyperparameter(optimizer, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.fit(dl_train, n_epochs=n_epochs, dataloader_valid=dl_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot model training stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_callback.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_callbacks:\n",
    "    parameters_callback.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_callbacks:\n",
    "    gradients_callback.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_callbacks:\n",
    "    activations_callback.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some inference to visualize model generalisation. First we define arrays containing a grid of the feature values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
    "x1 = np.linspace(X[:, 1].min(), X[:, 1].max(), 100)\n",
    "X0, X1 = np.meshgrid(x0, x1)\n",
    "X_plot = np.array([X0.ravel(), X1.ravel()]).T\n",
    "X_plot[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a dataset and collation function for a dataloader dedicated to inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumpyInferenceDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray):\n",
    "        self.X = X\n",
    "        self.n = len(X)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        x = torch.from_numpy(self.X[[idx], :]).float()\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "def collate_numpy_dataset_to_xblock(input: T.Tuple[torch.Tensor]) -> XBlock:\n",
    "    x = torch.concat(input).float()\n",
    "    return XBlock(x=x, batch_size=[x.shape[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_plot = NumpyInferenceDataset(X_plot)\n",
    "dl_plot = DataLoader(\n",
    "    ds_plot, batch_size=5, collate_fn=collate_numpy_dataset_to_xblock\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = learner.predict(dl_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prob = y_prob.detach().numpy()\n",
    "y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "im = ax.pcolormesh(X0, X1, y_prob.reshape(X0.shape), alpha=0.2)\n",
    "fig.colorbar(im, ax=ax)\n",
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, ax=ax, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
