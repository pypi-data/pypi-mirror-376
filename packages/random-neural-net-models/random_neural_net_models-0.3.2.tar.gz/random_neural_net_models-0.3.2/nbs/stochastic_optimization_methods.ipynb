{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD vs RMSProp vs Adam for logistic regression on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* SGD\n",
    "* RMSProp\n",
    "* Kingma et al. 2017, [Adam: A Method for Stochastic Optimization](http://arxiv.org/abs/1412.6980) -> Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import typing as T\n",
    "from collections import defaultdict\n",
    "from enum import Enum\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchinfo\n",
    "import tqdm\n",
    "from einops.layers.torch import Rearrange\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import random_neural_net_models.convolution_lecun1990 as conv_lecun1990\n",
    "import random_neural_net_models.telemetry as telemetry\n",
    "import random_neural_net_models.utils as rnnm_utils\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnm_utils.make_deterministic(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = rnnm_utils.get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_openml(\"Fashion-MNIST\", version=1, cache=True, parser=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = mnist[\"data\"]\n",
    "y = mnist[\"target\"]\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X0, X1, y0, y1 = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = conv_lecun1990.DigitsDataset(X0, y0)\n",
    "ds_test = conv_lecun1990.DigitsDataset(X1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "dataloader = DataLoader(ds, batch_size=batch_size, shuffle=True)\n",
    "dataloader_test = DataLoader(ds_test, batch_size=500, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Model` & `do_epoch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression model\n",
    "\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, h: int, w: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.rectangle2flat = Rearrange(\"b h w -> b (h w)\", h=h, w=w)\n",
    "        self.linear = nn.Linear(h * w, output_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(self.rectangle2flat(x))\n",
    "\n",
    "\n",
    "def do_epoch(\n",
    "    model: nn.Module,\n",
    "    dataloader: DataLoader,\n",
    "    dataloader_test: DataLoader,\n",
    "    opt: optim.Optimizer,\n",
    "    _iter: int,\n",
    ") -> int:\n",
    "    # training part\n",
    "    for X_batch, y_batch in dataloader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        logits = model(X_batch)\n",
    "\n",
    "        loss = F.cross_entropy(logits, y_batch)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        model.loss_history_train(loss, _iter)\n",
    "        model.parameter_history(_iter)\n",
    "\n",
    "        _iter += 1\n",
    "\n",
    "    # validation part\n",
    "    with torch.no_grad():\n",
    "        all_logits, all_targets = [], []\n",
    "        for X_batch, y_batch in dataloader_test:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            logits = model(X_batch)\n",
    "            all_logits.append(logits)\n",
    "            all_targets.append(y_batch)\n",
    "\n",
    "        all_logits = torch.cat(all_logits, dim=0)\n",
    "        all_targets = torch.cat(all_targets, dim=0)\n",
    "        loss = F.cross_entropy(all_logits, all_targets)\n",
    "        model.loss_history_test(loss, _iter)\n",
    "\n",
    "    return _iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_training(\n",
    "    model: nn.Module,\n",
    "    n_epochs: int,\n",
    "    dataloader: DataLoader,\n",
    "    dataloader_test: DataLoader,\n",
    "    opt: optim.Optimizer,\n",
    "):\n",
    "    _iter = 0\n",
    "    for _ in tqdm.tqdm(range(n_epochs), total=n_epochs, desc=\"Epoch\"):\n",
    "        _iter = do_epoch(model, dataloader, dataloader_test, opt, _iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySGD(optim.Optimizer):\n",
    "    def __init__(self, params: T.Generator, lr: float = 0.01):\n",
    "        super(MySGD, self).__init__(params, {\"lr\": lr})\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                p.data -= group[\"lr\"] * p.grad\n",
    "\n",
    "\n",
    "class MySGDWithMomentum(optim.Optimizer):\n",
    "    # https://paperswithcode.com/method/sgd-with-momentum\n",
    "    # https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d\n",
    "    def __init__(\n",
    "        self, params: T.Generator, lr: float = 0.01, momentum: float = 0.9\n",
    "    ):\n",
    "        super(MySGDWithMomentum, self).__init__(params, {\"lr\": lr})\n",
    "        self.momentum = momentum\n",
    "        self.state = defaultdict(dict)\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                self.state[p][\"momentum\"] = torch.zeros_like(p.data)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                self.state[p][\"momentum\"] = (\n",
    "                    self.momentum * self.state[p][\"momentum\"]\n",
    "                    + (1 - self.momentum) * p.grad\n",
    "                )\n",
    "                p.data -= group[\"lr\"] * self.state[p][\"momentum\"]\n",
    "\n",
    "\n",
    "class MyAdam(optim.Optimizer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: T.Generator,\n",
    "        alpha: float = 0.001,\n",
    "        betas: T.Tuple[float] = (0.9, 0.999),\n",
    "        eps: float = 1e-8,\n",
    "    ):\n",
    "        super(MyAdam, self).__init__(\n",
    "            params, {\"alpha\": alpha, \"eps\": eps, \"betas\": betas}\n",
    "        )\n",
    "\n",
    "        self.state = defaultdict(dict)\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                self.state[p][\"momentum_m\"] = torch.zeros_like(p.data)\n",
    "                self.state[p][\"momentum_v\"] = torch.zeros_like(p.data)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                self.state[p][\"momentum_m\"] = (\n",
    "                    group[\"betas\"][0] * self.state[p][\"momentum_m\"]\n",
    "                    + (1 - group[\"betas\"][0]) * p.grad\n",
    "                )\n",
    "                self.state[p][\"momentum_v\"] = group[\"betas\"][1] * self.state[p][\n",
    "                    \"momentum_v\"\n",
    "                ] + (1 - group[\"betas\"][1]) * p.grad.pow(2)\n",
    "                m_hat = self.state[p][\"momentum_m\"] / (1 - group[\"betas\"][0])\n",
    "                v_hat = self.state[p][\"momentum_v\"] / (1 - group[\"betas\"][1])\n",
    "                p.data -= group[\"alpha\"] * m_hat / (v_hat.sqrt() + group[\"eps\"])\n",
    "\n",
    "\n",
    "class MyRMSProp(optim.Optimizer):\n",
    "    # https://optimization.cbe.cornell.edu/index.php?title=RMSProp\n",
    "    def __init__(\n",
    "        self,\n",
    "        params: T.Generator,\n",
    "        lr: float = 0.001,\n",
    "        momentum: float = 0.9,\n",
    "        eps: float = 1e-8,\n",
    "    ):\n",
    "        super(MyRMSProp, self).__init__(\n",
    "            params, {\"lr\": lr, \"eps\": eps, \"momentum\": momentum}\n",
    "        )\n",
    "\n",
    "        self.state = defaultdict(dict)\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                self.state[p][\"momentum\"] = torch.zeros_like(p.data)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                self.state[p][\"momentum\"] = group[\"momentum\"] * self.state[p][\n",
    "                    \"momentum\"\n",
    "                ] + (1 - group[\"momentum\"]) * p.grad.pow(2)\n",
    "                p.data -= (\n",
    "                    group[\"lr\"]\n",
    "                    / (self.state[p][\"momentum\"].sqrt() + group[\"eps\"])\n",
    "                    * p.grad\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a training using a single optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(28, 28, 10)\n",
    "model = telemetry.ModelTelemetry(\n",
    "    model,\n",
    "    loss_names=(\"total\",),\n",
    "    loss_train_every_n=1,\n",
    "    loss_test_every_n=1,\n",
    "    parameters_name_patterns=(\"linear\",),\n",
    ")\n",
    "model = model.to(device).double()\n",
    "\n",
    "# define the optimizer\n",
    "# opt = MySGD(\n",
    "#     model.parameters(),\n",
    "#     lr=0.01,\n",
    "# )\n",
    "# opt = MySGDWithMomentum(\n",
    "#     model.parameters(),\n",
    "#     lr=0.01,\n",
    "#     momentum=0.9,\n",
    "# )\n",
    "# opt = MyAdam(\n",
    "#     model.parameters(),\n",
    "#     alpha=0.001,\n",
    "#     betas=(0.9, 0.999),\n",
    "# )\n",
    "opt = MyRMSProp(\n",
    "    model.parameters(),\n",
    "    lr=0.01,\n",
    "    momentum=0.9,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2\n",
    "do_training(model, n_epochs, dataloader, dataloader_test, opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.draw_loss_history_train()\n",
    "model.draw_loss_history_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running training for multiple optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerType(Enum):\n",
    "    SGD: str = \"SGD\"\n",
    "    Adam: str = \"Adam\"\n",
    "    RMSProp: str = \"RMSProp\"\n",
    "    MySGD: str = \"MySGD\"\n",
    "    MySGDWithMomentum: str = \"MySGDWithMomentum\"\n",
    "    MyAdam: str = \"MyAdam\"\n",
    "    MyRMSProp: str = \"MyRMSProp\"\n",
    "\n",
    "\n",
    "def get_optimizer(\n",
    "    name: OptimizerType, model_params: dict, optimizer_params: dict\n",
    ") -> optim.Optimizer:\n",
    "    if name == OptimizerType.SGD:\n",
    "        return optim.SGD(model_params, **optimizer_params)\n",
    "    elif name == OptimizerType.Adam:\n",
    "        return optim.Adam(model_params, **optimizer_params)\n",
    "    elif name == OptimizerType.MySGD:\n",
    "        return MySGD(model_params, **optimizer_params)\n",
    "    elif name == OptimizerType.MySGDWithMomentum:\n",
    "        return MySGDWithMomentum(model_params, **optimizer_params)\n",
    "    elif name == OptimizerType.MyAdam:\n",
    "        return MyAdam(model_params, **optimizer_params)\n",
    "    elif name == OptimizerType.MyRMSProp:\n",
    "        return MyRMSProp(model_params, **optimizer_params)\n",
    "    elif name == OptimizerType.RMSProp:\n",
    "        return optim.RMSprop(model_params, **optimizer_params)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer {name}\")\n",
    "\n",
    "\n",
    "def train_with_multiple_optimizers(\n",
    "    n_epochs: int,\n",
    "    dataloader: DataLoader,\n",
    "    dataloader_test: DataLoader,\n",
    "    optimizer_params: T.Dict[str, dict],\n",
    ") -> T.Dict[str, telemetry.ModelTelemetry]:\n",
    "    models = {}\n",
    "    for optimizer_name, optimizer_params in optimizer_params.items():\n",
    "        # define the model\n",
    "        model = LogisticRegression(28, 28, 10)\n",
    "        model = telemetry.ModelTelemetry(\n",
    "            model,\n",
    "            loss_names=(\"total\",),\n",
    "            loss_train_every_n=1,\n",
    "            loss_test_every_n=1,\n",
    "            parameters_name_patterns=(\"linear\",),\n",
    "        )\n",
    "        model = model.to(device).double()\n",
    "\n",
    "        # define the optimizer\n",
    "        opt = get_optimizer(\n",
    "            OptimizerType(optimizer_name), model.parameters(), optimizer_params\n",
    "        )\n",
    "\n",
    "        do_training(model, n_epochs, dataloader, dataloader_test, opt)\n",
    "\n",
    "        models[optimizer_name] = model\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "def get_rolling_loss_df(\n",
    "    models: T.Dict[str, telemetry.ModelTelemetry], group: str = \"train\"\n",
    ") -> pd.DataFrame:\n",
    "    losses = [\n",
    "        getattr(model, f\"loss_history_{group}\")\n",
    "        .get_rolling_mean_df()\n",
    "        .assign(optimizer=optimizer_name)\n",
    "        for optimizer_name, model in models.items()\n",
    "    ]\n",
    "    losses = pd.concat(losses, ignore_index=True)\n",
    "    return losses\n",
    "\n",
    "\n",
    "def plot_losses_for_optimizers(\n",
    "    models: T.Dict[str, telemetry.ModelTelemetry],\n",
    "    alpha: float = 0.5,\n",
    "    figsize: T.Tuple[int, int] = (10, 7),\n",
    "):\n",
    "    fig, axs = plt.subplots(nrows=2, figsize=figsize)\n",
    "\n",
    "    # train\n",
    "    ax = axs[0]\n",
    "    losses = get_rolling_loss_df(models, \"train\")\n",
    "    sns.lineplot(\n",
    "        data=losses, x=\"iter\", y=\"total\", hue=\"optimizer\", ax=ax, alpha=alpha\n",
    "    )\n",
    "    ax.set_title(\"Train loss\")\n",
    "\n",
    "    # test\n",
    "    ax = axs[1]\n",
    "    losses = get_rolling_loss_df(models, \"test\")\n",
    "    sns.lineplot(\n",
    "        data=losses, x=\"iter\", y=\"total\", hue=\"optimizer\", ax=ax, alpha=alpha\n",
    "    )\n",
    "    ax.set_title(\"Test loss\")\n",
    "\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_params = {\n",
    "    \"SGD\": {\"lr\": 0.1, \"momentum\": 0.9, \"nesterov\": True},\n",
    "    \"Adam\": {\"lr\": 0.001, \"betas\": (0.9, 0.999)},\n",
    "    \"RMSProp\": {\"lr\": 0.001, \"momentum\": 0.9},\n",
    "    \"MySGD\": {\"lr\": 0.1},\n",
    "    \"MySGDWithMomentum\": {\"lr\": 0.1, \"momentum\": 0.9},\n",
    "    \"MyAdam\": {\"alpha\": 0.001, \"betas\": (0.9, 0.999)},\n",
    "    \"MyRMSProp\": {\"lr\": 0.001, \"momentum\": 0.9},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "models = train_with_multiple_optimizers(\n",
    "    n_epochs, dataloader, dataloader_test, optimizer_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_for_optimizers(models, figsize=(10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
