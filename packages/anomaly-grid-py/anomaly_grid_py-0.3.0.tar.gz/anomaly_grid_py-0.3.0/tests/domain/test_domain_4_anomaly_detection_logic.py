#!/usr/bin/env python3
"""
ðŸ”¬ DOMAIN 4: ANOMALY DETECTION LOGIC

This test validates that the anomaly detection library correctly implements
fundamental anomaly detection principles and logical consistency.
"""


import numpy as np
from anomaly_grid_py import AnomalyDetector, roc_auc_score


class TestAnomalyDetectionLogic:
    """Test suite for anomaly detection logic."""

    def setup_method(self):
        """Set up test fixtures."""
        self.detector = AnomalyDetector(max_order=3)

        # Create clear normal patterns for training
        self.normal_patterns = [
            ["LOGIN", "BALANCE", "LOGOUT"],
            ["LOGIN", "WITHDRAW", "LOGOUT"],
            ["LOGIN", "TRANSFER", "LOGOUT"],
            ["LOGIN", "DEPOSIT", "LOGOUT"],
            ["LOGIN", "BALANCE", "TRANSFER", "LOGOUT"],
            ["LOGIN", "WITHDRAW", "BALANCE", "LOGOUT"],
        ]

        # Repeat patterns for statistical significance
        self.training_sequences = []
        for pattern in self.normal_patterns:
            for _ in range(50):
                self.training_sequences.append(pattern)

        # Train the detector
        self.detector.fit(self.training_sequences)

        # Define test sequences
        self.normal_test_sequences = [
            ["LOGIN", "BALANCE", "LOGOUT"],
            ["LOGIN", "WITHDRAW", "LOGOUT"],
            ["LOGIN", "TRANSFER", "LOGOUT"],
        ]

        self.anomalous_test_sequences = [
            ["HACK", "EXPLOIT", "STEAL"],
            ["MALWARE", "INFILTRATE", "EXFILTRATE"],
            ["BREACH", "ESCALATE", "PERSIST"],
            ["LOGIN", "HACK", "LOGOUT"],  # Partially anomalous
            ["ATTACK", "PENETRATE", "ESCAPE"],
        ]

    def test_4_1_anomaly_definition_consistency(self):
        """
        Test 4.1: Anomaly Definition Consistency

        Verify that the anomaly detection logic correctly distinguishes
        between normal and anomalous patterns with consistent scoring.
        """
        print("\nðŸ”¬ DOMAIN 4: ANOMALY DETECTION LOGIC")
        print("=" * 36)
        print("\nTest 4.1: Anomaly Definition Consistency")
        print("-" * 40)
        print("  Testing anomaly definition consistency...")

        # Test normal pattern detection
        print("    Testing normal pattern detection")
        normal_scores = self.detector.predict_proba(self.normal_test_sequences)

        for i, score in enumerate(normal_scores):
            print(f"      Normal sequence {i}: max anomaly score = {score:.6f}")
            # Normal patterns should have low anomaly scores
            assert (
                score < 0.2
            ), f"Normal sequence {i} has high anomaly score: {score:.6f}"

        # Test anomalous pattern detection
        print("    Testing anomalous pattern detection")
        anomalous_scores = self.detector.predict_proba(self.anomalous_test_sequences)

        for i, score in enumerate(anomalous_scores):
            print(f"      Anomalous sequence {i}: max anomaly score = {score:.6f}")
            # Anomalous patterns should have higher anomaly scores
            # Note: Some tolerance needed as the model might not see all patterns as highly anomalous

        # Test score ordering
        print("    Testing score ordering")
        normal_max_score = max(normal_scores)
        anomalous_min_score = min(anomalous_scores)

        print(f"      Normal max score: {normal_max_score:.6f}")
        print(f"      Anomalous min score: {anomalous_min_score:.6f}")

        # There should be some separation between normal and anomalous scores
        # (though perfect separation is not always expected)
        separation = anomalous_min_score - normal_max_score
        print(f"      Score separation: {separation:.6f}")

        # Test threshold consistency
        print("    Testing threshold consistency")
        thresholds = [0.0, 0.1, 0.5, 0.9]

        all_test_sequences = self.normal_test_sequences + self.anomalous_test_sequences

        for threshold in thresholds:
            predictions = self.detector.predict(all_test_sequences, threshold=threshold)
            anomaly_count = np.sum(predictions)
            print(f"      Threshold {threshold}: {anomaly_count} anomalies detected")

            # Higher thresholds should generally detect fewer anomalies
            assert (
                0 <= anomaly_count <= len(all_test_sequences)
            ), f"Invalid anomaly count: {anomaly_count}"

        print("    âœ… Anomaly definition consistency validated")

    def test_4_2_likelihood_based_detection(self):
        """
        Test 4.2: Likelihood-Based Detection

        Verify that anomaly detection is properly based on likelihood
        calculations and statistical principles.
        """
        print("\nTest 4.2: Likelihood-Based Detection")
        print("-" * 32)
        print("  Testing likelihood-based detection...")

        # Test likelihood calculation
        print("    Testing likelihood calculation")

        # High probability sequences (from training)
        high_prob_sequences = [
            ["LOGIN", "BALANCE", "LOGOUT"],
            ["LOGIN", "WITHDRAW", "LOGOUT"],
        ]

        # Low probability sequences (unseen combinations)
        low_prob_sequences = [
            ["LOGOUT", "LOGIN", "BALANCE"],  # Reversed order
            ["BALANCE", "WITHDRAW", "TRANSFER"],  # No LOGIN/LOGOUT
            ["UNKNOWN", "PATTERN", "HERE"],  # Completely unseen
        ]

        high_prob_scores = self.detector.predict_proba(high_prob_sequences)
        low_prob_scores = self.detector.predict_proba(low_prob_sequences)

        print("      High probability sequences:")
        for i, score in enumerate(high_prob_scores):
            print(f"        Sequence {i}: anomaly score = {score:.6f}")

        print("      Low probability sequences:")
        for i, score in enumerate(low_prob_scores):
            print(f"        Sequence {i}: anomaly score = {score:.6f}")

        # Verify likelihood ordering
        avg_high_prob_score = np.mean(high_prob_scores)
        avg_low_prob_score = np.mean(low_prob_scores)

        print(f"      Average high probability score: {avg_high_prob_score:.6f}")
        print(f"      Average low probability score: {avg_low_prob_score:.6f}")

        # Low probability sequences should have higher anomaly scores
        assert avg_low_prob_score > avg_high_prob_score, "Likelihood ordering violation"

        print("    âœ… Likelihood-based detection validated")

    def test_4_3_threshold_sensitivity_analysis(self):
        """
        Test 4.3: Threshold Sensitivity Analysis

        Analyze how the detection system responds to different threshold
        values and verify logical consistency.
        """
        print("\nTest 4.3: Threshold Sensitivity Analysis")
        print("-" * 36)
        print("  Testing threshold sensitivity...")

        # Create a mixed dataset
        mixed_sequences = self.normal_test_sequences + self.anomalous_test_sequences
        true_labels = [0] * len(self.normal_test_sequences) + [1] * len(
            self.anomalous_test_sequences
        )

        # Test multiple thresholds
        thresholds = [0.01, 0.05, 0.1, 0.2, 0.3, 0.5, 0.7, 0.9]

        print("    Threshold analysis:")
        print("    Threshold | Detected | Precision | Recall | F1-Score")
        print("    " + "-" * 50)

        previous_detected = len(mixed_sequences) + 1  # Start high

        for threshold in thresholds:
            predictions = self.detector.predict(mixed_sequences, threshold=threshold)
            detected_count = np.sum(predictions)

            # Calculate metrics
            tp = np.sum((np.array(true_labels) == 1) & (predictions == 1))
            fp = np.sum((np.array(true_labels) == 0) & (predictions == 1))
            fn = np.sum((np.array(true_labels) == 1) & (predictions == 0))

            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1 = (
                2 * precision * recall / (precision + recall)
                if (precision + recall) > 0
                else 0.0
            )

            print(
                f"    {threshold:9.2f} | {detected_count:8d} | {precision:9.3f} | {recall:6.3f} | {f1:8.3f}"
            )

            # Verify monotonicity: higher thresholds should detect fewer or equal anomalies
            assert (
                detected_count <= previous_detected
            ), f"Threshold monotonicity violation at {threshold}"
            previous_detected = detected_count

        print("    âœ… Threshold sensitivity validated")

    def test_4_4_roc_auc_performance(self):
        """
        Test 4.4: ROC-AUC Performance

        Evaluate the detector's performance using ROC-AUC metrics
        to ensure it provides meaningful discrimination.
        """
        print("\nTest 4.4: ROC-AUC Performance")
        print("-" * 26)
        print("  Testing ROC-AUC performance...")

        # Create balanced test set
        test_sequences = self.normal_test_sequences + self.anomalous_test_sequences
        true_labels = [0] * len(self.normal_test_sequences) + [1] * len(
            self.anomalous_test_sequences
        )

        # Get anomaly scores
        scores = self.detector.predict_proba(test_sequences)

        # Calculate ROC-AUC
        try:
            auc = roc_auc_score(true_labels, scores)
            print(f"    ROC-AUC Score: {auc:.3f}")

            # Interpret AUC score
            if auc >= 0.9:
                performance = "Excellent"
            elif auc >= 0.8:
                performance = "Good"
            elif auc >= 0.7:
                performance = "Fair"
            elif auc >= 0.6:
                performance = "Poor"
            else:
                performance = "Very Poor"

            print(f"    Performance: {performance}")

            # AUC should be better than random (0.5)
            assert auc > 0.5, f"AUC {auc:.3f} is not better than random"

            # For a well-designed detector, we expect reasonable performance
            if auc < 0.6:
                print(
                    f"    âš ï¸  Warning: Low AUC score ({auc:.3f}) - detector may need tuning"
                )

        except Exception as e:
            print(f"    âš ï¸  Could not calculate ROC-AUC: {e}")
            # Still validate that scores show some discrimination
            normal_avg = np.mean(scores[: len(self.normal_test_sequences)])
            anomalous_avg = np.mean(scores[len(self.normal_test_sequences) :])
            print(f"    Normal average score: {normal_avg:.3f}")
            print(f"    Anomalous average score: {anomalous_avg:.3f}")

            # There should be some difference
            assert (
                abs(anomalous_avg - normal_avg) > 0.01
            ), "No discrimination between normal and anomalous"

        print("    âœ… ROC-AUC performance validated")

    def test_4_5_sequence_length_robustness(self):
        """
        Test 4.5: Sequence Length Robustness

        Verify that the detector handles sequences of different lengths
        appropriately and maintains logical consistency.
        """
        print("\nTest 4.5: Sequence Length Robustness")
        print("-" * 32)
        print("  Testing sequence length robustness...")

        # Test sequences of different lengths
        length_tests = [
            # Short sequences
            (["LOGIN", "LOGOUT"], "Short normal"),
            (["HACK", "STEAL"], "Short anomalous"),
            # Medium sequences
            (["LOGIN", "BALANCE", "LOGOUT"], "Medium normal"),
            (["HACK", "EXPLOIT", "STEAL"], "Medium anomalous"),
            # Long sequences
            (
                ["LOGIN", "BALANCE", "TRANSFER", "WITHDRAW", "BALANCE", "LOGOUT"],
                "Long normal",
            ),
            (
                ["HACK", "EXPLOIT", "ESCALATE", "PERSIST", "EXFILTRATE", "ESCAPE"],
                "Long anomalous",
            ),
            # Very long sequences
            (["LOGIN"] + ["BALANCE", "WITHDRAW"] * 5 + ["LOGOUT"], "Very long normal"),
            (
                ["ATTACK"] + ["EXPLOIT", "PERSIST"] * 5 + ["ESCAPE"],
                "Very long anomalous",
            ),
        ]

        print("    Testing different sequence lengths:")

        for sequence, description in length_tests:
            score = self.detector.predict_proba([sequence])[0]
            print(
                f"      {description:20s} (len={len(sequence):2d}): score = {score:.6f}"
            )

            # Verify score is reasonable
            assert (
                0.0 <= score <= 1.0
            ), f"Score out of bounds for {description}: {score}"

        # Test that very short sequences are handled gracefully
        try:
            single_element = self.detector.predict_proba([["LOGIN"]])[0]
            print(f"      Single element score: {single_element:.6f}")
        except Exception as e:
            print(f"      Single element handling: {e}")

        print("    âœ… Sequence length robustness validated")

    def test_4_6_consistency_and_reproducibility(self):
        """
        Test 4.6: Consistency and Reproducibility

        Verify that the detector produces consistent results for
        identical inputs and maintains reproducibility.
        """
        print("\nTest 4.6: Consistency and Reproducibility")
        print("-" * 37)
        print("  Testing consistency and reproducibility...")

        # Test identical sequences multiple times
        test_sequence = [["LOGIN", "BALANCE", "LOGOUT"]]

        scores = []
        for i in range(5):
            score = self.detector.predict_proba(test_sequence)[0]
            scores.append(score)
            print(f"      Run {i+1}: score = {score:.6f}")

        # All scores should be identical
        score_variance = np.var(scores)
        print(f"      Score variance: {score_variance:.10f}")

        assert (
            score_variance < 1e-10
        ), f"Inconsistent scores: variance = {score_variance}"

        # Test with different but equivalent sequences
        equivalent_sequences = [
            ["LOGIN", "BALANCE", "LOGOUT"],
            ["LOGIN", "BALANCE", "LOGOUT"],  # Identical
        ]

        equiv_scores = self.detector.predict_proba(equivalent_sequences)
        print(f"      Equivalent sequence 1: {equiv_scores[0]:.6f}")
        print(f"      Equivalent sequence 2: {equiv_scores[1]:.6f}")

        assert (
            abs(equiv_scores[0] - equiv_scores[1]) < 1e-10
        ), "Identical sequences have different scores"

        # Test prediction consistency
        predictions1 = self.detector.predict(equivalent_sequences, threshold=0.1)
        predictions2 = self.detector.predict(equivalent_sequences, threshold=0.1)

        assert np.array_equal(predictions1, predictions2), "Inconsistent predictions"

        print("    âœ… Consistency and reproducibility validated")

    def test_4_7_edge_cases_and_error_handling(self):
        """
        Test 4.7: Edge Cases and Error Handling

        Test the detector's behavior with edge cases and verify
        proper error handling.
        """
        print("\nTest 4.7: Edge Cases and Error Handling")
        print("-" * 35)
        print("  Testing edge cases and error handling...")

        # Test empty sequences (should raise error)
        print("    Testing empty sequence handling:")
        try:
            self.detector.predict_proba([[]])
            print("      âŒ Empty sequence should raise error")
            assert False, "Empty sequence should raise error"
        except (ValueError, TypeError) as e:
            print(f"      âœ… Empty sequence correctly rejected: {type(e).__name__}")

        # Test single element sequences
        print("    Testing single element sequences:")
        try:
            score = self.detector.predict_proba([["SINGLE"]])[0]
            print(f"      Single element score: {score:.6f}")
        except Exception as e:
            print(f"      Single element error: {e}")

        # Test very long sequences
        print("    Testing very long sequences:")
        very_long_seq = ["A"] * 1000
        try:
            score = self.detector.predict_proba([very_long_seq])[0]
            print(f"      Very long sequence (1000 elements): {score:.6f}")
            assert 0.0 <= score <= 1.0, "Score out of bounds for very long sequence"
        except Exception as e:
            print(f"      Very long sequence error: {e}")

        # Test sequences with special characters
        print("    Testing special character handling:")
        special_sequences = [
            ["", "NORMAL"],  # Empty string element
            ["NORMAL", ""],  # Empty string element
            ["SPECIAL!@#", "CHARS$%^"],  # Special characters
            ["123", "456", "789"],  # Numeric strings
            ["UNICODE_Ã±", "UNICODE_Ã©"],  # Unicode characters
        ]

        for i, seq in enumerate(special_sequences):
            try:
                score = self.detector.predict_proba([seq])[0]
                print(f"      Special sequence {i+1}: {score:.6f}")
            except Exception as e:
                print(f"      Special sequence {i+1} error: {type(e).__name__}: {e}")

        print("    âœ… Edge cases and error handling validated")

    def run_domain_4_tests(self):
        """Run all Domain 4 tests and provide summary."""
        print("\nðŸ† DOMAIN 4 SUMMARY")
        print("=" * 19)

        test_methods = [
            self.test_4_1_anomaly_definition_consistency,
            self.test_4_2_likelihood_based_detection,
            self.test_4_3_threshold_sensitivity_analysis,
            self.test_4_4_roc_auc_performance,
            self.test_4_5_sequence_length_robustness,
            self.test_4_6_consistency_and_reproducibility,
            self.test_4_7_edge_cases_and_error_handling,
        ]

        passed_tests = 0
        total_tests = len(test_methods)

        for test_method in test_methods:
            try:
                test_method()
                passed_tests += 1
            except Exception as e:
                print(f"âŒ {test_method.__name__} failed: {e}")

        print(f"\nâœ… Domain 4 Results: {passed_tests}/{total_tests} tests passed")
        print("âœ… Anomaly detection logic validated")

        return passed_tests == total_tests


if __name__ == "__main__":
    test_suite = TestAnomalyDetectionLogic()
    test_suite.setup_method()
    success = test_suite.run_domain_4_tests()

    if success:
        print("\nðŸŽ‰ All Domain 4 tests passed!")
        exit(0)
    else:
        print("\nâŒ Some Domain 4 tests failed!")
        exit(1)
