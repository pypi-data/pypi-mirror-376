"""Code chunk processor for creating enhanced code chunk documents."""

from typing import Any

import structlog

from qdrant_loader.core.chunking.strategy.base.chunk_processor import BaseChunkProcessor
from qdrant_loader.core.chunking.strategy.code.processor.analysis import (
    analyze_code_content,
    extract_language_context,
)
from qdrant_loader.core.chunking.strategy.code.processor.quality import (
    assess_code_quality,
    assess_educational_value,
    calculate_reusability_score,
)
from qdrant_loader.core.document import Document

logger = structlog.get_logger(__name__)


class CodeChunkProcessor(BaseChunkProcessor):
    """Chunk processor for code documents with programming language context."""

    def __init__(self, settings):
        super().__init__(settings)
        self.logger = logger
        self.code_config = getattr(
            settings.global_config.chunking.strategies, "code", None
        )
        self.max_chunk_size_for_nlp = getattr(
            self.code_config, "max_chunk_size_for_nlp", 20000
        )
        self.skip_conditions = {
            "large_content": self.max_chunk_size_for_nlp,
            "binary_patterns": ["\x00", "\xff", "\xfe"],
            "minified_code_threshold": 0.1,
            "generated_code_patterns": [
                "auto-generated",
                "do not edit",
                "generated by",
            ],
        }

    def create_chunk_document(
        self,
        original_doc: Document,
        chunk_content: str,
        chunk_index: int,
        total_chunks: int,
        chunk_metadata: dict[str, Any],
        skip_nlp: bool = False,
    ) -> Document:
        chunk_id = self.generate_chunk_id(original_doc, chunk_index)
        base_metadata = self.create_base_chunk_metadata(
            original_doc, chunk_index, total_chunks, chunk_metadata
        )
        code_metadata = self._create_code_specific_metadata(
            chunk_content, chunk_metadata, original_doc
        )
        base_metadata.update(code_metadata)
        if not skip_nlp:
            skip_nlp, skip_reason = self.should_skip_semantic_analysis(
                chunk_content, chunk_metadata
            )
            if skip_nlp:
                base_metadata["nlp_skip_reason"] = skip_reason
        return Document(
            id=chunk_id,
            content=chunk_content,
            metadata=base_metadata,
            source=original_doc.source,
            source_type=original_doc.source_type,
            url=original_doc.url,
            content_type=original_doc.content_type,
            title=self._generate_chunk_title(original_doc, chunk_metadata, chunk_index),
        )

    def should_skip_semantic_analysis(
        self, chunk_content: str, chunk_metadata: dict[str, Any]
    ) -> tuple[bool, str]:
        content_length = len(chunk_content)
        if content_length > self.skip_conditions["large_content"]:
            return True, "content_too_large"
        if any(
            pattern in chunk_content
            for pattern in self.skip_conditions["binary_patterns"]
        ):
            return True, "binary_content"
        if self._is_minified_code(chunk_content):
            return True, "minified_code"
        if self._is_generated_code(chunk_content):
            return True, "generated_code"
        if self._is_mostly_comments(chunk_content):
            return True, "mostly_comments"
        if chunk_metadata.get("element_type") == "test" and content_length < 500:
            return True, "simple_test_code"
        if chunk_metadata.get("language") in ["json", "yaml", "xml", "ini"]:
            return True, "configuration_file"
        return False, "suitable_for_nlp"

    def _create_code_specific_metadata(
        self, content: str, chunk_metadata: dict[str, Any], original_doc: Document
    ) -> dict[str, Any]:
        return {
            "content_analysis": analyze_code_content(content),
            "language_context": extract_language_context(content, chunk_metadata),
            "code_quality": assess_code_quality(content, chunk_metadata),
            "educational_value": assess_educational_value(content, chunk_metadata),
            "reusability_score": calculate_reusability_score(content, chunk_metadata),
            "chunking_strategy": "code_modular",
            "element_context": (
                self._extract_element_context(
                    content, chunk_metadata.get("element_type", "unknown")
                )
                if chunk_metadata.get("element_type", "unknown") != "unknown"
                else None
            ),
        }

    def _generate_chunk_title(
        self, original_doc: Document, chunk_metadata: dict[str, Any], chunk_index: int
    ) -> str:
        base_title = original_doc.title
        element_name = chunk_metadata.get("element_name")
        element_type = chunk_metadata.get("element_type", "code")
        if element_name:
            return f"{base_title} — {element_type}: {element_name} (Part {chunk_index + 1})"
        if element_type and element_type != "code":
            return f"{base_title} — {element_type.title()} Part {chunk_index + 1}"
        return f"{base_title} — Code Chunk {chunk_index + 1}"

    # Local helpers (unchanged logic)
    def _is_minified_code(self, content: str) -> bool:
        lines = content.split("\n")
        non_empty = [line for line in lines if line.strip()]
        if not non_empty:
            return False
        avg_len = sum(len(line) for line in non_empty) / len(non_empty)
        specials = sum(
            1 for line in non_empty if any(ch in line for ch in ["{", "}", ";"])
        )
        ratio = specials / len(non_empty)
        return avg_len > 200 and ratio > self.skip_conditions["minified_code_threshold"]

    def _is_generated_code(self, content: str) -> bool:
        lower = content.lower()
        return any(
            pat in lower for pat in self.skip_conditions["generated_code_patterns"]
        )

    def _is_mostly_comments(self, content: str) -> bool:
        lines = content.split("\n")
        if not lines:
            return False
        comment_lines = [
            line for line in lines if line.strip().startswith(("#", "//", "/*", "--"))
        ]
        return len(comment_lines) / len(lines) > 0.6

    def _has_meaningful_names(self, content: str) -> bool:
        bad_names = ["tmp", "foo", "bar", "baz", "var", "data", "x", "y", "z"]
        return not any(f" {n} " in content for n in bad_names)

    def _determine_learning_level(
        self, content: str, chunk_metadata: dict[str, Any]
    ) -> str:
        complexity = chunk_metadata.get("complexity", 0)
        if complexity < 2:
            return "beginner"
        if complexity < 6:
            return "intermediate"
        return "advanced"

    def _identify_programming_concepts(self, content: str) -> list[str]:
        concepts: list[str] = []
        lower = content.lower()
        for k in [
            "recursion",
            "memoization",
            "concurrency",
            "polymorphism",
            "inheritance",
        ]:
            if k in lower:
                concepts.append(k)
        return concepts

    def _extract_element_context(
        self, content: str, element_type: str
    ) -> dict[str, Any]:
        context = {"element_type": element_type}
        if element_type in ["function", "method"]:
            context["has_return_statement"] = "return" in content
            context["param_count_estimate"] = (
                content.split("(", 1)[-1].split(")")[0].count(",") + 1
                if "(" in content and ")" in content
                else 0
            )
        elif element_type == "class":
            context["has_init"] = "__init__" in content
            context["method_count_estimate"] = content.count("def ")
        return context
