Metadata-Version: 2.4
Name: volt-llm-client
Version: 0.1.2
Summary: A lightweight client for local LLM APIs like Ollama or OpenWebUI
Home-page: https://github.com/stuarttempleton/volt-llm-client
Author: Voltur
License: MIT License
        
        Copyright (c) 2025 Voltur
        
        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:
        
        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.
        
        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.
        
Project-URL: Homepage, https://github.com/stuarttempleton/volt-llm-client
Project-URL: Repository, https://github.com/stuarttempleton/volt-llm-client
Project-URL: Bug Tracker, https://github.com/stuarttempleton/volt-llm-client/issues
Keywords: logging,structured,volt
Classifier: Programming Language :: Python :: 3
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Requires-Python: >=3.7
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: requests
Requires-Dist: volt-logger>=0.1.0
Provides-Extra: test
Requires-Dist: pytest; extra == "test"
Requires-Dist: requests; extra == "test"
Requires-Dist: pytest-mock; extra == "test"
Dynamic: author
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-python

# volt-llm-client

**volt-llm-client** is a lightweight Python client for interacting with local LLM APIs such as [Ollama](https://ollama.com/) or [OpenWebUI](https://github.com/open-webui/open-webui).  
It supports prompt completion, multi-turn conversations, and model listing â€” perfect for scripts, prototypes, or CLI tools.

---

## Features

- Send single prompts or full conversations
- Compatible with Ollama/OpenWebUI-style APIs
- Token-based authentication support
- Clean log output using [volt-logger](https://github.com/stuarttempleton/volt-logger)
- Minimal dependencies (`requests` only)

---

## Installation

Install both `volt-llm-client` and its logging dependency:

```bash
pip install volt-llm-client
````

For local development:

```bash
git clone https://github.com/stuarttempleton/volt-llm-client.git
cd volt-llm-client
pip install -r requirements.txt
```

---

## Usage Example

```python
import os
from voltllmclient import LLMClient

llm = LLMClient(
    token=os.getenv("LLM_API_TOKEN"),
    model="Gemma3"
)

reply = llm.send_prompt("What is the capital of France?")
print(reply)
```

You can also send a full conversation:

```python
messages = [
    { "role": "system", "content": "You are a helpful assistant." },
    { "role": "user", "content": "Who won the World Cup in 2018?" }
]

response = llm.send_conversation(messages)
print(response)
```

---

## Maintaining Context with LLMConversation

```python
from voltllmclient import LLMConversation
import os

conv = LLMConversation(model="gemma3", token=os.getenv("LLM_API_TOKEN"))

response = conv.send("What is quantum computing?")
print(response)

response = conv.send_with_full_context("How is it different from classical computing?")
print(response)

conv.save_transcript("session.json")
```

---

## Environment Variables

Set your API token via environment variable:

**Unix/macOS:**

```bash
export LLM_API_TOKEN=your_token_here
```

**PowerShell:**

```powershell
$env:LLM_API_TOKEN = "your_token_here"
```

---

## License

[MIT](LICENSE)

