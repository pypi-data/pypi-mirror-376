{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10287e20",
   "metadata": {},
   "source": [
    "# **Example:** Reading PDF Documents with Images using MarkItDownReader\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "<img src=\"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/docs/assets/markitdown_reader_button.svg#only-light\" alt=\"MarkItDownReader logo\">\n",
    "<img src=\"https://raw.githubusercontent.com/andreshere00/Splitter_MR/refs/heads/main/docs/assets/markitdown_reader_button_white.svg#only-dark\" alt=\"MarkItDownReader logo\">\n",
    "</p>\n",
    "\n",
    "As we have seen in previous examples, reading a PDF is not a simple task. In this case, we will see how to read a PDF using the **MarkItDown** framework, and connect this library to Visual Language Models (VLMs) to extract text or get annotations from images.\n",
    "\n",
    "## How to connect a VLM to MarkItDownReader\n",
    "\n",
    "For this example, we will use the same document as the [previous tutorial](https://github.com/andreshere00/Splitter_MR/blob/main/data/sample_pdf.pdf).\n",
    "\n",
    "To extract image descriptions or perform OCR, instantiate any model that implements the [`BaseModel` interface](https://andreshere00.github.io/Splitter_MR/api_reference/model/#basemodel) (vision variants inherit from it) and pass it into the [`MarkItDownReader`](https://andreshere00.github.io/Splitter_MR/api_reference/reader/#markitdownreader). Swapping providers only changes the model constructor; your Reader usage remains the same.\n",
    "\n",
    "### Supported models (and when to use them)\n",
    "\n",
    "| Model (docs)                                                                                                       | When to use                                       | Required environment variables                                                                                        |\n",
    "| ------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------- |\n",
    "| [`OpenAIVisionModel`](https://andreshere00.github.io/Splitter_MR/api_reference/model/#openaivisionmodel)           | You have an OpenAI API key and want OpenAI cloud. | `OPENAI_API_KEY` (optional: `OPENAI_MODEL`, defaults to `gpt-4o`)                                                     |\n",
    "| [`AzureOpenAIVisionModel`](https://andreshere00.github.io/Splitter_MR/api_reference/model/#azureopenaivisionmodel) | You use Azure OpenAI Service.                     | `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_DEPLOYMENT`, `AZURE_OPENAI_API_VERSION`                |\n",
    "| [`GrokVisionModel`](https://andreshere00.github.io/Splitter_MR/api_reference/model/#grokvisionmodel)               | You have access to xAI Grok multimodal.           | `XAI_API_KEY` (optional: `XAI_MODEL`, default `grok-4`)                                                               |\n",
    "| [`GeminiVisionModel`](https://andreshere00.github.io/Splitter_MR/api_reference/model/#geminivisionmodel)           | You want Googleâ€™s Gemini vision models.           | `GEMINI_API_KEY` (also install extras: `pip install \"splitter-mr[multimodal]\"`)                                       |\n",
    "| [`AnthropicVisionModel`](https://andreshere00.github.io/Splitter_MR/api_reference/model/#anthropicvisionmodel)     | You have an Anthropic key (Claude Vision).        | `ANTHROPIC_API_KEY` (optional: `ANTHROPIC_MODEL`)                                                                     |\n",
    "| [`HuggingFaceVisionModel`](https://andreshere00.github.io/Splitter_MR/api_reference/model/#huggingfacevisionmodel) | You prefer local/open-source/offline inference.   | Install extras: `pip install \"splitter-mr[multimodal]\"` (optional: `HF_ACCESS_TOKEN` if the chosen model requires it) |\n",
    "\n",
    "> **Note on HuggingFace models:** Not all HF models are supported (e.g., gated or uncommon architectures). A well-tested option is **SmolDocling**.\n",
    "\n",
    "### Environment variables\n",
    "\n",
    "Create a `.env` file alongside your Python script:\n",
    "\n",
    "<details>\n",
    "  <summary><strong>Show/hide environment variables needed for every provider</strong></summary>\n",
    "\n",
    "  <h4>OpenAI</h4> \n",
    "\n",
    "```txt\n",
    "# OpenAI\n",
    "OPENAI_API_KEY=<your-api-key>\n",
    "# (optional) OPENAI_MODEL=gpt-4o\n",
    "```\n",
    "\n",
    "  <h4>Azure OpenAI</h4>\n",
    "\n",
    "```txt\n",
    "# Azure OpenAI\n",
    "AZURE_OPENAI_API_KEY=<your-api-key>\n",
    "AZURE_OPENAI_ENDPOINT=<your-endpoint>\n",
    "AZURE_OPENAI_API_VERSION=<your-api-version>\n",
    "AZURE_OPENAI_DEPLOYMENT=<your-model-name>\n",
    "```\n",
    "\n",
    "  <h4>xAI Grok</h4>\n",
    "\n",
    "```txt\n",
    "# xAI Grok\n",
    "XAI_API_KEY=<your-api-key>\n",
    "# (optional) XAI_MODEL=grok-4\n",
    "```\n",
    "\n",
    "  <h4>Google Gemini</h4>\n",
    "\n",
    "```txt\n",
    "# Google Gemini\n",
    "GEMINI_API_KEY=<your-api-key>\n",
    "# Also: pip install \"splitter-mr[multimodal]\"\n",
    "```\n",
    "\n",
    "  <h4>Anthropic (Claude Vision)</h4>\n",
    "\n",
    "```txt\n",
    "# Anthropic (Claude Vision)\n",
    "ANTHROPIC_API_KEY=<your-api-key>\n",
    "# (optional) ANTHROPIC_MODEL=claude-sonnet-4-20250514\n",
    "```\n",
    "\n",
    "  <h4>Hugging Face (local/open-source)</h4>\n",
    "\n",
    "```txt\n",
    "# Hugging Face (optional, only if needed by the model)\n",
    "HF_ACCESS_TOKEN=<your-hf-token>\n",
    "# Also: pip install \"splitter-mr[multimodal]\"\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "### Instantiation examples\n",
    "\n",
    "<details>\n",
    "  <summary><strong>Show/hide instantiation snippets for all providers</strong></summary>\n",
    "\n",
    "  <h4>OpenAI</h4>\n",
    "\n",
    "```python\n",
    "from splitter_mr.model import OpenAIVisionModel\n",
    "\n",
    "# Reads OPENAI_API_KEY (and optional OPENAI_MODEL) from .env if present\n",
    "model = OpenAIVisionModel()\n",
    "# or pass explicitly:\n",
    "# model = OpenAIVisionModel(api_key=\"...\", model_name=\"gpt-4o\")\n",
    "```\n",
    "\n",
    "  <h4>Azure OpenAI</h4>\n",
    "\n",
    "```python\n",
    "from splitter_mr.model import AzureOpenAIVisionModel\n",
    "\n",
    "# Reads Azure vars from .env if present\n",
    "model = AzureOpenAIVisionModel()\n",
    "# or:\n",
    "# model = AzureOpenAIVisionModel(\n",
    "#     api_key=\"...\",\n",
    "#     azure_endpoint=\"https://<resource>.openai.azure.com/\",\n",
    "#     api_version=\"2024-02-15-preview\",\n",
    "#     azure_deployment=\"<your-deployment-name>\",\n",
    "# )\n",
    "```\n",
    "\n",
    "  <h4>xAI Grok</h4>\n",
    "\n",
    "```python\n",
    "from splitter_mr.model import GrokVisionModel\n",
    "\n",
    "# Reads XAI_API_KEY (and optional XAI_MODEL) from .env\n",
    "model = GrokVisionModel()\n",
    "```\n",
    "\n",
    "  <h4>Google Gemini</h4>\n",
    "\n",
    "```python\n",
    "from splitter_mr.model import GeminiVisionModel\n",
    "\n",
    "# Requires GEMINI_API_KEY and the 'multimodal' extra installed\n",
    "model = GeminiVisionModel()\n",
    "```\n",
    "\n",
    "  <h4>Anthropic (Claude Vision)</h4>\n",
    "\n",
    "```python\n",
    "from splitter_mr.model import AnthropicVisionModel\n",
    "\n",
    "# Reads ANTHROPIC_API_KEY (and optional ANTHROPIC_MODEL) from .env\n",
    "model = AnthropicVisionModel()\n",
    "```\n",
    "\n",
    "  <h4>Hugging Face (local/open-source)</h4>\n",
    "\n",
    "```python\n",
    "from splitter_mr.model import HuggingFaceVisionModel\n",
    "\n",
    "# Token only if the model requires gating\n",
    "model = HuggingFaceVisionModel()\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9b76664",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from splitter_mr.model import AzureOpenAIVisionModel\n",
    "from splitter_mr.reader import MarkItDownReader\n",
    "\n",
    "file = \"data/sample_pdf.pdf\"\n",
    "model = AzureOpenAIVisionModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c73bf62",
   "metadata": {},
   "source": [
    "\n",
    "Then, you can simply pass the model that you have instantiated to the Reader class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9526d18",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "reader = MarkItDownReader(model=model)\n",
    "output = reader.read(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a79c0b",
   "metadata": {},
   "source": [
    "\n",
    "This returns a `ReaderOutput` object with all document text and extracted image descriptions via the vision model. You can access metadata like `output.conversion_method`, `output.reader_method`, `output.ocr_method`, etc.\n",
    "\n",
    "To retrieve the text content, you can simply access to the `text` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "936d2bf0",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!-- page -->\n",
      "\n",
      "# Description:\n",
      "# A sample PDF\n",
      "\n",
      "Converting PDF files to other formats, such as Markdown, is a surprisingly complex task due to the nature of the PDF format itself. PDF (Portable Document Format) was designed primarily for preserving the visual layout of documents, making them look the same across different devices and platforms. However, this design goal introduces several challenges when trying to extract and convert the underlying content into a more flexible, structured format like Markdown.\n",
      "\n",
      "## 1. Lack of Structural Information\n",
      "\n",
      "Unlike formats such as HTML or DOCX, PDFs generally do not store information about the logical structure of the documentâ€”such as headings, paragraphs, lists, or tables. Instead, PDFs are often a collection of text blocks, images, and graphical elements placed at specific coordinates on a page. This makes it difficult to accurately infer the intended structure, such as determining what text is a heading versus a regular paragraph.\n",
      "\n",
      "## 2. Variability in PDF Content\n",
      "\n",
      "PDF files can contain a wide range of content types: plain text, styled text, images, tables, embedded fonts, and even vector graphics. Some PDFs are generated programmatically and have relatively clean underlying text, while others may be created from scans, resulting in image-based (non-selectable) content that requires OCR (Optical Character Recognition) for extraction. The variability in how PDFs are produced leads to inconsistent results when converting to Markdown.\n",
      "\n",
      "### An enumerate:\n",
      "1. One\n",
      "\n",
      "---\n",
      "\n",
      "![SplitterMR logo](https://example.com/splittermr-logo.png)\n",
      "\n",
      "<!-- page -->\n",
      "\n",
      "# Description:\n",
      "1. One\n",
      "2. Two\n",
      "3. Three\n",
      "\n",
      "### 3. Preservation of Formatting\n",
      "\n",
      "Markdown is a lightweight markup language that supports basic formattingâ€”such as headings, bold, italics, links, images, and lists. However, it does not support all the visual and layout options available in PDF, such as columns, custom fonts, footnotes, floating images, and complex tables. Deciding how (or whether) to preserve these elements can be difficult, and often requires trade-offs between fidelity and simplicity.\n",
      "\n",
      "\\[ f(x) = x^2, \\quad x \\in [0, 1] \\]\n",
      "\n",
      "#### An example list:\n",
      "- Element 1\n",
      "- Element 2\n",
      "- Element 3\n",
      "\n",
      "### 4. Table and Image Extraction\n",
      "\n",
      "Tables and images in PDFs present a particular challenge. Tables are often visually represented using lines and spacing, with no underlying indication that a group of text blocks is actually a table. Extracting these and converting them to Markdown tables (which have a much simpler syntax) is error-prone. Similarly, extracting images from a PDF and re-inserting them in a way that makes sense in Markdown requires careful handling.\n",
      "\n",
      "---\n",
      "\n",
      "This is a cite.\n",
      "\n",
      "---\n",
      "\n",
      "### 5. Multicolumn Layouts and Flowing Text\n",
      "\n",
      "Many PDFs use complex layouts with multiple columns, headers, footers, or sidebars. Converting these layouts to a single-flowing Markdown document requires decisions about reading order and content hierarchy. Itâ€™s easy to end up with text in the wrong order or to lose important contextual information.\n",
      "\n",
      "### 6. Encoding and Character Set Issues\n",
      "\n",
      "PDFs can use a variety of text encodings, embedded fonts, and even contain non-standard Unicode characters. Extracting text reliably without corruption or data loss is not always straightforward, especially for documents with special symbols or non-Latin scripts.\n",
      "\n",
      "<!-- page -->\n",
      "\n",
      "# Description:\n",
      "```markdown\n",
      "| Name         | Role        | Email               |\n",
      "|--------------|-------------|---------------------|\n",
      "| Alice Smith  | Developer   | alice@example.com    |\n",
      "| Bob Johnson   | Designer    | bob@example.com      |\n",
      "| Carol White  | Project Lead | carol@example.com    |\n",
      "\n",
      "## Conclusion\n",
      "\n",
      "While it may seem simple on the surface, converting PDFs to formats like Markdown involves a series of technical and interpretive challenges. Effective conversion tools must blend text extraction, document analysis, and sometimes machine learning techniques (such as OCR or structure recognition) to produce usable, readable, and faithful Markdown output. As a result, perfect conversion is rarely possible, and manual review and cleanup are often required.\n",
      "\n",
      "![Hummingbird](https://example.com/hummingbird-image)\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad8f4a",
   "metadata": {},
   "source": [
    "\n",
    "With the by-default method, you obtain the text extracted from the PDF as it is shown. This method scan the PDF pages as images and process them using a VLM. The result will be a markdown text with all the images detected in every page. Every page is highlighted with a markdown comment as a placeholder: `<!-- page -->`. \n",
    "\n",
    "## Experimenting with some keyword arguments\n",
    "\n",
    "In case that needed, you can pass use other keyword arguments to process the PDFs.\n",
    "\n",
    "For example, you can customize how to process the images by the VLM using the parameter prompt. For example, in case that you only need an excerpt or a brief description for every page, you can use the following prompt:\n",
    "\n",
    "```python\n",
    "output = reader.read(\n",
    "    file, \n",
    "    scan_pdf_pages = True, \n",
    "    prompt = \"Return only a short description for these pages\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531b79bf",
   "metadata": {},
   "source": [
    "\n",
    "In case that needed, it could be interesting split the PDF pages using another placeholder. You can configure that using the `page_placeholder` parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c103e39",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Â PAGE\n",
      "\n",
      "# Description:\n",
      "The document discusses the challenges of converting PDF files to more flexible formats like Markdown due to the inherent characteristics of PDFs. It highlights two main issues: the lack of structural information, which complicates the extraction of organized content, and the variability in PDF content types, leading to inconsistent results during conversion.\n",
      "\n",
      "##Â PAGE\n",
      "\n",
      "# Description:\n",
      "1. **Preservation of Formatting**: Discusses the limitations of Markdown in replicating complex formatting options found in PDFs, including challenges in preserving elements like custom fonts and complex tables.\n",
      "\n",
      "2. **Table and Image Extraction**: Explains the difficulties in accurately extracting tables and images from PDFs, noting the error-prone nature of converting visual representations into structured formats.\n",
      "\n",
      "3. **Multicolumn Layouts and Flowing Text**: Highlights the complications of converting PDFs with complex layouts into Markdown, emphasizing the potential for losing contextual information and proper reading order.\n",
      "\n",
      "4. **Encoding and Character Set Issues**: Covers the challenges related to various text encodings and special characters in PDFs, focusing on the risks of data loss and corruption during extraction.\n",
      "\n",
      "##Â PAGE\n",
      "\n",
      "# Description:\n",
      "The page summarizes the challenges of converting PDFs to Markdown formats, emphasizing the need for advanced tools that integrate text extraction, document analysis, and machine learning. It concludes that perfect conversion is difficult and often requires manual review and adjustments. Additionally, a table of team members with their roles and contact emails is included.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = reader.read(\n",
    "    file,\n",
    "    scan_pdf_pages=True,\n",
    "    prompt=\"Return only a short description for these pages\",\n",
    "    page_placeholder=\"##Â PAGE\",\n",
    ")\n",
    "print(output.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb3fc6d",
   "metadata": {},
   "source": [
    "\n",
    "In comparison, `MarkItDownReader` offers a faster conversion than Docling but with less options to be configured. In that sense, we cannot obtain directly the `base64` images from every image detected in our documents, or write image placeholders easily (despite we can do it using a prompt). In addition, you will always get a `# Description` placeholder every time you use a VLM for extraction and captioning in this Reader. \n",
    "\n",
    "As conclusion, using this reader with a VLM can be useful for those use cases when we need to efficiently extract the text from a PDF. In case that you need the highest reliability or customization, it is not the most suitable option.\n",
    "\n",
    "## Complete script\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "from splitter_mr.model import AzureOpenAIVisionModel\n",
    "from splitter_mr.reader import MarkItDownReader\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "file = \"data/sample_pdf.pdf\"\n",
    "model = AzureOpenAIVisionModel()\n",
    "# Ensure the output directory exists\n",
    "output_dir = os.path.join(os.path.dirname(__file__), \"markitdown_output\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def save_markdown(output, filename_base):\n",
    "    \"\"\"\n",
    "    Saves the ReaderOutput.text attribute to a markdown file in the markitdown_output directory.\n",
    "\n",
    "    Args:\n",
    "        output (ReaderOutput): The result object returned from DoclingReader.read().\n",
    "        filename_base (str): A descriptive base name for the file (e.g., 'vlm', 'scan_pages').\n",
    "    \"\"\"\n",
    "    filename = f\"{filename_base}.md\"\n",
    "    file_path = os.path.join(output_dir, filename)\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(output.text)\n",
    "    print(f\"Saved: {file_path}\")\n",
    "\n",
    "markitdown_reader = MarkItDownReader(model = model)\n",
    "markitdown_output = markitdown_reader.read(file)\n",
    "save_markdown(markitdown_output, \"vlm\")\n",
    "\n",
    "markitdown_output = markitdown_reader.read(file, scan_pdf_pages = True, prompt = \"Return only a short description for these pages\", page_placeholder = \"##Â PAGE\")\n",
    "save_markdown(markitdown_output, \"custom_vlm\")\n",
    "\n",
    "markitdown_reader = MarkItDownReader()\n",
    "markitdown_output = markitdown_reader.read(file)\n",
    "save_markdown(markitdown_output, \"no_vlm\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983093e7",
   "metadata": {},
   "source": [
    "\n",
    "!!! note\n",
    "    For more on available options, see the [**MarkItDownReader class documentation**](../../api_reference/reader.md#markitdownreader)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "splitter-mr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_format": "markdown",
  "source_file": "docs/examples/pdf/pdf_markitdown.md"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
