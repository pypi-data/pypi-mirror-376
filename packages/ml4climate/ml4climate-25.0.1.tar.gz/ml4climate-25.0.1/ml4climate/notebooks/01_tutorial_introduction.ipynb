{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb8c31b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tutorial 1 : Python basics "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6027c215",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Learning Outcomes</b>\n",
    "   \n",
    "- Data manipulation in python: numpy, matplotlib, pandas\n",
    "- First steps with scikit-learn\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b22f615",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Python is a popular computer language. We will use it along with 4 libraries: numpy for array handling, pandas for time series, scikit-learn for machine learning tools and matplotlib for plotting. There are *a lot* of resources on the web: the user manual remains the primary location where you should be looking for documentation about a specific function. https://stackoverflow.com/ is also a great source of information since many people before you have already had the same question as you. A good intro to numpy: https://sebastianraschka.com/blog/2020/numpy-intro.html\n",
    "\n",
    "Even though python is a great language, don't forget that it is an [Interpreted language](https://en.wikipedia.org/wiki/Interpreter_(computing)). That means that python can be very slow. For this reason you should \n",
    "\n",
    "- Rely on existing libraries when available\n",
    "- Write compact code\n",
    "- Avoid loops\n",
    "\n",
    "In fact, most modern libraries deffer the heavy lifting part to compiled bits of codes which typically run much faster than your own implementation.\n",
    "\n",
    "Last, don't forget to comment your codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d318e1af",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Numpy for array handling\n",
    "\n",
    "Numpy is the core library when it comes to manipulating \"dense\" arrays of numbers and you should always use it instead of raw python lists. Numpy also comes with a couple of linear algebra routines to perform basic operations (matrix multiplication, linear systems, eigenvalue solver). However for large-scale application (and sparse matrices) we will use different libraries (like scipy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79be0bbd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d3360",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Vectors are first declared as a list of number and are then converted to numpy arrays. Note that python does not make the difference between row vectors and column vectors: all vectors are treated as rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40d7a0d3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cf12ce",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Matrices are declared as an array of arrays in a similar way as vectors. Note that python is case sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "358f4113",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da94a9fa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In order to convert a row vector to a column vector, one must add a new dimension as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf2a3085",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "x_col = x[:,None]\n",
    "print(x_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c27bc4",
   "metadata": {},
   "source": [
    "You can compute the transpose of a vector with the method `.T`. However keep in mind that the transpose of a 1d vector does not really make sense *in python*. If you want to transpose a vector in python, it must have at least two dimensions.\n",
    "\n",
    "> ***Question***\n",
    "> - Try to transpose `x` and `x_col` and check their dimensions.\n",
    "- Hint: You can always check the dimensions of your array with the method `.shape`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a837b232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddbdcca",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Matrix multiplication can be achieved with `np.dot`, `np.matmul`, `@` or `np.einsum`.\n",
    "> ***Question***\n",
    "> - Pick the one you prefer to compute $\\mathbf A \\mathbf x$, $\\mathbf x \\mathbf x^\\top$, and $\\mathbf x^\\top \\mathbf x$.\n",
    "\n",
    "- Hint: Did I already warn you about transposing 1d vectors in python?\n",
    "- Hint2: Einsum is the most advanced function but also the most complicated. [This tutorial](https://ajcr.net/Basic-guide-to-einsum/) will tell you more about it\n",
    "- Hint3: you can add a question mark to the name of the function to access the help: `np.einsum?`\n",
    "\n",
    "With the  vectors and matrices defined above, you should get:\n",
    "$\\mathbf A \\mathbf x = [14, 32, 50]^\\top$\n",
    "\n",
    "\\begin{align}\n",
    " \\mathbf x \\mathbf x^\\top = &= \\begin{bmatrix}\n",
    "    1 & 2 & 3\\\\\n",
    "    2 & 4 & 6\\\\\n",
    "    3 & 6 &  9 \\\\\n",
    "  \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "and \n",
    "$\\mathbf x^\\top \\mathbf x = 14$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08bd26b7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac280443",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can access the first element of an array with `x[0]` and the last element with `x[-1]`. You can also select a subset using slices like `x[0:2]` (or `x[:2]`). Be careful that the last index of the slice is not part of the subset.\n",
    "\n",
    "> ***Question***\n",
    "> - Can you select the last two element of the vector $\\mathbf x$ with negative indices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c840f76",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bc7f9d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Numpy comes with the standard math function `np.exp`, `np.sqrt`, `np.sin`. Each of these functions applied to an array is applied element-wise. For instance `A**2` raises each element of `A` to the power of 2.\n",
    "\n",
    "> ***Question***\n",
    "> - Is `A*A` the same as `A@A`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c244bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e918eef6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Broadcasting\n",
    "\n",
    "In python, we can add, subtract, multiply, (etc.) arrays elementwise. In this context arrays have to be of the same size. But when we do `A+1`, we add the scalar `1` to each element of the array `A`. This type of operation is called [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html). \n",
    "\n",
    "> ***Question***\n",
    "> - Check the broadcasting rules (link above) and add `x` to each column of `A`\n",
    "\n",
    "You should get\n",
    "\\begin{bmatrix}\n",
    "    2 & 3 & 4\\\\\n",
    "    6 & 7 & 8\\\\\n",
    "    10 & 11 &  12 \\\\\n",
    "  \\end{bmatrix}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0481349",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31042ea6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generating arrays\n",
    "\n",
    "Empty arrays can be generated with `np.zeros`. We can also generate arrays with [random numbers](https://numpy.org/doc/stable/reference/random/index.html). The most useful functions are\n",
    "- [`np.random.rand`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.rand.html) (uniform distribution)\n",
    "- [`np.random.randn`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.randn.html) (standard normal distribution) \n",
    "- [`np.random.normal`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html) (normal distribution)\n",
    "\n",
    "> ***Question***\n",
    "> - Generate 2 arrays $v_1$ and $v_2$ with $N=500$ observations following the [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) $\\mathcal N (\\mu_1,\\sigma_1^2)$ for $v_1$ and $\\mathcal N (\\mu_2,\\sigma_2^2)$ for $v_2$, with $\\mu_1 = 2$, $\\mu_2 = 1$, $\\sigma_1 = 5$, $\\sigma_2 = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1763a979",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mu1 = 2\n",
    "mu2 = 1\n",
    "\n",
    "sigma1 = 5\n",
    "sigma2 = 1\n",
    "\n",
    "N = 500\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37deb45",
   "metadata": {},
   "source": [
    "> ***Question***\n",
    "> - Compute the mean of each array.\n",
    "> - How does it compare to $\\mu_1$ and $\\mu_2$? Can you explain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19c97944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292b4e6",
   "metadata": {},
   "source": [
    "> ***Question***\n",
    "> - Do you think $v_1$ and $v_2$ are correlated?\n",
    "> - Can you compute the actual covariance between $v_1$ and $v_2$ with `np.cov`?\n",
    "> - Recall the definition of the covariance between two variables. [Hint](https://en.wikipedia.org/wiki/Covariance)\n",
    "> - What is the expected covariance matrix in terms of $\\sigma_1$ and $\\sigma_2$\n",
    "> - Do you recognize the array `Cv` defined below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08dcac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "Cv = np.array([[sigma1**2, 0], [0,sigma2**2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4523493c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "As you just saw, $v_1$ and $v_2$ are not correlated. Let's apply the linear transformation\n",
    "\\begin{align}\n",
    "w_1& = \\cos \\theta v_1 - \\sin \\theta v_2\\\\\n",
    "w_2& = \\sin \\theta v_1 + \\cos \\theta v_2\n",
    "\\end{align}\n",
    "That you can also write for each realization\n",
    "\\begin{equation}\n",
    "\\mathbf{w} = \\mathbf{R}\\mathbf{v}\n",
    "\\end{equation}\n",
    "with $\\mathbf R$ the rotation matrix\n",
    "\\begin{align}\n",
    " \\mathbf{R} &= \\begin{bmatrix}\n",
    "    \\cos \\theta & -\\sin \\theta \\\\\n",
    "    \\sin \\theta & \\cos \\theta\n",
    "  \\end{bmatrix}\n",
    "\\end{align}\n",
    "and\n",
    "\\begin{align}\n",
    " \\mathbf{v} &= \\begin{bmatrix}\n",
    "    v_1  \\\\\n",
    "    v_2\n",
    "  \\end{bmatrix} & \\quad & \\mathbf{w} &= \\begin{bmatrix}\n",
    "    w_1  \\\\\n",
    "    w_2\n",
    "  \\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "> ***Question***\n",
    "> - Create the rotation matrix `R` with $\\theta=\\pi/3$\n",
    "> - In order to compute $\\mathbf w$ it is convenient to store $\\mathbf v = [v_1, v_2]$ in a matrix\n",
    "`V = np.array([v1,v2])`. Compute `V`.\n",
    "> - Compute `W` with one matrix multiplication. \n",
    "\n",
    "\n",
    "Remarks:\n",
    "- The equations above are valid for one single observation.\n",
    "- layout for `V` and `W` is the opposite as the one we adopted in class. We will adopt this new layout just for this tutorial. Take a moment to check if each observation is stored in a row or in a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ca5ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b454a717",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> ***Question***\n",
    "> - Compute the mean of $\\mathbf w = (w_1,w_2)$ with two different techniques: \n",
    ">     - direct computation with `np.mean` (be careful, you need to compute the mean along the second dimension only! Look at the documentation.)\n",
    ">     - with $R$ and the mean of $\\mathbf v$.\n",
    "\n",
    "For the second method, you will have to derive the formula from the definition of $\\mathbf w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6689ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944c3677",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> ***Question***\n",
    "> - Can you guess what the covariance between $w_1$ and $w_2$ will be? [Hint](https://en.wikipedia.org/wiki/Covariance#Covariance_of_linear_combinations)\n",
    "> - Compute the theoretical covariance matrix $C_w$ as a function of $R$ and $C_v$ (the covariance matrix of $\\mathbf v$) [Hint](https://en.wikipedia.org/wiki/Covariance#Auto-covariance_matrix_of_real_random_vectors)\n",
    "> - Compute the covariance matrix of $\\mathbf w$ with 2 methods: \n",
    ">      - with `np.cov`, \n",
    ">      - with `R` and `Cv`\n",
    "> - Comment your results\n",
    "\n",
    "You should get\n",
    "\\begin{align}\n",
    "C_w &=\\begin{bmatrix}\n",
    "    7 & 10.4\\\\\n",
    "    10.4 & 19\n",
    "\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46ff671d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48af9da1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Matplotlib for plotting\n",
    "\n",
    "To visualize data, matplotlib is the reference tool. Matplotlib is also what runs behind the scene of high level python libraries (pandas, xarray) and so knowing the basics will also be useful there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fdf13947",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2359a1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> ***Question***\n",
    "> - Can you visualize the data set `V` that we generated earlier. you can either pass the two variables to the function `plt.plot` in order to plot $v_2$ as a function of $v_1$ or we can use the function `plt.scatter`. Don't forget to \n",
    ">    - adjust the type and size of markers [Hint](https://matplotlib.org/stable/api/markers_api.html)\n",
    ">    - adjust the axis so that both axis are equally stretched [Hint](https://matplotlib.org/stable/gallery/subplots_axes_and_figures/axis_equal_demo.html).\n",
    ">    - add labels [Hint](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xlabel.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22460c68",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e2cc30",
   "metadata": {},
   "source": [
    "> ***Question***\n",
    "> - Do you *see* the lack of correlation between $v_1$ and $v_2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c041e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's analyze the probability density function (pdf) of this data set. The function [`plt.hist`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html) provides a discrete estimate of the pdf along a given dimension.\n",
    "\n",
    "> ***Question***\n",
    "> - Use `plt.hist` to compute and plot an estimation of the pdf of $w_1$ and $w_2$. Don't forget to adjust the number of bins.\n",
    "> - ***(Optional)*** Compare it with the analytical probability density function of $w_1$ and $w_2$? ([Hint](https://en.wikipedia.org/wiki/Sum_of_normally_distributed_random_variables))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8d3a6e7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d882462",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can also get a quick overview of the statistical distribution of the data with [box plot](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.boxplot.html).\n",
    "\n",
    "> ***Question***\n",
    "> - What is the meaning of a box plot?\n",
    "> - Try it on $w_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82ac254a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a45625",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> ***Question (optional)***\n",
    "> - Use `np.histogram2d` to compute the 2-dimensional pdf\n",
    "> - (***) On a single plot, superimpose the cloud of points and the contour lines of the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c4087b0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4a7610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "# The (x,y) indexing is tricky: use 2 different number of bins in each direction\n",
    "#psi, xedge, yedge = np.histogram2d(W[0,:],W[1,:],bins=[5,10])\n",
    "\n",
    "#x1 = 0.5*(xedge[1:] + xedge[:-1])\n",
    "#y1 = 0.5*(yedge[1:] + yedge[:-1])\n",
    "\n",
    "#xg,yg = np.meshgrid(x1,y1,indexing='ij')\n",
    "\n",
    "#plt.plot(W[0,:],W[1,:],'.', alpha=0.1)\n",
    "#plt.contour(xg,yg,psi)\n",
    "\n",
    "#plt.axis('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fc0dd6",
   "metadata": {},
   "source": [
    "> ***Question (Optional)***: \n",
    "> - The [Central limit theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) states that If $X_1, X_2,\\dots ,X_{N}$ are $N$ random samples drawn from a population with overall mean  $\\mu$ and finite variance $\\sigma ^{2}$ $\\bar {X}_{N}$ is the sample mean, then the limiting form of the distribution, $Z=\\lim _{N\\to \\infty }{\\sqrt {N}}{\\left({\\frac {{\\bar {X}}_{N}-\\mu }{\\sigma }}\\right)}$ is a standard normal distribution. Can you propose an visual illustration of this theorem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3794ca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aad77d-1e61-4256-9963-a2f25e1e679a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pandas for advanced array manipulation\n",
    "\n",
    "Compared to numpy which already handles arrays of numbers, Pandas essentially handles **metadata**. This will allow you to use predefined functions in pandas especially when it comes to time series manipulations. \n",
    "\n",
    "Pandas has a lot of other nice features like\n",
    "\n",
    "- Intuitive handling of missing data (NaN)\n",
    "- Input/output in many formats\n",
    "- Simplified plotting procedures (labels, axes, etc. are added automatically)\n",
    "- Better management of the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cf2acd-8cda-4d84-986b-0c47b585686d",
   "metadata": {},
   "source": [
    "### Data download\n",
    "\n",
    "If you are running a standalone version of the notebook, you will have to download the data first by running:\n",
    "\n",
    "`!mkdir data`\n",
    "\n",
    "`!wget -P data/ https://scrippsco2.ucsd.edu/assets/data/atmospheric/stations/in_situ_co2/daily/daily_in_situ_co2_mlo.csv`\n",
    "\n",
    "and in case wget is not installed, you can install it with\n",
    "\n",
    "!pip install wget\n",
    "\n",
    "!python -m wget -o 'data/daily_in_situ_co2_mlo.csv' https://scrippsco2.ucsd.edu/assets/data/atmospheric/stations/in_situ_co2/daily/daily_in_situ_co2_mlo.csv\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b1db1c-44b5-4f0f-9dab-a3f2fdcf5ac8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We are going to explore the basic functionalities of pandas with the historical CO2 measurements taken at Mona Loa also known as the [Keeling curve](https://en.wikipedia.org/wiki/Keeling_Curve). The data can be downloaded at [scrippsco2.ucsd.edu](http://scrippsco2.ucsd.edu) and we are going to analyze the daily output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a51c4a8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Take a moment to inspect the file that is located here `data/daily_in_situ_co2_mlo.csv` (either use a text editor or navigate within jupyter to open the file). Read the header to understand what is the data set about (you may have to investigate more to know about the units). \n",
    "\n",
    "> **Question**\n",
    "> - Why is there NaN in the data set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bc9851a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084fbf75",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We are going to use the function [`pd.read_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) to import the data in python. It is very important to do a clean import of the data so that we can work efficiently afterwards. Pay extra attention to \n",
    "- the commented part in the header (hint: `comment`)\n",
    "- add a name to each column. Use the names **year**, **month**, **day** for the corresponding columns. This way, pandas will directly recognize what they are (hint: `names`)\n",
    "- pandas handles spaces in cvs files in a very specific way: you may need to adjust its behavior with `skipinitialspace`.\n",
    "\n",
    "Use `pd.read_csv` to load the content of `data/daily_in_situ_co2_mlo.csv` into a DataFrame called `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d3c3775",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/daily_in_situ_co2_mlo.csv', comment='%',names=[\"Year\", \"Month\", \"Day\", \"CO2\", \"NB\", \"scale\"], skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cce54df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To access your data, you will need to know the metadata, i.e. the label of each column in the csv file (in this case provided by yourself). You can get a quick overview of your data by typing `df.head()` or `df.describe()`. You will see that the Dataframe `df` can be treated like a spreadsheet with labeled columns and entries for each column. You can view the labels with `df.columns` and the rows indices with `df.index`. Make sure you understand the type of each column of the dataset with `df.dtypes`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e0853b0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#look at the data set here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79666f34",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You can select a column of the DataFrame by passing one name to `df` like `df[\"Year\"]`. You can also select multiple columns by passing a list of names `df[[\"Year\", \"Month\"]]`. To select specific rows, you need to use the metho `df.loc[i0:i1]` with `i0` and `i1`, the lower and upper bounds of the index. In our case we did not provide labels for the index so pandas automatically numbered the rows with a simple counter `0,1,...`. In that case, `i0` and `i1` are integers but we will see later that `i0` and `i1` can also be two dates (if the index is a date).\n",
    "\n",
    "> ***Question***\n",
    "> - Create a new DataFrame that contains only the first 100 entries and with the columns `Year` and `CO2` only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e1431d4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d10236",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One killer feature of pandas is the way it handles time series. In order to use the full potential of such tool, one need to change the index to a date that is recognized by pandas. We use [`pd.to_datetime`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html) to combine the information in the year, month and day columns of the dataframe. `pd.to_datetime` handles several formats as input and you should be careful to use the correct input format. However, because we carefully chose the name of the columns when we imported the file, we can directly assemble the columns `Year`, `Month`, `Day` to create a new datetime object. You can follow the method in the [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#assembling-datetime-from-multiple-dataframe-columns) to re-allocate `df.index` to a `datetime` type variable. Henceforth, we will use `df.index` to access the time dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e5274ec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7346a8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> ***Question (Optional)***\n",
    "> - Note that one could have done this operation directly while reading the data. Use `parse_dates` in `pd.read_csv` to create a datetime object in `df`. Try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b87b2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b45e4d4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now `df.index` contain [Timestamp](https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.html)s. You can quickly access the day, month, year of the records by using `df.index.day`, `df.index.month` `df.index.year` (etc. see documentation). You can also construct time intervals (or [timedetla](https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.html)s) by subtracting two timestamps\n",
    "\n",
    "> ***Question***\n",
    "> - Subtract the first index to the last index to know how many days are in this data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c1a39be",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31de2fc",
   "metadata": {},
   "source": [
    "You can now select specific rows of your dataset by passing dates to the `.loc[]` method, as in `.loc[date0:date1]`. The two dates `date0` and `date1` can be written as strings in the format `'YYYY-MM-DD'` (e.g. `'2020-09-20'`) or simply `'YYYY-MM'`. \n",
    "\n",
    "If `date0` is only `'2020-09'` (you are missing the days), Pandas will interpret it as the first day of September 2020. Note that Pandas can also interpret `'2020-09'` as the whole month of September 2020 if this is the only argument as in `.loc[date0]`.\n",
    "\n",
    "> ***Question***\n",
    "> - Select the whole month of September 2020 with two different methods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4cbd77ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e030413",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pandas has built-in plotting features that are based on matplotlib. Since you now master matplotlib, you will find it very easy to plot data within pandas. \n",
    "\n",
    "> ***Question***\n",
    "> - Plot the CO2 time series as a function of time. (Hint: select the column of interest in df and simply apply the method `.plot()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60e54eb1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29beed2e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The CO2 curve is increasing. This is not really a surprised isn't it?. However, we also notice wiglly patterns superimposed to this trend. Are we looking at noisy data are does it have a physical meaning?\n",
    "\n",
    "> ***Question***\n",
    "> - In order to get a closer look, can you plot only 2 years of the CO2 time series between Jan 1st 2000 and Jan 1st 2002?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01624960",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feb6af3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So there seem to be a lot of variability and many missing data in this data set. We can try to average all available data for each single year that we have. That way we will probably see a smoother curve. The [`resample`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html) function is here to create a new time series with the desired sampling frequency. The acronym for standard time intervals are listed in the [Time series documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases). You need to either add the method `.mean()` or `.sum()` to the resampled data to indicate how to treat the data in the resampling interval.\n",
    "\n",
    "> ***Question***\n",
    "> - Plot the yearly average evolution of the CO2 concentration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39ecc55d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f38d5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "These wiggles are not random but are the actual annual cycle of the data.\n",
    "> ***Question (Optional)***\n",
    "> - Use the method `.groupby` to plot the annual cycle\n",
    "> - Discuss the physical origin of this anual cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "18a7e651",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d54f3c4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scikit-learn for data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822fc378",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[Scikit-learn](https://scikit-learn.org/) will be our toolbox for all statistical analysis. It provides the most popular machine learning algorithm, is well documented and has an active user community. Scikit learn does not work well with missing data. So in order to have a smooth transition between pandas and scikit-learn, it is convenient to create a copy of your DataFrame without any `NaN`. Make sure you use (and understand) the [`.copy()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.copy.html) method to create the new array because we are going to make modification to this new DataFrame. You can call this new DataFrame `dfv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0f8ad71c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "dfv = df.dropna().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f035614",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Because we cannot do a linear regression with dates as indices, we need to create an additional column that contains the day number since the begining of the dataset. Remember how we worked with timedelta (and broadcasting) above?\n",
    "\n",
    "> ***Question***\n",
    "> - Can you create this new column and name it `counter`?\n",
    "\n",
    "*Be careful that this new variable must have dtype int (or float) and not a timedelta. (Hint: look at the [attributes of timedelta](https://pandas.pydata.org/pandas-docs/stable/user_guide/timedeltas.html#attributes))*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0f09b985",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#dfv[\"counter\"] = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8460c4a1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We are now ready to use the [linear regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56d470f0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0646dc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We are first going to [fit](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit) the data. As explained in the documentation, this function takes at least two arguments: the predictors and the observations. Predictors must be a 2d array of shape (n_samples, n_features), **even if there is only ONE feature**. One simple way to achieve this is to select a list of only one variable in the DataFrame (i.e. `dfv[[\"counter\"]]`, note the usage of double brackets here).\n",
    "\n",
    "> ***Question***\n",
    "> - Create the input variable `X` (day since first record) and the output variable `y` (CO2 concentration)\n",
    "> - Perform the linear regression and put the output in the variable `reg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d829a84f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#X = \n",
    "#y  \n",
    "#reg = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c218be8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> ***Question***\n",
    "> - What is the value of the regression coefficient?\n",
    "> - What is its physical meaning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e004f059",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a06f874",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> ***Question***\n",
    "> - Use the method `.predict` to construct the predicted value at all observation times and add it to the `dfv` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6dd18269",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#dfv[\"CO2_fit\"] ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc11ec23",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> ***Question***\n",
    "> - Plot the observed value along with your prediction on a single plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8411e407",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e77b2e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It is now super easy to compute the co2 growth rate in ppm/year for several time periods.\n",
    "> ***Question***\n",
    "> - Perform 2 linear regressions: one between 1960 and 1980 and one between 2000 and 2020 and compare how the growth rate evolved over time.\n",
    "> - Can you give an estimate of the CO2 concentration in 2050?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3360b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reg_2000 = LinearRegression().fit(dfv[[\"counter\"]][dfv.index.year>2000], dfv[\"CO2\"][dfv.index.year>2000])\n",
    "#reg_1960 = LinearRegression().fit(dfv[[\"counter\"]][dfv.index.year<1980], dfv[\"CO2\"][dfv.index.year<1980])\n",
    "\n",
    "#print(\"The growth rate changed from {}ppm/day to {}ppm/day from 1970 to 2010\".format(reg_1960.coef_,reg_2000.coef_))\n",
    "\n",
    "## rough estimate of the number of days between 1960 and 2050 (90 years x 365 days)\n",
    "#nday_2050 = 90*365\n",
    "#predict_co2_2050 = reg_2000.intercept_ + nday_2050*reg_2000.coef_\n",
    "#print(f\"With the most recent growth rate, there will be {predict_co2_2050} ppm of CO2 in 2050\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e186998",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "## Credit\n",
    "\n",
    "[//]: # \"This notebook is part of [E4C Interdisciplinary Center - Education](https://gitlab.in2p3.fr/energy4climate/public/education).\"\n",
    "Contributors include Bruno Deremble and Alexis Tantet.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"display: flex; height: 70px\">\n",
    "    \n",
    "<img alt=\"Logo LMD\" src=\"images/logos/logo_lmd.jpg\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo IPSL\" src=\"images/logos/logo_ipsl.png\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo E4C\" src=\"images/logos/logo_e4c_final.png\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo EP\" src=\"images/logos/logo_ep.png\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo SU\" src=\"images/logos/logo_su.png\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo ENS\" src=\"images/logos/logo_ens.jpg\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo CNRS\" src=\"images/logos/logo_cnrs.png\" style=\"display: inline-block\"/>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<div style=\"display: flex\">\n",
    "    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0; margin-right: 10px\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a>\n",
    "    <br>This work is licensed under a &nbsp; <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
