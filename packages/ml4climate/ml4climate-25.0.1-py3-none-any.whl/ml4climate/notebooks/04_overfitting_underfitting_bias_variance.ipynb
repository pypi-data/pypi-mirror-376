{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45c0ff43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overfitting/Underfitting and Bias/Variance\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/git/https%3A%2F%2Fgitlab.in2p3.fr%2Fenergy4climate%2Fpublic%2Feducation%2Fmachine_learning_for_climate_and_energy/master?filepath=book%2Fnotebooks%2F03_overfitting_underfitting_bias_variance.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19beb10a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>Prerequisites</b>\n",
    "    \n",
    "- [Elements of Probability Theory](appendix_elements_of_probability_theory.ipynb)  \n",
    "- [Define a supervized-learning problem](2_supervised_learning_problem_ols.ipynb)\n",
    "- [Define and solve an Ordinary Least Squares problem](2_supervised_learning_problem_ols.ipynb)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98b11a0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Learning Outcomes</b>\n",
    "    \n",
    "- Understand when and why a model does or does not generalize well on unseen data\n",
    "- Understand the difference between the train error and the prediction error.\n",
    "- Understand overfitting-underfitting and bias-variance tradeoffs\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ad66a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overfitting and Underfitting\n",
    "\n",
    "Which fit do you prefer?\n",
    "\n",
    "<img alt=\"Linear fit\" src=\"images/linear_ols.svg\" width=\"400\" style=\"float:left\">\n",
    "<img alt=\"Linear fit\" src=\"images/linear_splines.svg\" width=\"400\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8471227b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Which model performs better on new data?\n",
    "\n",
    "<img alt=\"Linear fit\" src=\"images/linear_ols_test.svg\" width=\"400\" style=\"float:left\">\n",
    "<img alt=\"Linear fit\" src=\"images/linear_splines_test.svg\" width=\"400\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d8394d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A harder example:\n",
    "\n",
    "<img alt=\"Linear fit\" src=\"images/ols_simple_test.svg\" width=\"400\" style=\"float:left\">\n",
    "<img alt=\"Linear fit\" src=\"images/splines_cubic_test.svg\" width=\"400\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b1dfd6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Varying model complexity\n",
    "\n",
    "<img alt=\"Linear fit\" src=\"images/polynomial_overfit_truth.svg\" width=\"400\" style=\"float:left\">\n",
    "\n",
    "- Data generated by a random process\n",
    "  - Sample a value of $X$\n",
    "  - Transform with 9th-degree polynomial\n",
    "  - Add noise to get samples of $Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb71a909",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Varying model complexity\n",
    "\n",
    "<img alt=\"Linear fit\" src=\"images/polynomial_overfit_0.svg\" width=\"400\" style=\"float:left\">\n",
    "\n",
    "- Data generated by a random process\n",
    "- In fact, this process is unknown\n",
    "- We can only access observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b27a89",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Varying model complexity\n",
    "\n",
    "<img alt=\"Linear fit\" src=\"images/polynomial_overfit_1.svg\" width=\"400\" style=\"float:left\">\n",
    "\n",
    "- Data generated by a random process\n",
    "- In fact, this process is unknown\n",
    "- We can only access observations\n",
    "- Fit polynomials of various degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c5d251",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Varying model complexity\n",
    "\n",
    "<img alt=\"Linear fit\" src=\"images/polynomial_overfit_2.svg\" width=\"400\" style=\"float:left\">\n",
    "\n",
    "- Data generated by a random process\n",
    "- In fact, this process is unknown\n",
    "- We can only access observations\n",
    "- Fit polynomials of various degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290fd75c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Varying model complexity\n",
    "\n",
    "<img alt=\"Linear fit\" src=\"images/polynomial_overfit_5.svg\" width=\"400\" style=\"float:left\">\n",
    "\n",
    "- Data generated by a random process\n",
    "- In fact, this process is unknown\n",
    "- We can only access observations\n",
    "- Fit polynomials of various degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4519673",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Varying model complexity\n",
    "\n",
    "<img alt=\"Linear fit\" src=\"images/polynomial_overfit_9.svg\" width=\"400\" style=\"float:left\">\n",
    "\n",
    "- Data generated by a random process\n",
    "- In fact, this process is unknown\n",
    "- We can only access observations\n",
    "- Fit polynomials of various degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f89871b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Varying model complexity\n",
    "\n",
    "<img alt=\"Linear fit\" src=\"images/polynomial_overfit.svg\" width=\"400\" style=\"float:left\">\n",
    "\n",
    "- Data generated by a random process\n",
    "- In fact, this process is unknown\n",
    "- We can only access observations\n",
    "- Fit polynomials of various degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984e638b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Overfit: model too complex\n",
    "\n",
    "<img alt=\"Linear fit\" src=\"images/polynomial_overfit_simple_legend.svg\" width=\"400\" style=\"float:left;margin-right:40px\">\n",
    "\n",
    "Model too complex for the data:\n",
    "- Its best fit would approximate well the process\n",
    "- However, its flexibility captures noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfe7216",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Not enough data - Too much noise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4443f9a6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Underfit: model too simple\n",
    "\n",
    "<img alt=\"Linear fit\" src=\"images/polynomial_underfit_simple.svg\" width=\"400\" style=\"float:left;margin-right:40px\">\n",
    "\n",
    "Model too simple for the data:\n",
    "- Best fit would not approximate well the process\n",
    "- Yet it captures little noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9471a47",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Plenty of data - Low noise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e79bab9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Partial Summary\n",
    "\n",
    "- Models too complex for the data **overfit**:\n",
    "  - they explain too well the data that they have seen\n",
    "  - they do not generalize\n",
    "- Models too simple for the data **underfit**:\n",
    "  - they capture no noise\n",
    "  - they are limited by their expressivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c24eafc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Comparing Train and Test Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c84d4dc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Train vs Test (Prediction) Error\n",
    "\n",
    "<img src=\"images/linear_splines_test.svg\" style=\"float:left;margin-right:20px\" width=\"400\">\n",
    "\n",
    "- Error for $\\hat{f}$ from *train data* $(\\mathbf{x}_i, y_i), 1 \\le i \\le N$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\overline{\\mathrm{err}}_{\\hat{f}} = \\frac{1}{N} \\sum_{i = 1}^N L(y_i, \\hat{f}(\\mathbf{x}_i))\n",
    "\\end{equation}\n",
    "\n",
    "- Error on the *test data* (generalization):\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{Err}_\\hat{f} = \\mathbb{E}\\left[L(Y, \\hat{f}(\\boldsymbol{X})) | \\hat{f}\\right]\n",
    "\\end{equation}\n",
    "\n",
    "- The EPE is the expectation of $\\mathrm{Err}_\\hat{f}$ over all possible train data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea5897b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The train error is easy to estimate but does not tell us much:\n",
    "  - there is always a model that is sufficiently complex to predict the training data perfectly (e.g. 1-nearest neighbors, Legendre approximating polynomials, deep-enough neural network, etc.)\n",
    "  - but too complex models overfit\n",
    "- It is the test error that evaluates how the model performs on new data\n",
    "- It can be estimated:\n",
    "  - on some data spared from the training (test data)\n",
    "  - using more advanced methods such as cross-validation (see [Tutorial: Overfitting/Underfitting and Bias/Variance](04_tutorial_overfitting_underfitting_bias_variance.ipynb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727b20a2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Train vs test error: increasing complexity\n",
    "\n",
    "<img src=\"images/polynomial_overfit_test_1.svg\" width=\"400\" style=\"float:left\">\n",
    "<img src=\"images/polynomial_validation_curve_1.svg\" width=\"400\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb9973",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Train vs test error: increasing complexity\n",
    "\n",
    "<img src=\"images/polynomial_overfit_test_2.svg\" width=\"400\" style=\"float:left\">\n",
    "<img src=\"images/polynomial_validation_curve_2.svg\" width=\"400\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0a2b2f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Train vs test error: increasing complexity\n",
    "\n",
    "<img src=\"images/polynomial_overfit_test_5.svg\" width=\"400\" style=\"float:left\">\n",
    "<img src=\"images/polynomial_validation_curve_5.svg\" width=\"400\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fb62ea",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Train vs test error: increasing complexity\n",
    "\n",
    "<img src=\"images/polynomial_overfit_test_9.svg\" width=\"400\" style=\"float:left\">\n",
    "<img src=\"images/polynomial_validation_curve_15.svg\" width=\"400\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0613c03",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Train vs Test Error: Validation Curve\n",
    "\n",
    "<img src=\"images/polynomial_validation_curve_15_annotated.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2923a4f4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train vs Test Error: Varying Sample Size\n",
    "\n",
    "<img src=\"images/polynomial_overfit_ntrain_42.svg\" width=\"400\" style=\"float:left\">\n",
    "<img src=\"images/polynomial_learning_curve_42.svg\" width=\"400\" style=\"float:right\">\n",
    "\n",
    "<div style=\"clear:both;\"></div>\n",
    "\n",
    "<center><b>Overfit</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51f5045",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Train vs Test Error: Varying Sample Size\n",
    "\n",
    "<img src=\"images/polynomial_overfit_ntrain_145.svg\" width=\"400\" style=\"float:left\">\n",
    "<img src=\"images/polynomial_learning_curve_145.svg\" width=\"400\" style=\"float:right\">\n",
    "\n",
    "<div style=\"clear:both;\"></div>\n",
    "\n",
    "<center><b>Overfit less</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31cac2d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Train vs Test Error: Varying Sample Size\n",
    "\n",
    "<img src=\"images/polynomial_overfit_ntrain_1179.svg\" width=\"400\" style=\"float:left\">\n",
    "<img src=\"images/polynomial_learning_curve_1179.svg\" width=\"400\" style=\"float:right\">\n",
    "\n",
    "<div style=\"clear:both;\"></div>\n",
    "\n",
    "<center><b>Sweet spot?</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effe31a7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Train vs Test Error: Learning Curve\n",
    "\n",
    "<img src=\"images/polynomial_overfit_ntrain_6766.svg\" width=\"400\" style=\"float:left\">\n",
    "<img src=\"images/polynomial_learning_curve_6766.svg\" width=\"400\" style=\"float:right\">\n",
    "\n",
    "<div style=\"clear:both;\"></div>\n",
    "\n",
    "<center><b>Diminishing returns &#8594; Try more complex models?</b></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7386bce6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Irreducible Error\n",
    "\n",
    "<img src=\"images/polynomial_overfit_ntrain_6766.svg\" width=\"380\" style=\"float:left;margin-right:20px\">\n",
    "\n",
    "Error of best model trained on unlimited data\n",
    "\n",
    "Here, the data-generating process is a degree-9 polynomial\n",
    "\n",
    "A higher-degree polynomial will not do better\n",
    "\n",
    "**Predictions limited by noise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c563b9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Model Families\n",
    "\n",
    "Crucial to match:\n",
    "- statistical model\n",
    "- data-generating process\n",
    "\n",
    "So far: polynomial for both\n",
    "\n",
    "Some family names: *linear models, decision trees, random forests, kernel machines, multi-layer perceptrons*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f610692",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Different Model Families\n",
    "\n",
    "<img src=\"images/different_models_complex_4.svg\" width=\"380\" style=\"float:left;margin-right:20px\">\n",
    "\n",
    "- Different inductive (learning) bias\n",
    "- Different notion of complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14b5001",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Different Model Families\n",
    "\n",
    "<img src=\"images/different_models_complex_4.svg\" width=\"400\" style=\"float:left\">\n",
    "<img src=\"images/different_models_complex_16.svg\" width=\"400\" style=\"float:right\">\n",
    "\n",
    "<div style=\"clear:both\"></div>\n",
    "\n",
    "<div style=\"float:left\"><b>Simple variant</b></div>\n",
    "<div style=\"float:right\"><b>Complex variant</b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56985078",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Partial Summary\n",
    "\n",
    "Models **overfit**:\n",
    "- number of samples in the training set is too small for model's complexity\n",
    "- testing error is much bigger than training error\n",
    "\n",
    "Models **underfit**:\n",
    "- models fail to capture the shape of the training set\n",
    "- even the training error is large\n",
    "\n",
    "Different model families = different complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eebcc9f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bias versus Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998215d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Resampling the Training Set\n",
    "\n",
    "<div style=\"text-align:center; margin-left:25px; float:right\">\n",
    "    \n",
    "<img src=\"images/Simple_random_sampling.png\" width=\"400\">\n",
    "    \n",
    "<div style=\"font-size:small\">\n",
    "A visual representation of selecting a random sample.<br>\n",
    "<a href=\"https://commons.wikimedia.org/w/index.php?curid=36506020\" target=\"_blank\">By Dan Kernler - Own work, CC BY-SA 4.0</a>\n",
    "</div>\n",
    "</div>\n",
    "\n",
    "- A limited amount of training data\n",
    "- Training set is a small random subset<br>\n",
    "of all possible observations\n",
    "- What is the impact of this choice of training set<br>\n",
    "on the learned prediction function?\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd8588b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Practice: Varying Sample Size, Noise Level and Complexity\n",
    "\n",
    "The following plot represents a 8th-order polynomial to which Gaussian noise is added and to which a polynomial is fitted by OLS.\n",
    "\n",
    "> ***Question***\n",
    "> - Explain changes in the fits depending on the sample size $N$, noise level $\\sigma$ and model complexity $m$ in terms of bias (mean error) and variance of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dbad28c7-4435-4c4e-893c-b01e3e0dc5f2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0f135dea814d8a97673b713d946308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Checkbox(value=False, description='Resample'), IntSlider(value=100, description='N', max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot(Resample=False, N=100, sigma=0.5, m=1)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "# Import modules\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N = 100\n",
    "M = 7\n",
    "sc = 0.6\n",
    "roots = np.linspace(-1 / sc, 1. / sc, M)\n",
    "xlim = [-1.5, 1.5]\n",
    "ylim = [-3, 3]\n",
    "def poly(beta, x):\n",
    "    m = len(beta)\n",
    "    monomials = x**np.arange(m)\n",
    "    return beta @ monomials\n",
    "def poly_roots(roots, x):\n",
    "    return np.prod(x - roots) + x\n",
    "\n",
    "\n",
    "def plot(Resample=False,N=100, sigma=0.5, m=1):\n",
    "    x = np.sort(np.random.randn(N))\n",
    "    eps = np.random.randn(N)\n",
    "    fx = np.array([poly_roots(roots, x[i]) for i in range(len(x))])\n",
    "    ok = (fx > ylim[0]) & (fx < ylim[1])\n",
    "    x = x[ok]\n",
    "    fx = fx[ok]\n",
    "    \n",
    "    x0 = np.linspace(*xlim, 100)\n",
    "    fx0 = np.array([poly_roots(roots, x0[i]) for i in range(len(x0))])\n",
    "    sy_truth = pd.Series(fx0, index=x0)\n",
    "    sy_truth.name = 'f(x)'\n",
    "    \n",
    "    reg = linear_model.LinearRegression(fit_intercept=False)\n",
    "    X = np.array([x**i for i in range(m + 1)]).T\n",
    "\n",
    "    eps = eps[ok]\n",
    "    y = fx + sigma * eps\n",
    "    sy = pd.Series(y, index=x)\n",
    "    sy.index.name = 'x'\n",
    "    sy.name = 'y'\n",
    "    \n",
    "    reg.fit(X, y)\n",
    "    X0 = np.array([x0**i for i in range(m + 1)]).T\n",
    "    sy_pred = pd.Series(reg.predict(X0), index=x0)\n",
    "    sy_pred.name = \"f'(x)\"\n",
    "    title = \"f'(x) = \" + '+'.join(\n",
    "        '{}*x^{}'.format(str(np.round(reg.coef_[i], 1)), i)\n",
    "        for i in range(len(reg.coef_)))\n",
    "    \n",
    "    sy_truth.plot(linestyle='dashed', linewidth=3, color='black', legend=True)\n",
    "    sy.plot(style='.',title=title, xlim=xlim, ylim=ylim, color='k', legend=True)\n",
    "    sy_pred.plot(color='blue', linewidth=3, legend=True)\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "interact(plot,N=(10, 310, 10), sigma=(0., 1., 0.1), m=(0, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ffa39b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Overfit: variance\n",
    "\n",
    "<img src=\"images/polynomial_overfit_resample_0.svg\" width=\"400\" style=\"float:left\">\n",
    "<img src=\"images/target_variance_0.svg\" width=\"400\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2e4ba2a-b92e-4f6b-b26a-eb00ab3788a7",
   "metadata": {},
   "outputs": [],
   "source": [
    " from ipywidgets import interact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7573d1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Overfit: variance\n",
    "\n",
    "<img src=\"images/polynomial_overfit_resample_1.svg\" width=\"400\" style=\"float:left\">\n",
    "<img src=\"images/target_variance_1.svg\" width=\"400\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a45f114",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Overfit: variance\n",
    "\n",
    "<img src=\"images/polynomial_overfit_resample_2.svg\" width=\"400\" style=\"float:left\">\n",
    "<img src=\"images/target_variance_2.svg\" width=\"400\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476813a7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Overfit: variance\n",
    "\n",
    "<img src=\"images/polynomial_overfit_resample_all.svg\" width=\"400\" style=\"float:left\">\n",
    "<img src=\"images/target_variance.svg\" width=\"400\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c935d2a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Underfit: bias\n",
    "\n",
    "<img src=\"images/polynomial_underfit_resample_0.svg\" width=\"400\" style=\"float:left\">\n",
    "<img src=\"images/target_bias_0.svg\" width=\"400\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb64b248",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Underfit: bias\n",
    "\n",
    "<img src=\"images/polynomial_underfit_resample_1.svg\" width=\"400\" style=\"float:left\">\n",
    "<img src=\"images/target_bias_1.svg\" width=\"400\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fad37a2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Underfit: bias\n",
    "\n",
    "<img src=\"images/polynomial_underfit_resample_2.svg\" width=\"400\" style=\"float:left\">\n",
    "<img src=\"images/target_bias_2.svg\" width=\"400\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26cea4d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Underfit: bias\n",
    "\n",
    "<img src=\"images/polynomial_underfit_resample_all.svg\" width=\"400\" style=\"float:left\">\n",
    "<img src=\"images/target_bias.svg\" width=\"400\" style=\"float:right\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b2cecc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Underfit versus Overfit\n",
    "\n",
    "<div style=\"text-align:center;float:left\">\n",
    "    \n",
    "<img src=\"images/target_bias.svg\" width=\"400\">\n",
    "    \n",
    "<div><b>Underfit</b></div>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center;float:right\">\n",
    "    \n",
    "<img src=\"images/target_variance.svg\" width=\"400\">\n",
    "    \n",
    "<div><b>Overfit</b></div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f7206b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bias-Variance Decomposition of the EPE (Univariate)\n",
    "\n",
    "We assume that\n",
    "\n",
    "$$\n",
    "    Y = g(X) + \\varepsilon,\n",
    "$$\n",
    "\n",
    "where $\\mathbb{E}(\\varepsilon) = 0$ and $\\mathrm{Var}(\\varepsilon) = \\sigma_\\varepsilon^2$.\n",
    "\n",
    "The EPE at a new point $X = x_0$ for a model $f$ is\n",
    "\n",
    "$$\\mathrm{EPE}(f, x_0) = \\mathbb{E}[(Y - \\hat{f}(x_0))^2 | X = x_0)],$$\n",
    "\n",
    "where the expectation is taken with respect to the distribution of the train datasets (i.e. over all fits $\\hat{f}$ of $f$) and to the distribution of the output conditioned on $x_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b5d10f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> ***Question (optional)***\n",
    "> - Show that\n",
    "> $$\n",
    " \\mathrm{EPE}(f, x_0) = \\sigma_\\varepsilon^2 + \\mathrm{Bias}^2[\\hat{f}(x_0)] + \\mathrm{Var}[\\hat{f}(x_0)],\n",
    "  $$\n",
    "> where\n",
    ">   - $\\mathrm{Bias}^2[\\hat{f}(x_0)] = \\{\\mathbb{E}[\\hat{f}(x_0) - g(x_0)]\\}^2 = \\{\\mathbb{E}[\\hat{f}(x_0)] - g(x_0)\\}^2$,\n",
    ">   - $\\mathrm{Var}[\\hat{f}(x_0)] = \\mathbb{E}[(\\hat{f}(x_0) - \\mathbb{E}[\\hat{f}(x_0)])^2]$.\n",
    "\n",
    "> ***Question***\n",
    "> - What kind of error does each of these 3 terms represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64f13a6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div  style=\"text-align:center;float:left\">\n",
    "    \n",
    "<img src=\"images/polynomial_validation_curve_15.svg\" width=\"400\">\n",
    "    \n",
    "<div>Validation curve.</div>    \n",
    "</div>\n",
    "<div  style=\"text-align:center;float:right\">\n",
    "    \n",
    "<img src=\"images/polynomial_learning_curve_6766.svg\" width=\"400\">\n",
    "    \n",
    "<div>Learning curve.</div>    \n",
    "</div>\n",
    "\n",
    "<div style=\"clear:both;\"></div>\n",
    "\n",
    "> ***Question***\n",
    "> - Interpret the validation and learning curves above based on the bias-variance decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a62791f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Bias-Variance Decomposition for the OLS\n",
    "\n",
    "> ***Question (optional)***\n",
    "> - Assuming $\\mathbf{X}^\\top \\mathbf{X}$ invertible, show that the in-sample error is given by\n",
    "> $$\n",
    " \\frac{1}{N}\\sum_{i = 1}^N \\mathrm{EPE}(f, x_i) = \\sigma_\\varepsilon^2 + \\frac{1}{N} \\sum_{i = 1}^N \\{\\mathbb{E}[\\hat{f}(x_i)] - g(x_i)\\}^2 + \\frac{p}{N} \\sigma_\\varepsilon^2.\n",
    " $$\n",
    "> - What if $\\mathbf{X}^\\top \\mathbf{X}$ is not invertible?\n",
    "\n",
    "> ***Question***\n",
    "> - How does the variance part of the error depend on the parameters of the problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e2c9c5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Partial Summary\n",
    "\n",
    "**High bias** == **underfitting**:\n",
    "\n",
    "- systematic prediction errors\n",
    "- the model prefers to ignore some aspects of the data\n",
    "- mispecified models\n",
    "\n",
    "**High variance** == **overfitting**:\n",
    "\n",
    "- prediction errors without obvious structure\n",
    "- small change in the training set, large change in model\n",
    "- unstable models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa607118",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## To Go Further\n",
    "\n",
    "- Absence of overfitting issue in Bayesian approach (e.g. Chap. 3 in Bishop 2006)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84490ab4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "- [James, G., Witten, D., Hastie, T., Tibshirani, R., n.d. *An Introduction to Statistical Learning*, 2st ed. Springer, New York, NY.](https://www.statlearning.com/)\n",
    "- Chap. 2, 3 and 7 in [Hastie, T., Tibshirani, R., Friedman, J., 2009. *The Elements of Statistical Learning*, 2nd ed. Springer, New York.](https://doi.org/10.1007/978-0-387-84858-7)\n",
    "- For a Bayesian perspective, Chap. 3 in [Bishop, C. (2006). *Pattern Recognition and Machine Learning*. Springer-Verlag](https://www.cs.uoi.gr/~arly/courses/ml/tmp/Bishop_book.pdf)\n",
    "- Chap. 5 and 7 in [Wilks, D.S., 2019. *Statistical Methods in the Atmospheric Sciences*, 4th ed. Elsevier, Amsterdam.](https://doi.org/10.1016/C2017-0-03921-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd1fa11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "***\n",
    "## Credit\n",
    "\n",
    "[//]: # \"This notebook is part of [E4C Interdisciplinary Center - Education](https://gitlab.in2p3.fr/energy4climate/public/education).\"\n",
    "Contributors include Bruno Deremble and Alexis Tantet.\n",
    "Several slides and images are taken from the very good [Scikit-learn course](https://inria.github.io/scikit-learn-mooc/).\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"display: flex; height: 70px\">\n",
    "    \n",
    "<img alt=\"Logo LMD\" src=\"images/logos/logo_lmd.jpg\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo IPSL\" src=\"images/logos/logo_ipsl.png\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo E4C\" src=\"images/logos/logo_e4c_final.png\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo EP\" src=\"images/logos/logo_ep.png\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo SU\" src=\"images/logos/logo_su.png\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo ENS\" src=\"images/logos/logo_ens.jpg\" style=\"display: inline-block\"/>\n",
    "\n",
    "<img alt=\"Logo CNRS\" src=\"images/logos/logo_cnrs.png\" style=\"display: inline-block\"/>\n",
    "    \n",
    "</div>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<div style=\"display: flex\">\n",
    "    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0; margin-right: 10px\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a>\n",
    "    <br>This work is licensed under a &nbsp; <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
