# 
# Generated by the nebius.base.protos.compiler.  DO NOT EDIT!
# 

import builtins as builtins
import collections.abc as abc
import google.protobuf.descriptor as descriptor_1
import google.protobuf.message as message_1
import grpc as grpc
import nebius.aio.client as client
import nebius.aio.operation as operation
import nebius.aio.request as request_1
import nebius.api.nebius.common.v1 as v1_1
import nebius.api.nebius.common.v1.metadata_pb2 as metadata_pb2
import nebius.api.nebius.common.v1.operation_pb2 as operation_pb2
import nebius.api.nebius.msp.spark.v1alpha1.cluster_pb2 as cluster_pb2
import nebius.api.nebius.msp.spark.v1alpha1.cluster_service_pb2 as cluster_service_pb2
import nebius.api.nebius.msp.spark.v1alpha1.common_pb2 as common_pb2
import nebius.api.nebius.msp.spark.v1alpha1.job_pb2 as job_pb2
import nebius.api.nebius.msp.spark.v1alpha1.job_service_pb2 as job_service_pb2
import nebius.api.nebius.msp.spark.v1alpha1.preset_pb2 as preset_pb2
import nebius.api.nebius.msp.spark.v1alpha1.session_pb2 as session_pb2
import nebius.api.nebius.msp.spark.v1alpha1.session_service_pb2 as session_service_pb2
import nebius.api.nebius.msp.v1alpha1 as v1alpha1_1
import nebius.api.nebius.msp.v1alpha1.cluster_pb2 as cluster_pb2_1
import nebius.api.nebius.msp.v1alpha1.resource as resource_1
import nebius.api.nebius.msp.v1alpha1.resource.template_pb2 as template_pb2
import nebius.base.fieldmask_protobuf as fieldmask_protobuf
import nebius.base.protos.descriptor as descriptor
import nebius.base.protos.pb_classes as pb_classes
import nebius.base.protos.pb_enum as pb_enum
import nebius.base.protos.unset as unset
import typing as typing
#@ local imports here @#

# file: nebius/msp/spark/v1alpha1/cluster.proto
class Cluster(pb_classes.Message):
    __PB2_CLASS__ = cluster_pb2.Cluster
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.Cluster",cluster_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        metadata: "v1_1.ResourceMetadata|metadata_pb2.ResourceMetadata|None|unset.UnsetType" = unset.Unset,
        spec: "ClusterSpec|cluster_pb2.ClusterSpec|None|unset.UnsetType" = unset.Unset,
        status: "ClusterStatus|cluster_pb2.ClusterStatus|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(metadata, unset.UnsetType):
            self.metadata = metadata
        if not isinstance(spec, unset.UnsetType):
            self.spec = spec
        if not isinstance(status, unset.UnsetType):
            self.status = status
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "metadata",
            "spec",
            "status",
        ]
    
    @builtins.property
    def metadata(self) -> "v1_1.ResourceMetadata":
        return super()._get_field("metadata", explicit_presence=False,
        wrap=v1_1.ResourceMetadata,
        )
    @metadata.setter
    def metadata(self, value: "v1_1.ResourceMetadata|metadata_pb2.ResourceMetadata|None") -> None:
        return super()._set_field("metadata",value,explicit_presence=False,
        )
    
    @builtins.property
    def spec(self) -> "ClusterSpec":
        return super()._get_field("spec", explicit_presence=False,
        wrap=ClusterSpec,
        )
    @spec.setter
    def spec(self, value: "ClusterSpec|cluster_pb2.ClusterSpec|None") -> None:
        return super()._set_field("spec",value,explicit_presence=False,
        )
    
    @builtins.property
    def status(self) -> "ClusterStatus":
        return super()._get_field("status", explicit_presence=False,
        wrap=ClusterStatus,
        )
    @status.setter
    def status(self, value: "ClusterStatus|cluster_pb2.ClusterStatus|None") -> None:
        return super()._set_field("status",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "metadata":"metadata",
        "spec":"spec",
        "status":"status",
    }
    
class ClusterSpec(pb_classes.Message):
    """
    Cluster specification
    """
    
    __PB2_CLASS__ = cluster_pb2.ClusterSpec
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.ClusterSpec",cluster_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    class __OneOfClass__description__(pb_classes.OneOf):
        name: builtins.str= "_description"
        
        def __init__(self, msg: "ClusterSpec") -> None:
            super().__init__()
            self._message: "ClusterSpec" = msg
    
    class __OneOfClass__description_description__(__OneOfClass__description__):
        field: typing.Literal["description"] = "description"
        
        def __init__(self, msg: "ClusterSpec") -> None:
            super().__init__(msg)
        @builtins.property
        def value(self) -> "builtins.str":
            return self._message.description
    
    @builtins.property
    def _description(self) -> __OneOfClass__description_description__|None:
        field_name_1: str|None = super().which_field_in_oneof("_description")
        match field_name_1:
            case "description":
                return self.__OneOfClass__description_description__(self)
            case None:
                return None
            case _:
                raise pb_classes.OneOfMatchError(field_name_1)
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        description: "builtins.str|None|unset.UnsetType" = unset.Unset,
        limits: "Limits|cluster_pb2.Limits|None|unset.UnsetType" = unset.Unset,
        authorization: "Password|cluster_pb2.Password|None|unset.UnsetType" = unset.Unset,
        service_account_id: "builtins.str|None|unset.UnsetType" = unset.Unset,
        network_id: "builtins.str|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(description, unset.UnsetType):
            self.description = description
        if not isinstance(limits, unset.UnsetType):
            self.limits = limits
        if not isinstance(authorization, unset.UnsetType):
            self.authorization = authorization
        if not isinstance(service_account_id, unset.UnsetType):
            self.service_account_id = service_account_id
        if not isinstance(network_id, unset.UnsetType):
            self.network_id = network_id
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "description",
            "limits",
            "authorization",
            "service_account_id",
            "network_id",
            "_description",
        ]
    
    @builtins.property
    def description(self) -> "builtins.str|None":
        """
        Description of the cluster.
        """
        
        return super()._get_field("description", explicit_presence=True,
        )
    @description.setter
    def description(self, value: "builtins.str|None") -> None:
        return super()._set_field("description",value,explicit_presence=True,
        )
    
    @builtins.property
    def limits(self) -> "Limits":
        """
        Limits for the cluster
        """
        
        return super()._get_field("limits", explicit_presence=False,
        wrap=Limits,
        )
    @limits.setter
    def limits(self, value: "Limits|cluster_pb2.Limits|None") -> None:
        return super()._set_field("limits",value,explicit_presence=False,
        )
    
    @builtins.property
    def authorization(self) -> "Password":
        """
        Password for Spark History server and Sessions.
        """
        
        return super()._get_field("authorization", explicit_presence=False,
        wrap=Password,
        )
    @authorization.setter
    def authorization(self, value: "Password|cluster_pb2.Password|None") -> None:
        return super()._set_field("authorization",value,explicit_presence=False,
        )
    
    @builtins.property
    def service_account_id(self) -> "builtins.str":
        """
        ID of the user service account for accessing
        S3 buckets in the user project
        """
        
        return super()._get_field("service_account_id", explicit_presence=False,
        )
    @service_account_id.setter
    def service_account_id(self, value: "builtins.str|None") -> None:
        return super()._set_field("service_account_id",value,explicit_presence=False,
        )
    
    @builtins.property
    def network_id(self) -> "builtins.str":
        return super()._get_field("network_id", explicit_presence=False,
        )
    @network_id.setter
    def network_id(self, value: "builtins.str|None") -> None:
        return super()._set_field("network_id",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "description":"description",
        "limits":"limits",
        "authorization":"authorization",
        "service_account_id":"service_account_id",
        "network_id":"network_id",
        "_description":"_description",
    }
    
class ClusterStatus(pb_classes.Message):
    __PB2_CLASS__ = cluster_pb2.ClusterStatus
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.ClusterStatus",cluster_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    class __OneOfClass__history_server_endpoint__(pb_classes.OneOf):
        name: builtins.str= "_history_server_endpoint"
        
        def __init__(self, msg: "ClusterStatus") -> None:
            super().__init__()
            self._message: "ClusterStatus" = msg
    
    class __OneOfClass__history_server_endpoint_history_server_endpoint__(__OneOfClass__history_server_endpoint__):
        field: typing.Literal["history_server_endpoint"] = "history_server_endpoint"
        
        def __init__(self, msg: "ClusterStatus") -> None:
            super().__init__(msg)
        @builtins.property
        def value(self) -> "builtins.str":
            return self._message.history_server_endpoint
    
    @builtins.property
    def _history_server_endpoint(self) -> __OneOfClass__history_server_endpoint_history_server_endpoint__|None:
        field_name_1: str|None = super().which_field_in_oneof("_history_server_endpoint")
        match field_name_1:
            case "history_server_endpoint":
                return self.__OneOfClass__history_server_endpoint_history_server_endpoint__(self)
            case None:
                return None
            case _:
                raise pb_classes.OneOfMatchError(field_name_1)
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        phase: "v1alpha1_1.ClusterStatus.Phase|cluster_pb2_1.ClusterStatus.Phase|None|unset.UnsetType" = unset.Unset,
        state: "v1alpha1_1.ClusterStatus.State|cluster_pb2_1.ClusterStatus.State|None|unset.UnsetType" = unset.Unset,
        history_server_endpoint: "builtins.str|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(phase, unset.UnsetType):
            self.phase = phase
        if not isinstance(state, unset.UnsetType):
            self.state = state
        if not isinstance(history_server_endpoint, unset.UnsetType):
            self.history_server_endpoint = history_server_endpoint
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "phase",
            "state",
            "history_server_endpoint",
            "_history_server_endpoint",
        ]
    
    @builtins.property
    def phase(self) -> "v1alpha1_1.ClusterStatus.Phase":
        """
        Current phase (or stage) of the cluster.
        """
        
        return super()._get_field("phase", explicit_presence=False,
        wrap=v1alpha1_1.ClusterStatus.Phase,
        )
    @phase.setter
    def phase(self, value: "v1alpha1_1.ClusterStatus.Phase|cluster_pb2_1.ClusterStatus.Phase|None") -> None:
        return super()._set_field("phase",value,explicit_presence=False,
        )
    
    @builtins.property
    def state(self) -> "v1alpha1_1.ClusterStatus.State":
        """
        State reflects substatus of the stage to define whether it's healthy or not.
        """
        
        return super()._get_field("state", explicit_presence=False,
        wrap=v1alpha1_1.ClusterStatus.State,
        )
    @state.setter
    def state(self, value: "v1alpha1_1.ClusterStatus.State|cluster_pb2_1.ClusterStatus.State|None") -> None:
        return super()._set_field("state",value,explicit_presence=False,
        )
    
    @builtins.property
    def history_server_endpoint(self) -> "builtins.str|None":
        """
        History Server WebUI endpoint
        """
        
        return super()._get_field("history_server_endpoint", explicit_presence=True,
        )
    @history_server_endpoint.setter
    def history_server_endpoint(self, value: "builtins.str|None") -> None:
        return super()._set_field("history_server_endpoint",value,explicit_presence=True,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "phase":"phase",
        "state":"state",
        "history_server_endpoint":"history_server_endpoint",
        "_history_server_endpoint":"_history_server_endpoint",
    }
    
class Limits(pb_classes.Message):
    __PB2_CLASS__ = cluster_pb2.Limits
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.Limits",cluster_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        cpu: "builtins.int|None|unset.UnsetType" = unset.Unset,
        memory_gibibytes: "builtins.int|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(cpu, unset.UnsetType):
            self.cpu = cpu
        if not isinstance(memory_gibibytes, unset.UnsetType):
            self.memory_gibibytes = memory_gibibytes
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "cpu",
            "memory_gibibytes",
        ]
    
    @builtins.property
    def cpu(self) -> "builtins.int":
        return super()._get_field("cpu", explicit_presence=False,
        )
    @cpu.setter
    def cpu(self, value: "builtins.int|None") -> None:
        return super()._set_field("cpu",value,explicit_presence=False,
        )
    
    @builtins.property
    def memory_gibibytes(self) -> "builtins.int":
        return super()._get_field("memory_gibibytes", explicit_presence=False,
        )
    @memory_gibibytes.setter
    def memory_gibibytes(self, value: "builtins.int|None") -> None:
        return super()._set_field("memory_gibibytes",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "cpu":"cpu",
        "memory_gibibytes":"memory_gibibytes",
    }
    
class Password(pb_classes.Message):
    __PB2_CLASS__ = cluster_pb2.Password
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.Password",cluster_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        password: "builtins.str|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(password, unset.UnsetType):
            self.password = password
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "password",
        ]
    
    @builtins.property
    def password(self) -> "builtins.str":
        return super()._get_field("password", explicit_presence=False,
        )
    @password.setter
    def password(self, value: "builtins.str|None") -> None:
        return super()._set_field("password",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "password":"password",
    }
    
# file: nebius/msp/spark/v1alpha1/cluster_service.proto
class GetClusterRequest(pb_classes.Message):
    __PB2_CLASS__ = cluster_service_pb2.GetClusterRequest
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.GetClusterRequest",cluster_service_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        id: "builtins.str|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(id, unset.UnsetType):
            self.id = id
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "id",
        ]
    
    @builtins.property
    def id(self) -> "builtins.str":
        """
        ID of the cluster to retrieve.
        """
        
        return super()._get_field("id", explicit_presence=False,
        )
    @id.setter
    def id(self, value: "builtins.str|None") -> None:
        return super()._set_field("id",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "id":"id",
    }
    
class GetClusterByNameRequest(pb_classes.Message):
    __PB2_CLASS__ = cluster_service_pb2.GetClusterByNameRequest
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.GetClusterByNameRequest",cluster_service_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        parent_id: "builtins.str|None|unset.UnsetType" = unset.Unset,
        name: "builtins.str|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(parent_id, unset.UnsetType):
            self.parent_id = parent_id
        if not isinstance(name, unset.UnsetType):
            self.name = name
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "parent_id",
            "name",
        ]
    
    @builtins.property
    def parent_id(self) -> "builtins.str":
        """
        Parent ID of the cluster to retrieve.
        """
        
        return super()._get_field("parent_id", explicit_presence=False,
        )
    @parent_id.setter
    def parent_id(self, value: "builtins.str|None") -> None:
        return super()._set_field("parent_id",value,explicit_presence=False,
        )
    
    @builtins.property
    def name(self) -> "builtins.str":
        """
        Name of the cluster to retrieve.
        """
        
        return super()._get_field("name", explicit_presence=False,
        )
    @name.setter
    def name(self, value: "builtins.str|None") -> None:
        return super()._set_field("name",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "parent_id":"parent_id",
        "name":"name",
    }
    
class ListClustersRequest(pb_classes.Message):
    __PB2_CLASS__ = cluster_service_pb2.ListClustersRequest
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.ListClustersRequest",cluster_service_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        parent_id: "builtins.str|None|unset.UnsetType" = unset.Unset,
        page_size: "builtins.int|None|unset.UnsetType" = unset.Unset,
        page_token: "builtins.str|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(parent_id, unset.UnsetType):
            self.parent_id = parent_id
        if not isinstance(page_size, unset.UnsetType):
            self.page_size = page_size
        if not isinstance(page_token, unset.UnsetType):
            self.page_token = page_token
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "parent_id",
            "page_size",
            "page_token",
        ]
    
    @builtins.property
    def parent_id(self) -> "builtins.str":
        """
        Identifier of IAM container to list clusters from.
        """
        
        return super()._get_field("parent_id", explicit_presence=False,
        )
    @parent_id.setter
    def parent_id(self, value: "builtins.str|None") -> None:
        return super()._set_field("parent_id",value,explicit_presence=False,
        )
    
    @builtins.property
    def page_size(self) -> "builtins.int":
        """
        Specifies the maximum number of items to return in the response. Default value is 100.
        """
        
        return super()._get_field("page_size", explicit_presence=False,
        )
    @page_size.setter
    def page_size(self, value: "builtins.int|None") -> None:
        return super()._set_field("page_size",value,explicit_presence=False,
        )
    
    @builtins.property
    def page_token(self) -> "builtins.str":
        """
        Token for pagination, allowing the retrieval of the next set of results.
        """
        
        return super()._get_field("page_token", explicit_presence=False,
        )
    @page_token.setter
    def page_token(self, value: "builtins.str|None") -> None:
        return super()._set_field("page_token",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "parent_id":"parent_id",
        "page_size":"page_size",
        "page_token":"page_token",
    }
    
class ListClustersResponse(pb_classes.Message):
    __PB2_CLASS__ = cluster_service_pb2.ListClustersResponse
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.ListClustersResponse",cluster_service_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    class __OneOfClass__next_page_token__(pb_classes.OneOf):
        name: builtins.str= "_next_page_token"
        
        def __init__(self, msg: "ListClustersResponse") -> None:
            super().__init__()
            self._message: "ListClustersResponse" = msg
    
    class __OneOfClass__next_page_token_next_page_token__(__OneOfClass__next_page_token__):
        field: typing.Literal["next_page_token"] = "next_page_token"
        
        def __init__(self, msg: "ListClustersResponse") -> None:
            super().__init__(msg)
        @builtins.property
        def value(self) -> "builtins.str":
            return self._message.next_page_token
    
    @builtins.property
    def _next_page_token(self) -> __OneOfClass__next_page_token_next_page_token__|None:
        field_name_1: str|None = super().which_field_in_oneof("_next_page_token")
        match field_name_1:
            case "next_page_token":
                return self.__OneOfClass__next_page_token_next_page_token__(self)
            case None:
                return None
            case _:
                raise pb_classes.OneOfMatchError(field_name_1)
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        items: "abc.Iterable[Cluster]|None|unset.UnsetType" = unset.Unset,
        next_page_token: "builtins.str|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(items, unset.UnsetType):
            self.items = items
        if not isinstance(next_page_token, unset.UnsetType):
            self.next_page_token = next_page_token
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "items",
            "next_page_token",
            "_next_page_token",
        ]
    
    @builtins.property
    def items(self) -> "abc.MutableSequence[Cluster]":
        """
        List of clusters.
        """
        
        return super()._get_field("items", explicit_presence=False,
        wrap=pb_classes.Repeated.with_wrap(Cluster,None,None),
        )
    @items.setter
    def items(self, value: "abc.Iterable[Cluster]|None") -> None:
        return super()._set_field("items",value,explicit_presence=False,
        )
    
    @builtins.property
    def next_page_token(self) -> "builtins.str|None":
        """
        Token for pagination, indicating the next set of results can be retrieved using this token.
        """
        
        return super()._get_field("next_page_token", explicit_presence=True,
        )
    @next_page_token.setter
    def next_page_token(self, value: "builtins.str|None") -> None:
        return super()._set_field("next_page_token",value,explicit_presence=True,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "items":"items",
        "next_page_token":"next_page_token",
        "_next_page_token":"_next_page_token",
    }
    
class CreateClusterRequest(pb_classes.Message):
    __PB2_CLASS__ = cluster_service_pb2.CreateClusterRequest
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.CreateClusterRequest",cluster_service_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        metadata: "v1_1.ResourceMetadata|metadata_pb2.ResourceMetadata|None|unset.UnsetType" = unset.Unset,
        spec: "ClusterSpec|cluster_pb2.ClusterSpec|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(metadata, unset.UnsetType):
            self.metadata = metadata
        if not isinstance(spec, unset.UnsetType):
            self.spec = spec
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "metadata",
            "spec",
        ]
    
    @builtins.property
    def metadata(self) -> "v1_1.ResourceMetadata":
        """
        Metadata associated with the new cluster. Must include parent_id in which we create the cluster.
        """
        
        return super()._get_field("metadata", explicit_presence=False,
        wrap=v1_1.ResourceMetadata,
        )
    @metadata.setter
    def metadata(self, value: "v1_1.ResourceMetadata|metadata_pb2.ResourceMetadata|None") -> None:
        return super()._set_field("metadata",value,explicit_presence=False,
        )
    
    @builtins.property
    def spec(self) -> "ClusterSpec":
        """
        Specification for the new cluster.
        """
        
        return super()._get_field("spec", explicit_presence=False,
        wrap=ClusterSpec,
        )
    @spec.setter
    def spec(self, value: "ClusterSpec|cluster_pb2.ClusterSpec|None") -> None:
        return super()._set_field("spec",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "metadata":"metadata",
        "spec":"spec",
    }
    
class UpdateClusterRequest(pb_classes.Message):
    __PB2_CLASS__ = cluster_service_pb2.UpdateClusterRequest
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.UpdateClusterRequest",cluster_service_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        metadata: "v1_1.ResourceMetadata|metadata_pb2.ResourceMetadata|None|unset.UnsetType" = unset.Unset,
        spec: "ClusterSpec|cluster_pb2.ClusterSpec|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(metadata, unset.UnsetType):
            self.metadata = metadata
        if not isinstance(spec, unset.UnsetType):
            self.spec = spec
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "metadata",
            "spec",
        ]
    
    @builtins.property
    def metadata(self) -> "v1_1.ResourceMetadata":
        """
        Metadata associated with the cluster. Must include id of the cluster we are going to update.
        """
        
        return super()._get_field("metadata", explicit_presence=False,
        wrap=v1_1.ResourceMetadata,
        )
    @metadata.setter
    def metadata(self, value: "v1_1.ResourceMetadata|metadata_pb2.ResourceMetadata|None") -> None:
        return super()._set_field("metadata",value,explicit_presence=False,
        )
    
    @builtins.property
    def spec(self) -> "ClusterSpec":
        """
        Updated specification for the cluster.
        """
        
        return super()._get_field("spec", explicit_presence=False,
        wrap=ClusterSpec,
        )
    @spec.setter
    def spec(self, value: "ClusterSpec|cluster_pb2.ClusterSpec|None") -> None:
        return super()._set_field("spec",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "metadata":"metadata",
        "spec":"spec",
    }
    
class DeleteClusterRequest(pb_classes.Message):
    __PB2_CLASS__ = cluster_service_pb2.DeleteClusterRequest
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.DeleteClusterRequest",cluster_service_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        id: "builtins.str|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(id, unset.UnsetType):
            self.id = id
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "id",
        ]
    
    @builtins.property
    def id(self) -> "builtins.str":
        """
        ID of the cluster to delete.
        """
        
        return super()._get_field("id", explicit_presence=False,
        )
    @id.setter
    def id(self, value: "builtins.str|None") -> None:
        return super()._set_field("id",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "id":"id",
    }
    

class ClusterServiceClient(client.ClientWithOperations[v1_1.Operation,v1_1.OperationServiceClient]):
    """
    Supported until 08/12/25. Nebius AI Cloud no longer supports the service. Instead, use Application for Apache Spark™ Connect in Standalone Applications.
    """
    
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.ServiceDescriptor](".nebius.msp.spark.v1alpha1.ClusterService",cluster_service_pb2.DESCRIPTOR,descriptor_1.ServiceDescriptor)
    __service_name__ = ".nebius.msp.spark.v1alpha1.ClusterService"
    __operation_type__ = v1_1.Operation
    __operation_service_class__ = v1_1.OperationServiceClient
    __operation_source_method__ = "Create"
    __service_deprecation_details__ = (
    """Service .nebius.msp.spark.v1alpha1.ClusterService is deprecated. Supported until 08/12/25. Nebius AI Cloud no longer supports the service. Instead, use Application for Apache Spark™ Connect in Standalone Applications."""
    )
    
    def get(self,
        request: "GetClusterRequest",
        metadata: abc.Iterable[builtins.tuple[builtins.str,builtins.str]]|None = None,
        timeout: builtins.float|unset.UnsetType|None = unset.Unset,
        credentials: grpc.CallCredentials | None = None,
        compression: grpc.Compression | None = None,
        retries: builtins.int | None = 3,
        per_retry_timeout: builtins.float|unset.UnsetType|None = unset.Unset,
    ) -> request_1.Request["GetClusterRequest","Cluster"]:
        """
        Returns the specified cluster.
        """
        
        return super().request(
            method="Get",
            request=request,
            result_pb2_class=cluster_pb2.Cluster,
            metadata=metadata,
            timeout=timeout,
            credentials=credentials,
            compression=compression,
            retries=retries,
            per_retry_timeout=per_retry_timeout,
            result_wrapper=pb_classes.simple_wrapper(Cluster),
        )
    
    def get_by_name(self,
        request: "GetClusterByNameRequest",
        metadata: abc.Iterable[builtins.tuple[builtins.str,builtins.str]]|None = None,
        timeout: builtins.float|unset.UnsetType|None = unset.Unset,
        credentials: grpc.CallCredentials | None = None,
        compression: grpc.Compression | None = None,
        retries: builtins.int | None = 3,
        per_retry_timeout: builtins.float|unset.UnsetType|None = unset.Unset,
    ) -> request_1.Request["GetClusterByNameRequest","Cluster"]:
        """
        Returns the specified cluster by name.
        """
        
        return super().request(
            method="GetByName",
            request=request,
            result_pb2_class=cluster_pb2.Cluster,
            metadata=metadata,
            timeout=timeout,
            credentials=credentials,
            compression=compression,
            retries=retries,
            per_retry_timeout=per_retry_timeout,
            result_wrapper=pb_classes.simple_wrapper(Cluster),
        )
    
    def list(self,
        request: "ListClustersRequest",
        metadata: abc.Iterable[builtins.tuple[builtins.str,builtins.str]]|None = None,
        timeout: builtins.float|unset.UnsetType|None = unset.Unset,
        credentials: grpc.CallCredentials | None = None,
        compression: grpc.Compression | None = None,
        retries: builtins.int | None = 3,
        per_retry_timeout: builtins.float|unset.UnsetType|None = unset.Unset,
    ) -> request_1.Request["ListClustersRequest","ListClustersResponse"]:
        """
        Retrieves a list of clusters.
        """
        
        return super().request(
            method="List",
            request=request,
            result_pb2_class=cluster_service_pb2.ListClustersResponse,
            metadata=metadata,
            timeout=timeout,
            credentials=credentials,
            compression=compression,
            retries=retries,
            per_retry_timeout=per_retry_timeout,
            result_wrapper=pb_classes.simple_wrapper(ListClustersResponse),
        )
    
    def create(self,
        request: "CreateClusterRequest",
        metadata: abc.Iterable[builtins.tuple[builtins.str,builtins.str]]|None = None,
        timeout: builtins.float|unset.UnsetType|None = unset.Unset,
        credentials: grpc.CallCredentials | None = None,
        compression: grpc.Compression | None = None,
        retries: builtins.int | None = 3,
        per_retry_timeout: builtins.float|unset.UnsetType|None = unset.Unset,
    ) -> request_1.Request["CreateClusterRequest","operation.Operation[v1_1.Operation]"]:
        """
        Creates a cluster.
        """
        
        return super().request(
            method="Create",
            request=request,
            result_pb2_class=operation_pb2.Operation,
            metadata=metadata,
            timeout=timeout,
            credentials=credentials,
            compression=compression,
            retries=retries,
            per_retry_timeout=per_retry_timeout,
            result_wrapper=operation.Operation,
        )
    
    def update(self,
        request: "UpdateClusterRequest",
        metadata: abc.Iterable[builtins.tuple[builtins.str,builtins.str]]|None = None,
        timeout: builtins.float|unset.UnsetType|None = unset.Unset,
        credentials: grpc.CallCredentials | None = None,
        compression: grpc.Compression | None = None,
        retries: builtins.int | None = 3,
        per_retry_timeout: builtins.float|unset.UnsetType|None = unset.Unset,
    ) -> request_1.Request["UpdateClusterRequest","operation.Operation[v1_1.Operation]"]:
        """
        Updates a cluster.
        """
        
        metadata = fieldmask_protobuf.ensure_reset_mask_in_metadata(request, metadata)
        return super().request(
            method="Update",
            request=request,
            result_pb2_class=operation_pb2.Operation,
            metadata=metadata,
            timeout=timeout,
            credentials=credentials,
            compression=compression,
            retries=retries,
            per_retry_timeout=per_retry_timeout,
            result_wrapper=operation.Operation,
        )
    
    def delete(self,
        request: "DeleteClusterRequest",
        metadata: abc.Iterable[builtins.tuple[builtins.str,builtins.str]]|None = None,
        timeout: builtins.float|unset.UnsetType|None = unset.Unset,
        credentials: grpc.CallCredentials | None = None,
        compression: grpc.Compression | None = None,
        retries: builtins.int | None = 3,
        per_retry_timeout: builtins.float|unset.UnsetType|None = unset.Unset,
    ) -> request_1.Request["DeleteClusterRequest","operation.Operation[v1_1.Operation]"]:
        """
        Delete a cluster.
        """
        
        return super().request(
            method="Delete",
            request=request,
            result_pb2_class=operation_pb2.Operation,
            metadata=metadata,
            timeout=timeout,
            credentials=credentials,
            compression=compression,
            retries=retries,
            per_retry_timeout=per_retry_timeout,
            result_wrapper=operation.Operation,
        )
    

# file: nebius/msp/spark/v1alpha1/common.proto
class PythonConfig(pb_classes.Message):
    __PB2_CLASS__ = common_pb2.PythonConfig
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.PythonConfig",common_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        requirements: "abc.Iterable[builtins.str]|None|unset.UnsetType" = unset.Unset,
        file_uris: "abc.Iterable[builtins.str]|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(requirements, unset.UnsetType):
            self.requirements = requirements
        if not isinstance(file_uris, unset.UnsetType):
            self.file_uris = file_uris
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "requirements",
            "file_uris",
        ]
    
    @builtins.property
    def requirements(self) -> "abc.MutableSequence[builtins.str]":
        """
        Python requirements
        """
        
        return super()._get_field("requirements", explicit_presence=False,
        wrap=pb_classes.Repeated,
        )
    @requirements.setter
    def requirements(self, value: "abc.Iterable[builtins.str]|None") -> None:
        return super()._set_field("requirements",value,explicit_presence=False,
        )
    
    @builtins.property
    def file_uris(self) -> "abc.MutableSequence[builtins.str]":
        """
        S3 URIs of files to be placed in PYTHONPATH of driver and executors for python applications (.py, .zip, .egg)
        """
        
        return super()._get_field("file_uris", explicit_presence=False,
        wrap=pb_classes.Repeated,
        )
    @file_uris.setter
    def file_uris(self, value: "abc.Iterable[builtins.str]|None") -> None:
        return super()._set_field("file_uris",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "requirements":"requirements",
        "file_uris":"file_uris",
    }
    
class JavaConfig(pb_classes.Message):
    __PB2_CLASS__ = common_pb2.JavaConfig
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.JavaConfig",common_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        entrypoint_class: "builtins.str|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(entrypoint_class, unset.UnsetType):
            self.entrypoint_class = entrypoint_class
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "entrypoint_class",
        ]
    
    @builtins.property
    def entrypoint_class(self) -> "builtins.str":
        """
        Entrypoint class for Java application
        """
        
        return super()._get_field("entrypoint_class", explicit_presence=False,
        )
    @entrypoint_class.setter
    def entrypoint_class(self, value: "builtins.str|None") -> None:
        return super()._set_field("entrypoint_class",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "entrypoint_class":"entrypoint_class",
    }
    
# file: nebius/msp/spark/v1alpha1/preset.proto
class DriverTemplateSpec(pb_classes.Message):
    __PB2_CLASS__ = preset_pb2.DriverTemplateSpec
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.DriverTemplateSpec",preset_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        disk: "resource_1.DiskSpec|template_pb2.DiskSpec|None|unset.UnsetType" = unset.Unset,
        resources: "resource_1.ResourcesSpec|template_pb2.ResourcesSpec|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(disk, unset.UnsetType):
            self.disk = disk
        if not isinstance(resources, unset.UnsetType):
            self.resources = resources
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "disk",
            "resources",
        ]
    
    @builtins.property
    def disk(self) -> "resource_1.DiskSpec":
        return super()._get_field("disk", explicit_presence=False,
        wrap=resource_1.DiskSpec,
        )
    @disk.setter
    def disk(self, value: "resource_1.DiskSpec|template_pb2.DiskSpec|None") -> None:
        return super()._set_field("disk",value,explicit_presence=False,
        )
    
    @builtins.property
    def resources(self) -> "resource_1.ResourcesSpec":
        return super()._get_field("resources", explicit_presence=False,
        wrap=resource_1.ResourcesSpec,
        )
    @resources.setter
    def resources(self, value: "resource_1.ResourcesSpec|template_pb2.ResourcesSpec|None") -> None:
        return super()._set_field("resources",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "disk":"disk",
        "resources":"resources",
    }
    
class DynamicAllocationSpec(pb_classes.Message):
    __PB2_CLASS__ = preset_pb2.DynamicAllocationSpec
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.DynamicAllocationSpec",preset_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        min: "builtins.int|None|unset.UnsetType" = unset.Unset,
        max: "builtins.int|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(min, unset.UnsetType):
            self.min = min
        if not isinstance(max, unset.UnsetType):
            self.max = max
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "min",
            "max",
        ]
    
    @builtins.property
    def min(self) -> "builtins.int":
        return super()._get_field("min", explicit_presence=False,
        )
    @min.setter
    def min(self, value: "builtins.int|None") -> None:
        return super()._set_field("min",value,explicit_presence=False,
        )
    
    @builtins.property
    def max(self) -> "builtins.int":
        return super()._get_field("max", explicit_presence=False,
        )
    @max.setter
    def max(self, value: "builtins.int|None") -> None:
        return super()._set_field("max",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "min":"min",
        "max":"max",
    }
    
class ExecutorTemplateSpec(pb_classes.Message):
    __PB2_CLASS__ = preset_pb2.ExecutorTemplateSpec
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.ExecutorTemplateSpec",preset_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    class __OneOfClass_hosts_spec__(pb_classes.OneOf):
        name: builtins.str= "hosts_spec"
        
        def __init__(self, msg: "ExecutorTemplateSpec") -> None:
            super().__init__()
            self._message: "ExecutorTemplateSpec" = msg
    
    class __OneOfClass_hosts_spec_hosts__(__OneOfClass_hosts_spec__):
        field: typing.Literal["hosts"] = "hosts"
        
        def __init__(self, msg: "ExecutorTemplateSpec") -> None:
            super().__init__(msg)
        @builtins.property
        def value(self) -> "resource_1.HostSpec":
            return self._message.hosts
    
    class __OneOfClass_hosts_spec_hosts_dynamic_allocation__(__OneOfClass_hosts_spec__):
        field: typing.Literal["hosts_dynamic_allocation"] = "hosts_dynamic_allocation"
        
        def __init__(self, msg: "ExecutorTemplateSpec") -> None:
            super().__init__(msg)
        @builtins.property
        def value(self) -> "DynamicAllocationSpec":
            return self._message.hosts_dynamic_allocation
    
    @builtins.property
    def hosts_spec(self) -> __OneOfClass_hosts_spec_hosts__|__OneOfClass_hosts_spec_hosts_dynamic_allocation__|None:
        field_name_1: str|None = super().which_field_in_oneof("hosts_spec")
        match field_name_1:
            case "hosts":
                return self.__OneOfClass_hosts_spec_hosts__(self)
            case "hosts_dynamic_allocation":
                return self.__OneOfClass_hosts_spec_hosts_dynamic_allocation__(self)
            case None:
                return None
            case _:
                raise pb_classes.OneOfMatchError(field_name_1)
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        disk: "resource_1.DiskSpec|template_pb2.DiskSpec|None|unset.UnsetType" = unset.Unset,
        resources: "resource_1.ResourcesSpec|template_pb2.ResourcesSpec|None|unset.UnsetType" = unset.Unset,
        hosts: "resource_1.HostSpec|template_pb2.HostSpec|None|unset.UnsetType" = unset.Unset,
        hosts_dynamic_allocation: "DynamicAllocationSpec|preset_pb2.DynamicAllocationSpec|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(disk, unset.UnsetType):
            self.disk = disk
        if not isinstance(resources, unset.UnsetType):
            self.resources = resources
        if not isinstance(hosts, unset.UnsetType):
            self.hosts = hosts
        if not isinstance(hosts_dynamic_allocation, unset.UnsetType):
            self.hosts_dynamic_allocation = hosts_dynamic_allocation
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "disk",
            "resources",
            "hosts",
            "hosts_dynamic_allocation",
            "hosts_spec",
        ]
    
    @builtins.property
    def disk(self) -> "resource_1.DiskSpec":
        return super()._get_field("disk", explicit_presence=False,
        wrap=resource_1.DiskSpec,
        )
    @disk.setter
    def disk(self, value: "resource_1.DiskSpec|template_pb2.DiskSpec|None") -> None:
        return super()._set_field("disk",value,explicit_presence=False,
        )
    
    @builtins.property
    def resources(self) -> "resource_1.ResourcesSpec":
        return super()._get_field("resources", explicit_presence=False,
        wrap=resource_1.ResourcesSpec,
        )
    @resources.setter
    def resources(self, value: "resource_1.ResourcesSpec|template_pb2.ResourcesSpec|None") -> None:
        return super()._set_field("resources",value,explicit_presence=False,
        )
    
    @builtins.property
    def hosts(self) -> "resource_1.HostSpec|None":
        return super()._get_field("hosts", explicit_presence=True,
        wrap=resource_1.HostSpec,
        )
    @hosts.setter
    def hosts(self, value: "resource_1.HostSpec|template_pb2.HostSpec|None") -> None:
        return super()._set_field("hosts",value,explicit_presence=True,
        )
    
    @builtins.property
    def hosts_dynamic_allocation(self) -> "DynamicAllocationSpec|None":
        return super()._get_field("hosts_dynamic_allocation", explicit_presence=True,
        wrap=DynamicAllocationSpec,
        )
    @hosts_dynamic_allocation.setter
    def hosts_dynamic_allocation(self, value: "DynamicAllocationSpec|preset_pb2.DynamicAllocationSpec|None") -> None:
        return super()._set_field("hosts_dynamic_allocation",value,explicit_presence=True,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "disk":"disk",
        "resources":"resources",
        "hosts":"hosts",
        "hosts_dynamic_allocation":"hosts_dynamic_allocation",
        "hosts_spec":"hosts_spec",
    }
    
# file: nebius/msp/spark/v1alpha1/job.proto
class JobResultCode(pb_enum.Enum):
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.EnumDescriptor](".nebius.msp.spark.v1alpha1.JobResultCode",job_pb2.DESCRIPTOR,descriptor_1.EnumDescriptor)
    JOB_RESULT_CODE_UNSPECIFIED = 0
    SUCCEEDED = 1
    ERROR = 2

class Job(pb_classes.Message):
    __PB2_CLASS__ = job_pb2.Job
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.Job",job_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        metadata: "v1_1.ResourceMetadata|metadata_pb2.ResourceMetadata|None|unset.UnsetType" = unset.Unset,
        spec: "JobSpec|job_pb2.JobSpec|None|unset.UnsetType" = unset.Unset,
        status: "JobStatus|job_pb2.JobStatus|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(metadata, unset.UnsetType):
            self.metadata = metadata
        if not isinstance(spec, unset.UnsetType):
            self.spec = spec
        if not isinstance(status, unset.UnsetType):
            self.status = status
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "metadata",
            "spec",
            "status",
        ]
    
    @builtins.property
    def metadata(self) -> "v1_1.ResourceMetadata":
        return super()._get_field("metadata", explicit_presence=False,
        wrap=v1_1.ResourceMetadata,
        )
    @metadata.setter
    def metadata(self, value: "v1_1.ResourceMetadata|metadata_pb2.ResourceMetadata|None") -> None:
        return super()._set_field("metadata",value,explicit_presence=False,
        )
    
    @builtins.property
    def spec(self) -> "JobSpec":
        return super()._get_field("spec", explicit_presence=False,
        wrap=JobSpec,
        )
    @spec.setter
    def spec(self, value: "JobSpec|job_pb2.JobSpec|None") -> None:
        return super()._set_field("spec",value,explicit_presence=False,
        )
    
    @builtins.property
    def status(self) -> "JobStatus":
        return super()._get_field("status", explicit_presence=False,
        wrap=JobStatus,
        )
    @status.setter
    def status(self, value: "JobStatus|job_pb2.JobStatus|None") -> None:
        return super()._set_field("status",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "metadata":"metadata",
        "spec":"spec",
        "status":"status",
    }
    
class JobSpec(pb_classes.Message):
    """
    Spark Job specification
    """
    
    __PB2_CLASS__ = job_pb2.JobSpec
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.JobSpec",job_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    class SparkConfEntry(pb_classes.Message):
        __PB2_CLASS__ = job_pb2.JobSpec.SparkConfEntry
        __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.JobSpec.SparkConfEntry",job_pb2.DESCRIPTOR,descriptor_1.Descriptor)
        __mask_functions__ = {
        }
        
        def __init__(
            self,
            initial_message: message_1.Message|None = None,
            *,
            key: "builtins.str|None|unset.UnsetType" = unset.Unset,
            value: "builtins.str|None|unset.UnsetType" = unset.Unset,
        ) -> None:
            super().__init__(initial_message)
            if not isinstance(key, unset.UnsetType):
                self.key = key
            if not isinstance(value, unset.UnsetType):
                self.value = value
        
        def __dir__(self) ->abc.Iterable[builtins.str]:
            return [
                "key",
                "value",
            ]
        
        @builtins.property
        def key(self) -> "builtins.str":
            return super()._get_field("key", explicit_presence=False,
            )
        @key.setter
        def key(self, value: "builtins.str|None") -> None:
            return super()._set_field("key",value,explicit_presence=False,
            )
        
        @builtins.property
        def value(self) -> "builtins.str":
            return super()._get_field("value", explicit_presence=False,
            )
        @value.setter
        def value(self, value: "builtins.str|None") -> None:
            return super()._set_field("value",value,explicit_presence=False,
            )
        
        __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
            "key":"key",
            "value":"value",
        }
        
    
    class __OneOfClass_runtime_config__(pb_classes.OneOf):
        name: builtins.str= "runtime_config"
        
        def __init__(self, msg: "JobSpec") -> None:
            super().__init__()
            self._message: "JobSpec" = msg
    
    class __OneOfClass_runtime_config_python__(__OneOfClass_runtime_config__):
        field: typing.Literal["python"] = "python"
        
        def __init__(self, msg: "JobSpec") -> None:
            super().__init__(msg)
        @builtins.property
        def value(self) -> "PythonConfig":
            return self._message.python
    
    class __OneOfClass_runtime_config_java__(__OneOfClass_runtime_config__):
        field: typing.Literal["java"] = "java"
        
        def __init__(self, msg: "JobSpec") -> None:
            super().__init__(msg)
        @builtins.property
        def value(self) -> "JavaConfig":
            return self._message.java
    
    @builtins.property
    def runtime_config(self) -> __OneOfClass_runtime_config_python__|__OneOfClass_runtime_config_java__|None:
        """
        Runtime-specific job config
        """
        
        field_name_1: str|None = super().which_field_in_oneof("runtime_config")
        match field_name_1:
            case "python":
                return self.__OneOfClass_runtime_config_python__(self)
            case "java":
                return self.__OneOfClass_runtime_config_java__(self)
            case None:
                return None
            case _:
                raise pb_classes.OneOfMatchError(field_name_1)
    
    class __OneOfClass__description__(pb_classes.OneOf):
        name: builtins.str= "_description"
        
        def __init__(self, msg: "JobSpec") -> None:
            super().__init__()
            self._message: "JobSpec" = msg
    
    class __OneOfClass__description_description__(__OneOfClass__description__):
        field: typing.Literal["description"] = "description"
        
        def __init__(self, msg: "JobSpec") -> None:
            super().__init__(msg)
        @builtins.property
        def value(self) -> "builtins.str":
            return self._message.description
    
    @builtins.property
    def _description(self) -> __OneOfClass__description_description__|None:
        field_name_1: str|None = super().which_field_in_oneof("_description")
        match field_name_1:
            case "description":
                return self.__OneOfClass__description_description__(self)
            case None:
                return None
            case _:
                raise pb_classes.OneOfMatchError(field_name_1)
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        description: "builtins.str|None|unset.UnsetType" = unset.Unset,
        application_file_uri: "builtins.str|None|unset.UnsetType" = unset.Unset,
        driver: "DriverTemplateSpec|preset_pb2.DriverTemplateSpec|None|unset.UnsetType" = unset.Unset,
        executor: "ExecutorTemplateSpec|preset_pb2.ExecutorTemplateSpec|None|unset.UnsetType" = unset.Unset,
        spark_version: "builtins.str|None|unset.UnsetType" = unset.Unset,
        application_args: "abc.Iterable[builtins.str]|None|unset.UnsetType" = unset.Unset,
        file_uris: "abc.Iterable[builtins.str]|None|unset.UnsetType" = unset.Unset,
        jar_uris: "abc.Iterable[builtins.str]|None|unset.UnsetType" = unset.Unset,
        packages: "abc.Iterable[builtins.str]|None|unset.UnsetType" = unset.Unset,
        spark_conf: "abc.Mapping[builtins.str,builtins.str]|None|unset.UnsetType" = unset.Unset,
        python: "PythonConfig|common_pb2.PythonConfig|None|unset.UnsetType" = unset.Unset,
        java: "JavaConfig|common_pb2.JavaConfig|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(description, unset.UnsetType):
            self.description = description
        if not isinstance(application_file_uri, unset.UnsetType):
            self.application_file_uri = application_file_uri
        if not isinstance(driver, unset.UnsetType):
            self.driver = driver
        if not isinstance(executor, unset.UnsetType):
            self.executor = executor
        if not isinstance(spark_version, unset.UnsetType):
            self.spark_version = spark_version
        if not isinstance(application_args, unset.UnsetType):
            self.application_args = application_args
        if not isinstance(file_uris, unset.UnsetType):
            self.file_uris = file_uris
        if not isinstance(jar_uris, unset.UnsetType):
            self.jar_uris = jar_uris
        if not isinstance(packages, unset.UnsetType):
            self.packages = packages
        if not isinstance(spark_conf, unset.UnsetType):
            self.spark_conf = spark_conf
        if not isinstance(python, unset.UnsetType):
            self.python = python
        if not isinstance(java, unset.UnsetType):
            self.java = java
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "description",
            "application_file_uri",
            "driver",
            "executor",
            "spark_version",
            "application_args",
            "file_uris",
            "jar_uris",
            "packages",
            "spark_conf",
            "python",
            "java",
            "SparkConfEntry",
            "runtime_config",
            "_description",
        ]
    
    @builtins.property
    def description(self) -> "builtins.str|None":
        """
        Description of the job.
        """
        
        return super()._get_field("description", explicit_presence=True,
        )
    @description.setter
    def description(self, value: "builtins.str|None") -> None:
        return super()._set_field("description",value,explicit_presence=True,
        )
    
    @builtins.property
    def application_file_uri(self) -> "builtins.str":
        """
        S3 URI of main application file
        Example: s3a://mybucket/myapp.py
        """
        
        return super()._get_field("application_file_uri", explicit_presence=False,
        )
    @application_file_uri.setter
    def application_file_uri(self, value: "builtins.str|None") -> None:
        return super()._set_field("application_file_uri",value,explicit_presence=False,
        )
    
    @builtins.property
    def driver(self) -> "DriverTemplateSpec":
        return super()._get_field("driver", explicit_presence=False,
        wrap=DriverTemplateSpec,
        )
    @driver.setter
    def driver(self, value: "DriverTemplateSpec|preset_pb2.DriverTemplateSpec|None") -> None:
        return super()._set_field("driver",value,explicit_presence=False,
        )
    
    @builtins.property
    def executor(self) -> "ExecutorTemplateSpec":
        return super()._get_field("executor", explicit_presence=False,
        wrap=ExecutorTemplateSpec,
        )
    @executor.setter
    def executor(self, value: "ExecutorTemplateSpec|preset_pb2.ExecutorTemplateSpec|None") -> None:
        return super()._set_field("executor",value,explicit_presence=False,
        )
    
    @builtins.property
    def spark_version(self) -> "builtins.str":
        return super()._get_field("spark_version", explicit_presence=False,
        )
    @spark_version.setter
    def spark_version(self, value: "builtins.str|None") -> None:
        return super()._set_field("spark_version",value,explicit_presence=False,
        )
    
    @builtins.property
    def application_args(self) -> "abc.MutableSequence[builtins.str]":
        """
        Application args
        """
        
        return super()._get_field("application_args", explicit_presence=False,
        wrap=pb_classes.Repeated,
        )
    @application_args.setter
    def application_args(self, value: "abc.Iterable[builtins.str]|None") -> None:
        return super()._set_field("application_args",value,explicit_presence=False,
        )
    
    @builtins.property
    def file_uris(self) -> "abc.MutableSequence[builtins.str]":
        """
        S3 URIs of files to be placed in executor working directory
        """
        
        return super()._get_field("file_uris", explicit_presence=False,
        wrap=pb_classes.Repeated,
        )
    @file_uris.setter
    def file_uris(self, value: "abc.Iterable[builtins.str]|None") -> None:
        return super()._set_field("file_uris",value,explicit_presence=False,
        )
    
    @builtins.property
    def jar_uris(self) -> "abc.MutableSequence[builtins.str]":
        """
        S3 URIs of Jars to be placed in classpaths of driver and executors for java applications
        """
        
        return super()._get_field("jar_uris", explicit_presence=False,
        wrap=pb_classes.Repeated,
        )
    @jar_uris.setter
    def jar_uris(self, value: "abc.Iterable[builtins.str]|None") -> None:
        return super()._set_field("jar_uris",value,explicit_presence=False,
        )
    
    @builtins.property
    def packages(self) -> "abc.MutableSequence[builtins.str]":
        """
        List of maven coordinates of jars to include on the driver and executor classpaths
        """
        
        return super()._get_field("packages", explicit_presence=False,
        wrap=pb_classes.Repeated,
        )
    @packages.setter
    def packages(self, value: "abc.Iterable[builtins.str]|None") -> None:
        return super()._set_field("packages",value,explicit_presence=False,
        )
    
    @builtins.property
    def spark_conf(self) -> "abc.MutableMapping[builtins.str,builtins.str]":
        """
        Map of spark configuration parameters
        """
        
        return super()._get_field("spark_conf", explicit_presence=False,
        wrap=pb_classes.Map,
        )
    @spark_conf.setter
    def spark_conf(self, value: "abc.Mapping[builtins.str,builtins.str]|None") -> None:
        return super()._set_field("spark_conf",value,explicit_presence=False,
        )
    
    @builtins.property
    def python(self) -> "PythonConfig|None":
        return super()._get_field("python", explicit_presence=True,
        wrap=PythonConfig,
        )
    @python.setter
    def python(self, value: "PythonConfig|common_pb2.PythonConfig|None") -> None:
        return super()._set_field("python",value,explicit_presence=True,
        )
    
    @builtins.property
    def java(self) -> "JavaConfig|None":
        return super()._get_field("java", explicit_presence=True,
        wrap=JavaConfig,
        )
    @java.setter
    def java(self, value: "JavaConfig|common_pb2.JavaConfig|None") -> None:
        return super()._set_field("java",value,explicit_presence=True,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "description":"description",
        "application_file_uri":"application_file_uri",
        "driver":"driver",
        "executor":"executor",
        "spark_version":"spark_version",
        "application_args":"application_args",
        "file_uris":"file_uris",
        "jar_uris":"jar_uris",
        "packages":"packages",
        "spark_conf":"spark_conf",
        "python":"python",
        "java":"java",
        "SparkConfEntry":"SparkConfEntry",
        "runtime_config":"runtime_config",
        "_description":"_description",
    }
    
class JobResultDetails(pb_classes.Message):
    __PB2_CLASS__ = job_pb2.JobResultDetails
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.JobResultDetails",job_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        code: "JobResultCode|job_pb2.JobResultCode|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(code, unset.UnsetType):
            self.code = code
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "code",
        ]
    
    @builtins.property
    def code(self) -> "JobResultCode":
        """
        Result code
        """
        
        return super()._get_field("code", explicit_presence=False,
        wrap=JobResultCode,
        )
    @code.setter
    def code(self, value: "JobResultCode|job_pb2.JobResultCode|None") -> None:
        return super()._set_field("code",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "code":"code",
    }
    
class JobStatus(pb_classes.Message):
    __PB2_CLASS__ = job_pb2.JobStatus
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.JobStatus",job_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    class __OneOfClass__driver_endpoint__(pb_classes.OneOf):
        name: builtins.str= "_driver_endpoint"
        
        def __init__(self, msg: "JobStatus") -> None:
            super().__init__()
            self._message: "JobStatus" = msg
    
    class __OneOfClass__driver_endpoint_driver_endpoint__(__OneOfClass__driver_endpoint__):
        field: typing.Literal["driver_endpoint"] = "driver_endpoint"
        
        def __init__(self, msg: "JobStatus") -> None:
            super().__init__(msg)
        @builtins.property
        def value(self) -> "builtins.str":
            return self._message.driver_endpoint
    
    @builtins.property
    def _driver_endpoint(self) -> __OneOfClass__driver_endpoint_driver_endpoint__|None:
        field_name_1: str|None = super().which_field_in_oneof("_driver_endpoint")
        match field_name_1:
            case "driver_endpoint":
                return self.__OneOfClass__driver_endpoint_driver_endpoint__(self)
            case None:
                return None
            case _:
                raise pb_classes.OneOfMatchError(field_name_1)
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        phase: "v1alpha1_1.ClusterStatus.Phase|cluster_pb2_1.ClusterStatus.Phase|None|unset.UnsetType" = unset.Unset,
        state: "v1alpha1_1.ClusterStatus.State|cluster_pb2_1.ClusterStatus.State|None|unset.UnsetType" = unset.Unset,
        driver_endpoint: "builtins.str|None|unset.UnsetType" = unset.Unset,
        driver_preset_details: "resource_1.PresetDetails|template_pb2.PresetDetails|None|unset.UnsetType" = unset.Unset,
        executor_preset_details: "resource_1.PresetDetails|template_pb2.PresetDetails|None|unset.UnsetType" = unset.Unset,
        result_details: "JobResultDetails|job_pb2.JobResultDetails|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(phase, unset.UnsetType):
            self.phase = phase
        if not isinstance(state, unset.UnsetType):
            self.state = state
        if not isinstance(driver_endpoint, unset.UnsetType):
            self.driver_endpoint = driver_endpoint
        if not isinstance(driver_preset_details, unset.UnsetType):
            self.driver_preset_details = driver_preset_details
        if not isinstance(executor_preset_details, unset.UnsetType):
            self.executor_preset_details = executor_preset_details
        if not isinstance(result_details, unset.UnsetType):
            self.result_details = result_details
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "phase",
            "state",
            "driver_endpoint",
            "driver_preset_details",
            "executor_preset_details",
            "result_details",
            "_driver_endpoint",
        ]
    
    @builtins.property
    def phase(self) -> "v1alpha1_1.ClusterStatus.Phase":
        """
        Current phase (or stage) of the cluster.
        """
        
        return super()._get_field("phase", explicit_presence=False,
        wrap=v1alpha1_1.ClusterStatus.Phase,
        )
    @phase.setter
    def phase(self, value: "v1alpha1_1.ClusterStatus.Phase|cluster_pb2_1.ClusterStatus.Phase|None") -> None:
        return super()._set_field("phase",value,explicit_presence=False,
        )
    
    @builtins.property
    def state(self) -> "v1alpha1_1.ClusterStatus.State":
        """
        State reflects substatus of the stage to define whether it's healthy or not.
        """
        
        return super()._get_field("state", explicit_presence=False,
        wrap=v1alpha1_1.ClusterStatus.State,
        )
    @state.setter
    def state(self, value: "v1alpha1_1.ClusterStatus.State|cluster_pb2_1.ClusterStatus.State|None") -> None:
        return super()._set_field("state",value,explicit_presence=False,
        )
    
    @builtins.property
    def driver_endpoint(self) -> "builtins.str|None":
        """
        Job Driver Web UI endpoint
        """
        
        return super()._get_field("driver_endpoint", explicit_presence=True,
        )
    @driver_endpoint.setter
    def driver_endpoint(self, value: "builtins.str|None") -> None:
        return super()._set_field("driver_endpoint",value,explicit_presence=True,
        )
    
    @builtins.property
    def driver_preset_details(self) -> "resource_1.PresetDetails":
        """
        Job driver resource preset details
        """
        
        return super()._get_field("driver_preset_details", explicit_presence=False,
        wrap=resource_1.PresetDetails,
        )
    @driver_preset_details.setter
    def driver_preset_details(self, value: "resource_1.PresetDetails|template_pb2.PresetDetails|None") -> None:
        return super()._set_field("driver_preset_details",value,explicit_presence=False,
        )
    
    @builtins.property
    def executor_preset_details(self) -> "resource_1.PresetDetails":
        """
        Job executor resource preset details
        """
        
        return super()._get_field("executor_preset_details", explicit_presence=False,
        wrap=resource_1.PresetDetails,
        )
    @executor_preset_details.setter
    def executor_preset_details(self, value: "resource_1.PresetDetails|template_pb2.PresetDetails|None") -> None:
        return super()._set_field("executor_preset_details",value,explicit_presence=False,
        )
    
    @builtins.property
    def result_details(self) -> "JobResultDetails":
        """
        Job execution result details
        """
        
        return super()._get_field("result_details", explicit_presence=False,
        wrap=JobResultDetails,
        )
    @result_details.setter
    def result_details(self, value: "JobResultDetails|job_pb2.JobResultDetails|None") -> None:
        return super()._set_field("result_details",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "phase":"phase",
        "state":"state",
        "driver_endpoint":"driver_endpoint",
        "driver_preset_details":"driver_preset_details",
        "executor_preset_details":"executor_preset_details",
        "result_details":"result_details",
        "_driver_endpoint":"_driver_endpoint",
    }
    
# file: nebius/msp/spark/v1alpha1/job_service.proto
class GetJobRequest(pb_classes.Message):
    __PB2_CLASS__ = job_service_pb2.GetJobRequest
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.GetJobRequest",job_service_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        id: "builtins.str|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(id, unset.UnsetType):
            self.id = id
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "id",
        ]
    
    @builtins.property
    def id(self) -> "builtins.str":
        """
        ID of the job to retrieve.
        """
        
        return super()._get_field("id", explicit_presence=False,
        )
    @id.setter
    def id(self, value: "builtins.str|None") -> None:
        return super()._set_field("id",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "id":"id",
    }
    
class ListJobsRequest(pb_classes.Message):
    __PB2_CLASS__ = job_service_pb2.ListJobsRequest
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.ListJobsRequest",job_service_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        parent_id: "builtins.str|None|unset.UnsetType" = unset.Unset,
        page_size: "builtins.int|None|unset.UnsetType" = unset.Unset,
        page_token: "builtins.str|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(parent_id, unset.UnsetType):
            self.parent_id = parent_id
        if not isinstance(page_size, unset.UnsetType):
            self.page_size = page_size
        if not isinstance(page_token, unset.UnsetType):
            self.page_token = page_token
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "parent_id",
            "page_size",
            "page_token",
        ]
    
    @builtins.property
    def parent_id(self) -> "builtins.str":
        """
        Identifier of IAM container to list jobs from.
        """
        
        return super()._get_field("parent_id", explicit_presence=False,
        )
    @parent_id.setter
    def parent_id(self, value: "builtins.str|None") -> None:
        return super()._set_field("parent_id",value,explicit_presence=False,
        )
    
    @builtins.property
    def page_size(self) -> "builtins.int":
        """
        Specifies the maximum number of items to return in the response. Default value is 100.
        """
        
        return super()._get_field("page_size", explicit_presence=False,
        )
    @page_size.setter
    def page_size(self, value: "builtins.int|None") -> None:
        return super()._set_field("page_size",value,explicit_presence=False,
        )
    
    @builtins.property
    def page_token(self) -> "builtins.str":
        """
        Token for pagination, allowing the retrieval of the next set of results.
        """
        
        return super()._get_field("page_token", explicit_presence=False,
        )
    @page_token.setter
    def page_token(self, value: "builtins.str|None") -> None:
        return super()._set_field("page_token",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "parent_id":"parent_id",
        "page_size":"page_size",
        "page_token":"page_token",
    }
    
class ListJobsResponse(pb_classes.Message):
    __PB2_CLASS__ = job_service_pb2.ListJobsResponse
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.ListJobsResponse",job_service_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    class __OneOfClass__next_page_token__(pb_classes.OneOf):
        name: builtins.str= "_next_page_token"
        
        def __init__(self, msg: "ListJobsResponse") -> None:
            super().__init__()
            self._message: "ListJobsResponse" = msg
    
    class __OneOfClass__next_page_token_next_page_token__(__OneOfClass__next_page_token__):
        field: typing.Literal["next_page_token"] = "next_page_token"
        
        def __init__(self, msg: "ListJobsResponse") -> None:
            super().__init__(msg)
        @builtins.property
        def value(self) -> "builtins.str":
            return self._message.next_page_token
    
    @builtins.property
    def _next_page_token(self) -> __OneOfClass__next_page_token_next_page_token__|None:
        field_name_1: str|None = super().which_field_in_oneof("_next_page_token")
        match field_name_1:
            case "next_page_token":
                return self.__OneOfClass__next_page_token_next_page_token__(self)
            case None:
                return None
            case _:
                raise pb_classes.OneOfMatchError(field_name_1)
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        items: "abc.Iterable[Job]|None|unset.UnsetType" = unset.Unset,
        next_page_token: "builtins.str|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(items, unset.UnsetType):
            self.items = items
        if not isinstance(next_page_token, unset.UnsetType):
            self.next_page_token = next_page_token
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "items",
            "next_page_token",
            "_next_page_token",
        ]
    
    @builtins.property
    def items(self) -> "abc.MutableSequence[Job]":
        """
        List of jobs.
        """
        
        return super()._get_field("items", explicit_presence=False,
        wrap=pb_classes.Repeated.with_wrap(Job,None,None),
        )
    @items.setter
    def items(self, value: "abc.Iterable[Job]|None") -> None:
        return super()._set_field("items",value,explicit_presence=False,
        )
    
    @builtins.property
    def next_page_token(self) -> "builtins.str|None":
        """
        Token for pagination, indicating the next set of results can be retrieved using this token.
        """
        
        return super()._get_field("next_page_token", explicit_presence=True,
        )
    @next_page_token.setter
    def next_page_token(self, value: "builtins.str|None") -> None:
        return super()._set_field("next_page_token",value,explicit_presence=True,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "items":"items",
        "next_page_token":"next_page_token",
        "_next_page_token":"_next_page_token",
    }
    
class CreateJobRequest(pb_classes.Message):
    __PB2_CLASS__ = job_service_pb2.CreateJobRequest
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.CreateJobRequest",job_service_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        metadata: "v1_1.ResourceMetadata|metadata_pb2.ResourceMetadata|None|unset.UnsetType" = unset.Unset,
        spec: "JobSpec|job_pb2.JobSpec|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(metadata, unset.UnsetType):
            self.metadata = metadata
        if not isinstance(spec, unset.UnsetType):
            self.spec = spec
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "metadata",
            "spec",
        ]
    
    @builtins.property
    def metadata(self) -> "v1_1.ResourceMetadata":
        """
        Metadata associated with the new job. Must include parent_id - ID of the cluster to create job in.
        """
        
        return super()._get_field("metadata", explicit_presence=False,
        wrap=v1_1.ResourceMetadata,
        )
    @metadata.setter
    def metadata(self, value: "v1_1.ResourceMetadata|metadata_pb2.ResourceMetadata|None") -> None:
        return super()._set_field("metadata",value,explicit_presence=False,
        )
    
    @builtins.property
    def spec(self) -> "JobSpec":
        """
        Specification for the new job.
        """
        
        return super()._get_field("spec", explicit_presence=False,
        wrap=JobSpec,
        )
    @spec.setter
    def spec(self, value: "JobSpec|job_pb2.JobSpec|None") -> None:
        return super()._set_field("spec",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "metadata":"metadata",
        "spec":"spec",
    }
    
class CancelJobRequest(pb_classes.Message):
    __PB2_CLASS__ = job_service_pb2.CancelJobRequest
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.CancelJobRequest",job_service_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        id: "builtins.str|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(id, unset.UnsetType):
            self.id = id
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "id",
        ]
    
    @builtins.property
    def id(self) -> "builtins.str":
        """
        ID of the job to cancel.
        """
        
        return super()._get_field("id", explicit_presence=False,
        )
    @id.setter
    def id(self, value: "builtins.str|None") -> None:
        return super()._set_field("id",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "id":"id",
    }
    

class JobServiceClient(client.ClientWithOperations[v1_1.Operation,v1_1.OperationServiceClient]):
    """
    Supported until 08/12/25. Nebius AI Cloud no longer supports the service. Instead, use Application for Apache Spark™ Connect in Standalone Applications.
    """
    
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.ServiceDescriptor](".nebius.msp.spark.v1alpha1.JobService",job_service_pb2.DESCRIPTOR,descriptor_1.ServiceDescriptor)
    __service_name__ = ".nebius.msp.spark.v1alpha1.JobService"
    __operation_type__ = v1_1.Operation
    __operation_service_class__ = v1_1.OperationServiceClient
    __operation_source_method__ = "Create"
    __service_deprecation_details__ = (
    """Service .nebius.msp.spark.v1alpha1.JobService is deprecated. Supported until 08/12/25. Nebius AI Cloud no longer supports the service. Instead, use Application for Apache Spark™ Connect in Standalone Applications."""
    )
    
    def get(self,
        request: "GetJobRequest",
        metadata: abc.Iterable[builtins.tuple[builtins.str,builtins.str]]|None = None,
        timeout: builtins.float|unset.UnsetType|None = unset.Unset,
        credentials: grpc.CallCredentials | None = None,
        compression: grpc.Compression | None = None,
        retries: builtins.int | None = 3,
        per_retry_timeout: builtins.float|unset.UnsetType|None = unset.Unset,
    ) -> request_1.Request["GetJobRequest","Job"]:
        """
        Returns the specified job.
        """
        
        return super().request(
            method="Get",
            request=request,
            result_pb2_class=job_pb2.Job,
            metadata=metadata,
            timeout=timeout,
            credentials=credentials,
            compression=compression,
            retries=retries,
            per_retry_timeout=per_retry_timeout,
            result_wrapper=pb_classes.simple_wrapper(Job),
        )
    
    def list(self,
        request: "ListJobsRequest",
        metadata: abc.Iterable[builtins.tuple[builtins.str,builtins.str]]|None = None,
        timeout: builtins.float|unset.UnsetType|None = unset.Unset,
        credentials: grpc.CallCredentials | None = None,
        compression: grpc.Compression | None = None,
        retries: builtins.int | None = 3,
        per_retry_timeout: builtins.float|unset.UnsetType|None = unset.Unset,
    ) -> request_1.Request["ListJobsRequest","ListJobsResponse"]:
        """
        Retrieves a list of jobs.
        """
        
        return super().request(
            method="List",
            request=request,
            result_pb2_class=job_service_pb2.ListJobsResponse,
            metadata=metadata,
            timeout=timeout,
            credentials=credentials,
            compression=compression,
            retries=retries,
            per_retry_timeout=per_retry_timeout,
            result_wrapper=pb_classes.simple_wrapper(ListJobsResponse),
        )
    
    def create(self,
        request: "CreateJobRequest",
        metadata: abc.Iterable[builtins.tuple[builtins.str,builtins.str]]|None = None,
        timeout: builtins.float|unset.UnsetType|None = unset.Unset,
        credentials: grpc.CallCredentials | None = None,
        compression: grpc.Compression | None = None,
        retries: builtins.int | None = 3,
        per_retry_timeout: builtins.float|unset.UnsetType|None = unset.Unset,
    ) -> request_1.Request["CreateJobRequest","operation.Operation[v1_1.Operation]"]:
        """
        Creates a job.
        """
        
        return super().request(
            method="Create",
            request=request,
            result_pb2_class=operation_pb2.Operation,
            metadata=metadata,
            timeout=timeout,
            credentials=credentials,
            compression=compression,
            retries=retries,
            per_retry_timeout=per_retry_timeout,
            result_wrapper=operation.Operation,
        )
    
    def cancel(self,
        request: "CancelJobRequest",
        metadata: abc.Iterable[builtins.tuple[builtins.str,builtins.str]]|None = None,
        timeout: builtins.float|unset.UnsetType|None = unset.Unset,
        credentials: grpc.CallCredentials | None = None,
        compression: grpc.Compression | None = None,
        retries: builtins.int | None = 3,
        per_retry_timeout: builtins.float|unset.UnsetType|None = unset.Unset,
    ) -> request_1.Request["CancelJobRequest","operation.Operation[v1_1.Operation]"]:
        """
        Cancel the job.
        """
        
        return super().request(
            method="Cancel",
            request=request,
            result_pb2_class=operation_pb2.Operation,
            metadata=metadata,
            timeout=timeout,
            credentials=credentials,
            compression=compression,
            retries=retries,
            per_retry_timeout=per_retry_timeout,
            result_wrapper=operation.Operation,
        )
    

# file: nebius/msp/spark/v1alpha1/session.proto
class Session(pb_classes.Message):
    __PB2_CLASS__ = session_pb2.Session
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.Session",session_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        metadata: "v1_1.ResourceMetadata|metadata_pb2.ResourceMetadata|None|unset.UnsetType" = unset.Unset,
        spec: "SessionSpec|session_pb2.SessionSpec|None|unset.UnsetType" = unset.Unset,
        status: "SessionStatus|session_pb2.SessionStatus|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(metadata, unset.UnsetType):
            self.metadata = metadata
        if not isinstance(spec, unset.UnsetType):
            self.spec = spec
        if not isinstance(status, unset.UnsetType):
            self.status = status
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "metadata",
            "spec",
            "status",
        ]
    
    @builtins.property
    def metadata(self) -> "v1_1.ResourceMetadata":
        return super()._get_field("metadata", explicit_presence=False,
        wrap=v1_1.ResourceMetadata,
        )
    @metadata.setter
    def metadata(self, value: "v1_1.ResourceMetadata|metadata_pb2.ResourceMetadata|None") -> None:
        return super()._set_field("metadata",value,explicit_presence=False,
        )
    
    @builtins.property
    def spec(self) -> "SessionSpec":
        return super()._get_field("spec", explicit_presence=False,
        wrap=SessionSpec,
        )
    @spec.setter
    def spec(self, value: "SessionSpec|session_pb2.SessionSpec|None") -> None:
        return super()._set_field("spec",value,explicit_presence=False,
        )
    
    @builtins.property
    def status(self) -> "SessionStatus":
        return super()._get_field("status", explicit_presence=False,
        wrap=SessionStatus,
        )
    @status.setter
    def status(self, value: "SessionStatus|session_pb2.SessionStatus|None") -> None:
        return super()._set_field("status",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "metadata":"metadata",
        "spec":"spec",
        "status":"status",
    }
    
class SessionSpec(pb_classes.Message):
    """
    Spark Session specification
    """
    
    __PB2_CLASS__ = session_pb2.SessionSpec
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.SessionSpec",session_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    class SparkConfEntry(pb_classes.Message):
        __PB2_CLASS__ = session_pb2.SessionSpec.SparkConfEntry
        __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.SessionSpec.SparkConfEntry",session_pb2.DESCRIPTOR,descriptor_1.Descriptor)
        __mask_functions__ = {
        }
        
        def __init__(
            self,
            initial_message: message_1.Message|None = None,
            *,
            key: "builtins.str|None|unset.UnsetType" = unset.Unset,
            value: "builtins.str|None|unset.UnsetType" = unset.Unset,
        ) -> None:
            super().__init__(initial_message)
            if not isinstance(key, unset.UnsetType):
                self.key = key
            if not isinstance(value, unset.UnsetType):
                self.value = value
        
        def __dir__(self) ->abc.Iterable[builtins.str]:
            return [
                "key",
                "value",
            ]
        
        @builtins.property
        def key(self) -> "builtins.str":
            return super()._get_field("key", explicit_presence=False,
            )
        @key.setter
        def key(self, value: "builtins.str|None") -> None:
            return super()._set_field("key",value,explicit_presence=False,
            )
        
        @builtins.property
        def value(self) -> "builtins.str":
            return super()._get_field("value", explicit_presence=False,
            )
        @value.setter
        def value(self, value: "builtins.str|None") -> None:
            return super()._set_field("value",value,explicit_presence=False,
            )
        
        __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
            "key":"key",
            "value":"value",
        }
        
    
    class __OneOfClass__description__(pb_classes.OneOf):
        name: builtins.str= "_description"
        
        def __init__(self, msg: "SessionSpec") -> None:
            super().__init__()
            self._message: "SessionSpec" = msg
    
    class __OneOfClass__description_description__(__OneOfClass__description__):
        field: typing.Literal["description"] = "description"
        
        def __init__(self, msg: "SessionSpec") -> None:
            super().__init__(msg)
        @builtins.property
        def value(self) -> "builtins.str":
            return self._message.description
    
    @builtins.property
    def _description(self) -> __OneOfClass__description_description__|None:
        field_name_1: str|None = super().which_field_in_oneof("_description")
        match field_name_1:
            case "description":
                return self.__OneOfClass__description_description__(self)
            case None:
                return None
            case _:
                raise pb_classes.OneOfMatchError(field_name_1)
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        description: "builtins.str|None|unset.UnsetType" = unset.Unset,
        driver: "DriverTemplateSpec|preset_pb2.DriverTemplateSpec|None|unset.UnsetType" = unset.Unset,
        executor: "ExecutorTemplateSpec|preset_pb2.ExecutorTemplateSpec|None|unset.UnsetType" = unset.Unset,
        spark_version: "builtins.str|None|unset.UnsetType" = unset.Unset,
        file_uris: "abc.Iterable[builtins.str]|None|unset.UnsetType" = unset.Unset,
        jar_uris: "abc.Iterable[builtins.str]|None|unset.UnsetType" = unset.Unset,
        packages: "abc.Iterable[builtins.str]|None|unset.UnsetType" = unset.Unset,
        spark_conf: "abc.Mapping[builtins.str,builtins.str]|None|unset.UnsetType" = unset.Unset,
        python: "PythonConfig|common_pb2.PythonConfig|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(description, unset.UnsetType):
            self.description = description
        if not isinstance(driver, unset.UnsetType):
            self.driver = driver
        if not isinstance(executor, unset.UnsetType):
            self.executor = executor
        if not isinstance(spark_version, unset.UnsetType):
            self.spark_version = spark_version
        if not isinstance(file_uris, unset.UnsetType):
            self.file_uris = file_uris
        if not isinstance(jar_uris, unset.UnsetType):
            self.jar_uris = jar_uris
        if not isinstance(packages, unset.UnsetType):
            self.packages = packages
        if not isinstance(spark_conf, unset.UnsetType):
            self.spark_conf = spark_conf
        if not isinstance(python, unset.UnsetType):
            self.python = python
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "description",
            "driver",
            "executor",
            "spark_version",
            "file_uris",
            "jar_uris",
            "packages",
            "spark_conf",
            "python",
            "SparkConfEntry",
            "_description",
        ]
    
    @builtins.property
    def description(self) -> "builtins.str|None":
        """
        Description of the session.
        """
        
        return super()._get_field("description", explicit_presence=True,
        )
    @description.setter
    def description(self, value: "builtins.str|None") -> None:
        return super()._set_field("description",value,explicit_presence=True,
        )
    
    @builtins.property
    def driver(self) -> "DriverTemplateSpec":
        return super()._get_field("driver", explicit_presence=False,
        wrap=DriverTemplateSpec,
        )
    @driver.setter
    def driver(self, value: "DriverTemplateSpec|preset_pb2.DriverTemplateSpec|None") -> None:
        return super()._set_field("driver",value,explicit_presence=False,
        )
    
    @builtins.property
    def executor(self) -> "ExecutorTemplateSpec":
        return super()._get_field("executor", explicit_presence=False,
        wrap=ExecutorTemplateSpec,
        )
    @executor.setter
    def executor(self, value: "ExecutorTemplateSpec|preset_pb2.ExecutorTemplateSpec|None") -> None:
        return super()._set_field("executor",value,explicit_presence=False,
        )
    
    @builtins.property
    def spark_version(self) -> "builtins.str":
        return super()._get_field("spark_version", explicit_presence=False,
        )
    @spark_version.setter
    def spark_version(self, value: "builtins.str|None") -> None:
        return super()._set_field("spark_version",value,explicit_presence=False,
        )
    
    @builtins.property
    def file_uris(self) -> "abc.MutableSequence[builtins.str]":
        """
        S3 URIs of files to be placed in executor working directory
        """
        
        return super()._get_field("file_uris", explicit_presence=False,
        wrap=pb_classes.Repeated,
        )
    @file_uris.setter
    def file_uris(self, value: "abc.Iterable[builtins.str]|None") -> None:
        return super()._set_field("file_uris",value,explicit_presence=False,
        )
    
    @builtins.property
    def jar_uris(self) -> "abc.MutableSequence[builtins.str]":
        """
        S3 URIs of Jars to be placed in classpaths of driver and executors for java applications
        """
        
        return super()._get_field("jar_uris", explicit_presence=False,
        wrap=pb_classes.Repeated,
        )
    @jar_uris.setter
    def jar_uris(self, value: "abc.Iterable[builtins.str]|None") -> None:
        return super()._set_field("jar_uris",value,explicit_presence=False,
        )
    
    @builtins.property
    def packages(self) -> "abc.MutableSequence[builtins.str]":
        """
        List of maven coordinates of jars to include on the driver and executor classpaths
        """
        
        return super()._get_field("packages", explicit_presence=False,
        wrap=pb_classes.Repeated,
        )
    @packages.setter
    def packages(self, value: "abc.Iterable[builtins.str]|None") -> None:
        return super()._set_field("packages",value,explicit_presence=False,
        )
    
    @builtins.property
    def spark_conf(self) -> "abc.MutableMapping[builtins.str,builtins.str]":
        """
        Map of spark configuration parameters
        """
        
        return super()._get_field("spark_conf", explicit_presence=False,
        wrap=pb_classes.Map,
        )
    @spark_conf.setter
    def spark_conf(self, value: "abc.Mapping[builtins.str,builtins.str]|None") -> None:
        return super()._set_field("spark_conf",value,explicit_presence=False,
        )
    
    @builtins.property
    def python(self) -> "PythonConfig":
        """
        Python runtime-specific session config
        """
        
        return super()._get_field("python", explicit_presence=False,
        wrap=PythonConfig,
        )
    @python.setter
    def python(self, value: "PythonConfig|common_pb2.PythonConfig|None") -> None:
        return super()._set_field("python",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "description":"description",
        "driver":"driver",
        "executor":"executor",
        "spark_version":"spark_version",
        "file_uris":"file_uris",
        "jar_uris":"jar_uris",
        "packages":"packages",
        "spark_conf":"spark_conf",
        "python":"python",
        "SparkConfEntry":"SparkConfEntry",
        "_description":"_description",
    }
    
class SessionStatus(pb_classes.Message):
    __PB2_CLASS__ = session_pb2.SessionStatus
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.SessionStatus",session_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    class __OneOfClass__spark_connect_endpoint__(pb_classes.OneOf):
        name: builtins.str= "_spark_connect_endpoint"
        
        def __init__(self, msg: "SessionStatus") -> None:
            super().__init__()
            self._message: "SessionStatus" = msg
    
    class __OneOfClass__spark_connect_endpoint_spark_connect_endpoint__(__OneOfClass__spark_connect_endpoint__):
        field: typing.Literal["spark_connect_endpoint"] = "spark_connect_endpoint"
        
        def __init__(self, msg: "SessionStatus") -> None:
            super().__init__(msg)
        @builtins.property
        def value(self) -> "builtins.str":
            return self._message.spark_connect_endpoint
    
    @builtins.property
    def _spark_connect_endpoint(self) -> __OneOfClass__spark_connect_endpoint_spark_connect_endpoint__|None:
        field_name_1: str|None = super().which_field_in_oneof("_spark_connect_endpoint")
        match field_name_1:
            case "spark_connect_endpoint":
                return self.__OneOfClass__spark_connect_endpoint_spark_connect_endpoint__(self)
            case None:
                return None
            case _:
                raise pb_classes.OneOfMatchError(field_name_1)
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        phase: "v1alpha1_1.ClusterStatus.Phase|cluster_pb2_1.ClusterStatus.Phase|None|unset.UnsetType" = unset.Unset,
        state: "v1alpha1_1.ClusterStatus.State|cluster_pb2_1.ClusterStatus.State|None|unset.UnsetType" = unset.Unset,
        spark_connect_endpoint: "builtins.str|None|unset.UnsetType" = unset.Unset,
        driver_preset_details: "resource_1.PresetDetails|template_pb2.PresetDetails|None|unset.UnsetType" = unset.Unset,
        executor_preset_details: "resource_1.PresetDetails|template_pb2.PresetDetails|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(phase, unset.UnsetType):
            self.phase = phase
        if not isinstance(state, unset.UnsetType):
            self.state = state
        if not isinstance(spark_connect_endpoint, unset.UnsetType):
            self.spark_connect_endpoint = spark_connect_endpoint
        if not isinstance(driver_preset_details, unset.UnsetType):
            self.driver_preset_details = driver_preset_details
        if not isinstance(executor_preset_details, unset.UnsetType):
            self.executor_preset_details = executor_preset_details
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "phase",
            "state",
            "spark_connect_endpoint",
            "driver_preset_details",
            "executor_preset_details",
            "_spark_connect_endpoint",
        ]
    
    @builtins.property
    def phase(self) -> "v1alpha1_1.ClusterStatus.Phase":
        """
        Current phase (or stage) of the cluster.
        """
        
        return super()._get_field("phase", explicit_presence=False,
        wrap=v1alpha1_1.ClusterStatus.Phase,
        )
    @phase.setter
    def phase(self, value: "v1alpha1_1.ClusterStatus.Phase|cluster_pb2_1.ClusterStatus.Phase|None") -> None:
        return super()._set_field("phase",value,explicit_presence=False,
        )
    
    @builtins.property
    def state(self) -> "v1alpha1_1.ClusterStatus.State":
        """
        State reflects substatus of the stage to define whether it's healthy or not.
        """
        
        return super()._get_field("state", explicit_presence=False,
        wrap=v1alpha1_1.ClusterStatus.State,
        )
    @state.setter
    def state(self, value: "v1alpha1_1.ClusterStatus.State|cluster_pb2_1.ClusterStatus.State|None") -> None:
        return super()._set_field("state",value,explicit_presence=False,
        )
    
    @builtins.property
    def spark_connect_endpoint(self) -> "builtins.str|None":
        """
        Spark Connect endpoint
        """
        
        return super()._get_field("spark_connect_endpoint", explicit_presence=True,
        )
    @spark_connect_endpoint.setter
    def spark_connect_endpoint(self, value: "builtins.str|None") -> None:
        return super()._set_field("spark_connect_endpoint",value,explicit_presence=True,
        )
    
    @builtins.property
    def driver_preset_details(self) -> "resource_1.PresetDetails":
        """
        Session driver resource preset details
        """
        
        return super()._get_field("driver_preset_details", explicit_presence=False,
        wrap=resource_1.PresetDetails,
        )
    @driver_preset_details.setter
    def driver_preset_details(self, value: "resource_1.PresetDetails|template_pb2.PresetDetails|None") -> None:
        return super()._set_field("driver_preset_details",value,explicit_presence=False,
        )
    
    @builtins.property
    def executor_preset_details(self) -> "resource_1.PresetDetails":
        """
        Session executor resource preset details
        """
        
        return super()._get_field("executor_preset_details", explicit_presence=False,
        wrap=resource_1.PresetDetails,
        )
    @executor_preset_details.setter
    def executor_preset_details(self, value: "resource_1.PresetDetails|template_pb2.PresetDetails|None") -> None:
        return super()._set_field("executor_preset_details",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "phase":"phase",
        "state":"state",
        "spark_connect_endpoint":"spark_connect_endpoint",
        "driver_preset_details":"driver_preset_details",
        "executor_preset_details":"executor_preset_details",
        "_spark_connect_endpoint":"_spark_connect_endpoint",
    }
    
# file: nebius/msp/spark/v1alpha1/session_service.proto
class GetSessionRequest(pb_classes.Message):
    __PB2_CLASS__ = session_service_pb2.GetSessionRequest
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.GetSessionRequest",session_service_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        id: "builtins.str|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(id, unset.UnsetType):
            self.id = id
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "id",
        ]
    
    @builtins.property
    def id(self) -> "builtins.str":
        """
        ID of the session to retrieve.
        """
        
        return super()._get_field("id", explicit_presence=False,
        )
    @id.setter
    def id(self, value: "builtins.str|None") -> None:
        return super()._set_field("id",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "id":"id",
    }
    
class GetSessionByNameRequest(pb_classes.Message):
    __PB2_CLASS__ = session_service_pb2.GetSessionByNameRequest
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.GetSessionByNameRequest",session_service_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        parent_id: "builtins.str|None|unset.UnsetType" = unset.Unset,
        name: "builtins.str|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(parent_id, unset.UnsetType):
            self.parent_id = parent_id
        if not isinstance(name, unset.UnsetType):
            self.name = name
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "parent_id",
            "name",
        ]
    
    @builtins.property
    def parent_id(self) -> "builtins.str":
        """
        Parent ID of the session to retrieve.
        """
        
        return super()._get_field("parent_id", explicit_presence=False,
        )
    @parent_id.setter
    def parent_id(self, value: "builtins.str|None") -> None:
        return super()._set_field("parent_id",value,explicit_presence=False,
        )
    
    @builtins.property
    def name(self) -> "builtins.str":
        """
        Name of the session to retrieve.
        """
        
        return super()._get_field("name", explicit_presence=False,
        )
    @name.setter
    def name(self, value: "builtins.str|None") -> None:
        return super()._set_field("name",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "parent_id":"parent_id",
        "name":"name",
    }
    
class ListSessionsRequest(pb_classes.Message):
    __PB2_CLASS__ = session_service_pb2.ListSessionsRequest
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.ListSessionsRequest",session_service_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        parent_id: "builtins.str|None|unset.UnsetType" = unset.Unset,
        page_size: "builtins.int|None|unset.UnsetType" = unset.Unset,
        page_token: "builtins.str|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(parent_id, unset.UnsetType):
            self.parent_id = parent_id
        if not isinstance(page_size, unset.UnsetType):
            self.page_size = page_size
        if not isinstance(page_token, unset.UnsetType):
            self.page_token = page_token
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "parent_id",
            "page_size",
            "page_token",
        ]
    
    @builtins.property
    def parent_id(self) -> "builtins.str":
        """
        Identifier of IAM container to list sessions from.
        """
        
        return super()._get_field("parent_id", explicit_presence=False,
        )
    @parent_id.setter
    def parent_id(self, value: "builtins.str|None") -> None:
        return super()._set_field("parent_id",value,explicit_presence=False,
        )
    
    @builtins.property
    def page_size(self) -> "builtins.int":
        """
        Specifies the maximum number of items to return in the response. Default value is 100.
        """
        
        return super()._get_field("page_size", explicit_presence=False,
        )
    @page_size.setter
    def page_size(self, value: "builtins.int|None") -> None:
        return super()._set_field("page_size",value,explicit_presence=False,
        )
    
    @builtins.property
    def page_token(self) -> "builtins.str":
        """
        Token for pagination, allowing the retrieval of the next set of results.
        """
        
        return super()._get_field("page_token", explicit_presence=False,
        )
    @page_token.setter
    def page_token(self, value: "builtins.str|None") -> None:
        return super()._set_field("page_token",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "parent_id":"parent_id",
        "page_size":"page_size",
        "page_token":"page_token",
    }
    
class ListSessionsResponse(pb_classes.Message):
    __PB2_CLASS__ = session_service_pb2.ListSessionsResponse
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.ListSessionsResponse",session_service_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    class __OneOfClass__next_page_token__(pb_classes.OneOf):
        name: builtins.str= "_next_page_token"
        
        def __init__(self, msg: "ListSessionsResponse") -> None:
            super().__init__()
            self._message: "ListSessionsResponse" = msg
    
    class __OneOfClass__next_page_token_next_page_token__(__OneOfClass__next_page_token__):
        field: typing.Literal["next_page_token"] = "next_page_token"
        
        def __init__(self, msg: "ListSessionsResponse") -> None:
            super().__init__(msg)
        @builtins.property
        def value(self) -> "builtins.str":
            return self._message.next_page_token
    
    @builtins.property
    def _next_page_token(self) -> __OneOfClass__next_page_token_next_page_token__|None:
        field_name_1: str|None = super().which_field_in_oneof("_next_page_token")
        match field_name_1:
            case "next_page_token":
                return self.__OneOfClass__next_page_token_next_page_token__(self)
            case None:
                return None
            case _:
                raise pb_classes.OneOfMatchError(field_name_1)
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        items: "abc.Iterable[Session]|None|unset.UnsetType" = unset.Unset,
        next_page_token: "builtins.str|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(items, unset.UnsetType):
            self.items = items
        if not isinstance(next_page_token, unset.UnsetType):
            self.next_page_token = next_page_token
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "items",
            "next_page_token",
            "_next_page_token",
        ]
    
    @builtins.property
    def items(self) -> "abc.MutableSequence[Session]":
        """
        List of sessions.
        """
        
        return super()._get_field("items", explicit_presence=False,
        wrap=pb_classes.Repeated.with_wrap(Session,None,None),
        )
    @items.setter
    def items(self, value: "abc.Iterable[Session]|None") -> None:
        return super()._set_field("items",value,explicit_presence=False,
        )
    
    @builtins.property
    def next_page_token(self) -> "builtins.str|None":
        """
        Token for pagination, indicating the next set of results can be retrieved using this token.
        """
        
        return super()._get_field("next_page_token", explicit_presence=True,
        )
    @next_page_token.setter
    def next_page_token(self, value: "builtins.str|None") -> None:
        return super()._set_field("next_page_token",value,explicit_presence=True,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "items":"items",
        "next_page_token":"next_page_token",
        "_next_page_token":"_next_page_token",
    }
    
class CreateSessionRequest(pb_classes.Message):
    __PB2_CLASS__ = session_service_pb2.CreateSessionRequest
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.CreateSessionRequest",session_service_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        metadata: "v1_1.ResourceMetadata|metadata_pb2.ResourceMetadata|None|unset.UnsetType" = unset.Unset,
        spec: "SessionSpec|session_pb2.SessionSpec|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(metadata, unset.UnsetType):
            self.metadata = metadata
        if not isinstance(spec, unset.UnsetType):
            self.spec = spec
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "metadata",
            "spec",
        ]
    
    @builtins.property
    def metadata(self) -> "v1_1.ResourceMetadata":
        """
        Metadata associated with the new session. Must include parent_id - ID of the cluster to create session in.
        """
        
        return super()._get_field("metadata", explicit_presence=False,
        wrap=v1_1.ResourceMetadata,
        )
    @metadata.setter
    def metadata(self, value: "v1_1.ResourceMetadata|metadata_pb2.ResourceMetadata|None") -> None:
        return super()._set_field("metadata",value,explicit_presence=False,
        )
    
    @builtins.property
    def spec(self) -> "SessionSpec":
        """
        Specification for the new session.
        """
        
        return super()._get_field("spec", explicit_presence=False,
        wrap=SessionSpec,
        )
    @spec.setter
    def spec(self, value: "SessionSpec|session_pb2.SessionSpec|None") -> None:
        return super()._set_field("spec",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "metadata":"metadata",
        "spec":"spec",
    }
    
class DeleteSessionRequest(pb_classes.Message):
    __PB2_CLASS__ = session_service_pb2.DeleteSessionRequest
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.Descriptor](".nebius.msp.spark.v1alpha1.DeleteSessionRequest",session_service_pb2.DESCRIPTOR,descriptor_1.Descriptor)
    __mask_functions__ = {
    }
    
    def __init__(
        self,
        initial_message: message_1.Message|None = None,
        *,
        id: "builtins.str|None|unset.UnsetType" = unset.Unset,
    ) -> None:
        super().__init__(initial_message)
        if not isinstance(id, unset.UnsetType):
            self.id = id
    
    def __dir__(self) ->abc.Iterable[builtins.str]:
        return [
            "id",
        ]
    
    @builtins.property
    def id(self) -> "builtins.str":
        """
        ID of the session to delete.
        """
        
        return super()._get_field("id", explicit_presence=False,
        )
    @id.setter
    def id(self, value: "builtins.str|None") -> None:
        return super()._set_field("id",value,explicit_presence=False,
        )
    
    __PY_TO_PB2__: builtins.dict[builtins.str,builtins.str] = {
        "id":"id",
    }
    

class SessionServiceClient(client.ClientWithOperations[v1_1.Operation,v1_1.OperationServiceClient]):
    """
    Supported until 08/12/25. Nebius AI Cloud no longer supports the service. Instead, use Application for Apache Spark™ Connect in Standalone Applications.
    """
    
    __PB2_DESCRIPTOR__ = descriptor.DescriptorWrap[descriptor_1.ServiceDescriptor](".nebius.msp.spark.v1alpha1.SessionService",session_service_pb2.DESCRIPTOR,descriptor_1.ServiceDescriptor)
    __service_name__ = ".nebius.msp.spark.v1alpha1.SessionService"
    __operation_type__ = v1_1.Operation
    __operation_service_class__ = v1_1.OperationServiceClient
    __operation_source_method__ = "Create"
    __service_deprecation_details__ = (
    """Service .nebius.msp.spark.v1alpha1.SessionService is deprecated. Supported until 08/12/25. Nebius AI Cloud no longer supports the service. Instead, use Application for Apache Spark™ Connect in Standalone Applications."""
    )
    
    def get(self,
        request: "GetSessionRequest",
        metadata: abc.Iterable[builtins.tuple[builtins.str,builtins.str]]|None = None,
        timeout: builtins.float|unset.UnsetType|None = unset.Unset,
        credentials: grpc.CallCredentials | None = None,
        compression: grpc.Compression | None = None,
        retries: builtins.int | None = 3,
        per_retry_timeout: builtins.float|unset.UnsetType|None = unset.Unset,
    ) -> request_1.Request["GetSessionRequest","Session"]:
        """
        Returns the specified session.
        """
        
        return super().request(
            method="Get",
            request=request,
            result_pb2_class=session_pb2.Session,
            metadata=metadata,
            timeout=timeout,
            credentials=credentials,
            compression=compression,
            retries=retries,
            per_retry_timeout=per_retry_timeout,
            result_wrapper=pb_classes.simple_wrapper(Session),
        )
    
    def get_by_name(self,
        request: "GetSessionByNameRequest",
        metadata: abc.Iterable[builtins.tuple[builtins.str,builtins.str]]|None = None,
        timeout: builtins.float|unset.UnsetType|None = unset.Unset,
        credentials: grpc.CallCredentials | None = None,
        compression: grpc.Compression | None = None,
        retries: builtins.int | None = 3,
        per_retry_timeout: builtins.float|unset.UnsetType|None = unset.Unset,
    ) -> request_1.Request["GetSessionByNameRequest","Session"]:
        """
        Returns the specified session by name.
        """
        
        return super().request(
            method="GetByName",
            request=request,
            result_pb2_class=session_pb2.Session,
            metadata=metadata,
            timeout=timeout,
            credentials=credentials,
            compression=compression,
            retries=retries,
            per_retry_timeout=per_retry_timeout,
            result_wrapper=pb_classes.simple_wrapper(Session),
        )
    
    def list(self,
        request: "ListSessionsRequest",
        metadata: abc.Iterable[builtins.tuple[builtins.str,builtins.str]]|None = None,
        timeout: builtins.float|unset.UnsetType|None = unset.Unset,
        credentials: grpc.CallCredentials | None = None,
        compression: grpc.Compression | None = None,
        retries: builtins.int | None = 3,
        per_retry_timeout: builtins.float|unset.UnsetType|None = unset.Unset,
    ) -> request_1.Request["ListSessionsRequest","ListSessionsResponse"]:
        """
        Retrieves a list of sessions.
        """
        
        return super().request(
            method="List",
            request=request,
            result_pb2_class=session_service_pb2.ListSessionsResponse,
            metadata=metadata,
            timeout=timeout,
            credentials=credentials,
            compression=compression,
            retries=retries,
            per_retry_timeout=per_retry_timeout,
            result_wrapper=pb_classes.simple_wrapper(ListSessionsResponse),
        )
    
    def create(self,
        request: "CreateSessionRequest",
        metadata: abc.Iterable[builtins.tuple[builtins.str,builtins.str]]|None = None,
        timeout: builtins.float|unset.UnsetType|None = unset.Unset,
        credentials: grpc.CallCredentials | None = None,
        compression: grpc.Compression | None = None,
        retries: builtins.int | None = 3,
        per_retry_timeout: builtins.float|unset.UnsetType|None = unset.Unset,
    ) -> request_1.Request["CreateSessionRequest","operation.Operation[v1_1.Operation]"]:
        """
        Creates a session.
        """
        
        return super().request(
            method="Create",
            request=request,
            result_pb2_class=operation_pb2.Operation,
            metadata=metadata,
            timeout=timeout,
            credentials=credentials,
            compression=compression,
            retries=retries,
            per_retry_timeout=per_retry_timeout,
            result_wrapper=operation.Operation,
        )
    
    def delete(self,
        request: "DeleteSessionRequest",
        metadata: abc.Iterable[builtins.tuple[builtins.str,builtins.str]]|None = None,
        timeout: builtins.float|unset.UnsetType|None = unset.Unset,
        credentials: grpc.CallCredentials | None = None,
        compression: grpc.Compression | None = None,
        retries: builtins.int | None = 3,
        per_retry_timeout: builtins.float|unset.UnsetType|None = unset.Unset,
    ) -> request_1.Request["DeleteSessionRequest","operation.Operation[v1_1.Operation]"]:
        """
        Deletes a session.
        """
        
        return super().request(
            method="Delete",
            request=request,
            result_pb2_class=operation_pb2.Operation,
            metadata=metadata,
            timeout=timeout,
            credentials=credentials,
            compression=compression,
            retries=retries,
            per_retry_timeout=per_retry_timeout,
            result_wrapper=operation.Operation,
        )
    

__all__ = [
    #@ local import names here @#
    "Cluster",
    "ClusterSpec",
    "ClusterStatus",
    "Limits",
    "Password",
    "GetClusterRequest",
    "GetClusterByNameRequest",
    "ListClustersRequest",
    "ListClustersResponse",
    "CreateClusterRequest",
    "UpdateClusterRequest",
    "DeleteClusterRequest",
    "ClusterServiceClient",
    "PythonConfig",
    "JavaConfig",
    "DriverTemplateSpec",
    "DynamicAllocationSpec",
    "ExecutorTemplateSpec",
    "JobResultCode",
    "Job",
    "JobSpec",
    "JobResultDetails",
    "JobStatus",
    "GetJobRequest",
    "ListJobsRequest",
    "ListJobsResponse",
    "CreateJobRequest",
    "CancelJobRequest",
    "JobServiceClient",
    "Session",
    "SessionSpec",
    "SessionStatus",
    "GetSessionRequest",
    "GetSessionByNameRequest",
    "ListSessionsRequest",
    "ListSessionsResponse",
    "CreateSessionRequest",
    "DeleteSessionRequest",
    "SessionServiceClient",
]
