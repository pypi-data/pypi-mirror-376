{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optax\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from numpy.polynomial.polynomial import Polynomial\n",
    "\n",
    "from example_models.linear_chain import get_linear_chain_2v\n",
    "from mxlpy import Model, Simulator, fns, npe, plot, scan, surrogates\n",
    "from mxlpy.distributions import LogNormal, Normal, sample\n",
    "from mxlpy.types import AbstractSurrogate, unwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mechanistic Learning\n",
    "\n",
    "Mechanistic learning is the intersection of mechanistic modelling and machine learning.  \n",
    "*mxlpy* currently supports two such approaches: surrogates and neural posterior estimation.  \n",
    "\n",
    "In the following we will mostly use the `mxlpy.surrogates` and `mxlpy.npe` modules to learn about both approaches.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surrogate models\n",
    "\n",
    "\n",
    "**Surrogate models** replace whole parts of a mechanistic model (or even the entire model) with machine learning models.  \n",
    "\n",
    "<img src=\"assets/surrogate.png\" style=\"max-height: 300px;\">\n",
    "\n",
    "This allows combining together multiple models of arbitrary size, without having to worry about the internal state of each model.  \n",
    "They are especially useful for improving the description of *boundary effects*, e.g. a dynamic description of downstream consumption. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual construction\n",
    "\n",
    "Surrogates can have return two kind of values in `mxply`: `derived quantities` and `reactions`.  \n",
    "\n",
    "We will start by defining a polynomial surrogate that will get the value of a variable `x` and output the derived quantity `y`.  \n",
    "Note that due to their nature surrogates can take multiple inputs and return multiple outputs, so we will always use iterables when defining them.  \n",
    "\n",
    "We then also add a derived value `z` that uses the output of our surrogate to see that we are getting the correct output.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model()\n",
    "m.add_variable(\"x\", 1.0)\n",
    "m.add_surrogate(\n",
    "    \"surrogate\",\n",
    "    surrogates.poly.Surrogate(\n",
    "        model=Polynomial(coef=[2]),\n",
    "        args=[\"x\"],\n",
    "        outputs=[\"y\"],\n",
    "    ),\n",
    ")\n",
    "m.add_derived(\"z\", fns.add, args=[\"x\", \"y\"])\n",
    "\n",
    "# Check output\n",
    "m.get_args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we extend that idea to create a reaction.  \n",
    "The only thing we need to change here is to also add the `stoichiometries` of the respective output variable.  \n",
    "\n",
    "I've renamed the output to `v1` here to fit convention, but that is not technically necessary.  \n",
    "`mxlpy` will always infer structurally into what kind of value your surrogate will be translated.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Model()\n",
    "m.add_variable(\"x\", 1.0)\n",
    "m.add_surrogate(\n",
    "    \"surrogate\",\n",
    "    surrogates.poly.Surrogate(\n",
    "        model=Polynomial(coef=[2]),\n",
    "        args=[\"x\"],\n",
    "        outputs=[\"v1\"],\n",
    "        stoichiometries={\"v1\": {\"x\": -1}},\n",
    "    ),\n",
    ")\n",
    "m.add_derived(\"z\", fns.add, args=[\"x\", \"v1\"])\n",
    "\n",
    "# Check output\n",
    "m.get_right_hand_side()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that if you have **multiple outputs**, it is perfectly fine for them to mix between derived values and reactions.  \n",
    "\n",
    "```python\n",
    "Surrogate(\n",
    "    model=...,\n",
    "    args=[\"x\", \"y\"],\n",
    "    outputs=[\"d1\", \"v1\"],               # outputs derived value d1 and rate v1\n",
    "    stoichiometries={\"v1\": {\"x\": -1}},  # only rate v1 is given stoichiometries\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a surrogate from data and using it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a simple linear chain model\n",
    "\n",
    "$$ \\Large \\varnothing \\xrightarrow{v_1} x \\xrightarrow{v_2} y \\xrightarrow{v_3} \\varnothing $$\n",
    "\n",
    "where we want to read out the steady-state rate of $v_3$ dependent on the fixed concentration of $x$, while ignoring the inner state of the model.  \n",
    "\n",
    "\n",
    "$$ \\Large  x \\xrightarrow{} ... \\xrightarrow{v_3}$$\n",
    "\n",
    "Since we need to fix a `variable` as an `parameter`, we can use the `make_variable_static` method to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now \"x\" is a parameter\n",
    "get_linear_chain_2v().make_variable_static(\"x\").parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can already create a function to create a model, which will take our surrogate as an input.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_with_surrogate(surrogate: AbstractSurrogate) -> Model:\n",
    "    model = Model()\n",
    "    model.add_variables({\"x\": 1.0, \"z\": 0.0})\n",
    "\n",
    "    # Adding the surrogate\n",
    "    model.add_surrogate(\n",
    "        \"surrogate\",\n",
    "        surrogate,\n",
    "        args=[\"x\"],\n",
    "        outputs=[\"v2\"],\n",
    "        stoichiometries={\n",
    "            \"v2\": {\"x\": -1, \"z\": 1},\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Note that besides the surrogate we haven't defined any other reaction!\n",
    "    # We could have though\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data\n",
    "\n",
    "The surrogates used in the following will all use the **steady-state** fluxes depending on the inputs.  \n",
    "\n",
    "We can thus create the necessary training data usign `scan.steady_state`.  \n",
    "Since this is usually a large amount of data, we recommend caching the results using `Cache`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_features = pd.DataFrame({\"x\": np.geomspace(1e-12, 2.0, 21)})\n",
    "\n",
    "surrogate_targets = scan.steady_state(\n",
    "    get_linear_chain_2v().make_variable_static(\"x\"),\n",
    "    to_scan=surrogate_features,\n",
    ").fluxes.loc[:, [\"v3\"]]\n",
    "\n",
    "# It's always a good idea to check the inputs and outputs\n",
    "fig, (ax1, ax2) = plot.two_axes(figsize=(6, 3), sharex=False)\n",
    "_ = plot.violins(surrogate_features, ax=ax1)[1].set(\n",
    "    title=\"Features\", ylabel=\"Flux / a.u.\"\n",
    ")\n",
    "_ = plot.violins(surrogate_targets, ax=ax2)[1].set(\n",
    "    title=\"Targets\", ylabel=\"Flux / a.u.\"\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial surrogate\n",
    "\n",
    "We can train our polynomial surrogate using `train_polynomial_surrogate`.  \n",
    "By default this will train polynomials for the degrees `(1, 2, 3, 4, 5, 6, 7)`, but you can change that by using the `degrees` argument.  \n",
    "The function returns the trained surrogate and the training information for the different polynomial degrees.  \n",
    "\n",
    "> **Currently the polynomial surrogates are limited to a single feature and a single target**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate, info = surrogates.poly.train(\n",
    "    surrogate_features[\"x\"],\n",
    "    surrogate_targets[\"v3\"],\n",
    ")\n",
    "\n",
    "print(\"Model\", surrogate.model, end=\"\\n\\n\")\n",
    "print(info[\"score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then insert the surrogate into the model using the function we defined earlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concs, fluxes = unwrap(\n",
    "    Simulator(get_model_with_surrogate(surrogate)).simulate(10).get_result()\n",
    ")\n",
    "\n",
    "fig, (ax1, ax2) = plot.two_axes(figsize=(8, 3))\n",
    "plot.lines(concs, ax=ax1)\n",
    "plot.lines(fluxes, ax=ax2)\n",
    "ax1.set(xlabel=\"time / a.u.\", ylabel=\"concentration / a.u.\")\n",
    "ax2.set(xlabel=\"time / a.u.\", ylabel=\"flux / a.u.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While polynomial regression can model nonlinear relationships between variables, it often struggles when the underlying relationship is more complex than a polynomial function.  \n",
    "You will learn about using neural networks in the next section.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network surrogate using PyTorch\n",
    "\n",
    "Neural networks are designed to capture highly complex and nonlinear relationships.  \n",
    "Through layers of neurons and activation functions, neural networks can learn intricate patterns that are not easily represented by e.g. a polynomial.  \n",
    "They have the flexibility to approximate any continuous function, given sufficient depth and appropriate training.  \n",
    "\n",
    "You can train a neural network surrogate based on the popular [PyTorch](https://pytorch.org/) library using `train_torch_surrogate`.  \n",
    "That function takes the `features`, `targets` and the number of `epochs` as inputs for it's training.  \n",
    "\n",
    "`train_torch_surrogate` returns the trained surrogate, as well as the training `loss`.  \n",
    "It is always a good idea to check whether that training loss approaches 0.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate, loss = surrogates.torch.train(\n",
    "    features=surrogate_features,\n",
    "    targets=surrogate_targets,\n",
    "    batch_size=100,\n",
    "    epochs=250,\n",
    ")\n",
    "\n",
    "ax = loss.plot(ax=plt.subplots(figsize=(4, 2.5))[1])\n",
    "ax.set_ylim(0, None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, you can then insert the surrogate into the model using the function we defined earlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concs, fluxes = unwrap(\n",
    "    Simulator(get_model_with_surrogate(surrogate)).simulate(10).get_result()\n",
    ")\n",
    "\n",
    "fig, (ax1, ax2) = plot.two_axes(figsize=(8, 3))\n",
    "plot.lines(concs, ax=ax1)\n",
    "plot.lines(fluxes, ax=ax2)\n",
    "ax1.set(xlabel=\"time / a.u.\", ylabel=\"concentration / a.u.\")\n",
    "ax2.set(xlabel=\"time / a.u.\", ylabel=\"flux / a.u.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-entrant training\n",
    "\n",
    "Quite often you don't know the amount of epochs you are going to need in order to reach the required loss.  \n",
    "In this case, you can directly use the `TorchSurrogateTrainer` class to continue training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = surrogates.torch.Trainer(\n",
    "    features=surrogate_features,\n",
    "    targets=surrogate_targets,\n",
    ")\n",
    "\n",
    "# First training epochs\n",
    "trainer.train(epochs=100)\n",
    "trainer.get_loss().plot(figsize=(4, 2.5)).set_ylim(0, None)\n",
    "plt.show()\n",
    "\n",
    "# Decide to continue training\n",
    "trainer.train(epochs=150)\n",
    "trainer.get_loss().plot(figsize=(4, 2.5)).set_ylim(0, None)\n",
    "plt.show()\n",
    "\n",
    "surrogate = trainer.get_surrogate(surrogate_outputs=[\"x\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "\n",
    "It often can make sense to check specific predictions of the surrogate.  \n",
    "For example, what does it predict when the inputs are all 0?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(surrogate.predict_raw(np.array([-0.1])))\n",
    "print(surrogate.predict_raw(np.array([0.0])))\n",
    "print(surrogate.predict_raw(np.array([0.1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using keras instead of torch\n",
    "\n",
    "If you installed keras, you can use it with exactly the same interface torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate, loss = surrogates.keras.train(\n",
    "    features=surrogate_features,\n",
    "    targets=surrogate_targets,\n",
    "    batch_size=100,\n",
    "    epochs=250,\n",
    ")\n",
    "\n",
    "ax = loss.plot(ax=plt.subplots(figsize=(4, 2.5))[1])\n",
    "ax.set_ylim(0, None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using equinox instead of torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate, loss = surrogates.equinox.train(\n",
    "    features=surrogate_features,\n",
    "    targets=surrogate_targets,\n",
    "    batch_size=100,\n",
    "    epochs=250,\n",
    "    optimizer=optax.adamw(learning_rate=0.001),\n",
    ")\n",
    "\n",
    "ax = loss.plot(ax=plt.subplots(figsize=(4, 2.5))[1])\n",
    "ax.set_ylim(0, None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural posterior estimation\n",
    "\n",
    "\n",
    "**Neural posterior estimation** answers the question: **what parameters could have generated the data I measured?**  \n",
    "Here you use an ODE model and prior knowledge about the parameters of interest to create *synthetic data*.  \n",
    "You then use the generated synthetic data as the *features* and the input parameters as the *targets* to train an *inverse problem*.  \n",
    "Once that training is successful, the neural network can now predict the input parameters for real world data.  \n",
    "\n",
    "<img src=\"assets/npe.png\" style=\"max-height: 175px;\">\n",
    "\n",
    "You can use this technique for both steady-state as well as time course data.  \n",
    "The only difference is in using `scan.time_course`.  \n",
    "\n",
    "> Take care here to save the targets as well in case you use cached data :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that now the parameters are the targets\n",
    "npe_targets = sample(\n",
    "    {\n",
    "        \"k1\": LogNormal(mean=1.0, sigma=0.3),\n",
    "    },\n",
    "    n=1_000,\n",
    ")\n",
    "\n",
    "# And the generated data are the features\n",
    "npe_features = (\n",
    "    scan.steady_state(\n",
    "        get_linear_chain_2v(),\n",
    "        to_scan=npe_targets,\n",
    "    )\n",
    "    .get_args()\n",
    "    .loc[:, [\"y\", \"v2\", \"v3\"]]\n",
    ")\n",
    "\n",
    "# It's always a good idea to check the inputs and outputs\n",
    "fig, (ax1, ax2) = plot.two_axes(figsize=(6, 3), sharex=False)\n",
    "_ = plot.violins(npe_features, ax=ax1)[1].set(title=\"Features\", ylabel=\"Flux / a.u.\")\n",
    "_ = plot.violins(npe_targets, ax=ax2)[1].set(title=\"Targets\", ylabel=\"Flux / a.u.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NPE\n",
    "\n",
    "You can then train your neural posterior estimator using `npe.train_torch_ss_estimator` (or `npe.train_torch_time_course_estimator` if you have time course data).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator, losses = npe.torch.train_steady_state(\n",
    "    features=npe_features,\n",
    "    targets=npe_targets,\n",
    "    epochs=100,\n",
    "    batch_size=100,\n",
    ")\n",
    "\n",
    "ax = losses.plot(figsize=(4, 2.5))\n",
    "ax.set(xlabel=\"epoch\", ylabel=\"loss\")\n",
    "ax.set_ylim(0, None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check: do prior and posterior match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plot.two_axes(figsize=(6, 2))\n",
    "\n",
    "ax = sns.kdeplot(npe_targets, fill=True, ax=ax1)\n",
    "ax.set_title(\"Prior\")\n",
    "\n",
    "posterior = estimator.predict(npe_features)\n",
    "ax = sns.kdeplot(posterior, fill=True, ax=ax2)\n",
    "ax.set_title(\"Posterior\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-entrant training\n",
    "\n",
    "As with the surrogates you often you don't know the amount of epochs you are going to need in order to reach the required loss.  \n",
    "For the neural posterior estimation you can use the `npe.TorchSteadyStateTrainer` and `npe.TorchTimeCourseTrainer` respectively to continue training.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = npe.torch.SteadyStateTrainer(\n",
    "    features=npe_features,\n",
    "    targets=npe_targets,\n",
    ")\n",
    "\n",
    "# Initial training\n",
    "trainer.train(epochs=20, batch_size=100)\n",
    "trainer.get_loss().plot(figsize=(4, 2.5)).set_ylim(0, None)\n",
    "plt.show()\n",
    "\n",
    "# Continue training\n",
    "trainer.train(epochs=20, batch_size=100)\n",
    "trainer.get_loss().plot(figsize=(4, 2.5)).set_ylim(0, None)\n",
    "plt.show()\n",
    "\n",
    "# Get trainer if loss is deemed suitable\n",
    "estimator = trainer.get_estimator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: #ffffff; background-color: #04AA6D; padding: 3rem 1rem 3rem 1rem; box-sizing: border-box\">\n",
    "    <h2>First finish line</h2>\n",
    "    With that you now know most of what you will need from a day-to-day basis about labelled models in mxlpy.\n",
    "    <br />\n",
    "    <br />\n",
    "    Congratulations!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom loss function\n",
    "\n",
    "You can use a custom loss function by simply injecting a function that takes the predicted tensor `x` and the data `y` and produces another tensor.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TYPE_CHECKING\n",
    "\n",
    "import torch\n",
    "\n",
    "from mxlpy import LinearLabelMapper, Simulator\n",
    "from mxlpy.distributions import sample\n",
    "from mxlpy.fns import michaelis_menten_1s\n",
    "from mxlpy.parallel import parallelise\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from mxlpy.types import AbstractEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_abs(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.mean(torch.abs(x - y))\n",
    "\n",
    "\n",
    "trainer = surrogates.torch.Trainer(\n",
    "    features=surrogate_features,\n",
    "    targets=surrogate_targets,\n",
    "    loss_fn=mean_abs,\n",
    ")\n",
    "\n",
    "trainer = npe.torch.SteadyStateTrainer(\n",
    "    features=npe_features,\n",
    "    targets=npe_targets,\n",
    "    loss_fn=mean_abs,\n",
    ")\n",
    "\n",
    "trainer = npe.torch.TimeCourseTrainer(\n",
    "    features=npe_features,\n",
    "    targets=npe_targets,\n",
    "    loss_fn=mean_abs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label NPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: todo\n",
    "# Show how to change Adam settings or user other optimizers\n",
    "# Show how to change the surrogate network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closed_cycle() -> tuple[Model, dict[str, int], dict[str, list[int]]]:\n",
    "    \"\"\"\n",
    "\n",
    "    | Reaction       | Labelmap |\n",
    "    | -------------- | -------- |\n",
    "    | x1 ->[v1] x2   | [0, 1]   |\n",
    "    | x2 ->[v2a] x3  | [0, 1]   |\n",
    "    | x2 ->[v2b] x3  | [1, 0]   |\n",
    "    | x3 ->[v3] x1   | [0, 1]   |\n",
    "\n",
    "    \"\"\"\n",
    "    model = (\n",
    "        Model()\n",
    "        .add_parameters(\n",
    "            {\n",
    "                \"vmax_1\": 1.0,\n",
    "                \"km_1\": 0.5,\n",
    "                \"vmax_2a\": 1.0,\n",
    "                \"vmax_2b\": 1.0,\n",
    "                \"km_2\": 0.5,\n",
    "                \"vmax_3\": 1.0,\n",
    "                \"km_3\": 0.5,\n",
    "            }\n",
    "        )\n",
    "        .add_variables({\"x1\": 1.0, \"x2\": 0.0, \"x3\": 0.0})\n",
    "        .add_reaction(\n",
    "            \"v1\",\n",
    "            michaelis_menten_1s,\n",
    "            stoichiometry={\"x1\": -1, \"x2\": 1},\n",
    "            args=[\"x1\", \"vmax_1\", \"km_1\"],\n",
    "        )\n",
    "        .add_reaction(\n",
    "            \"v2a\",\n",
    "            michaelis_menten_1s,\n",
    "            stoichiometry={\"x2\": -1, \"x3\": 1},\n",
    "            args=[\"x2\", \"vmax_2a\", \"km_2\"],\n",
    "        )\n",
    "        .add_reaction(\n",
    "            \"v2b\",\n",
    "            michaelis_menten_1s,\n",
    "            stoichiometry={\"x2\": -1, \"x3\": 1},\n",
    "            args=[\"x2\", \"vmax_2b\", \"km_2\"],\n",
    "        )\n",
    "        .add_reaction(\n",
    "            \"v3\",\n",
    "            michaelis_menten_1s,\n",
    "            stoichiometry={\"x3\": -1, \"x1\": 1},\n",
    "            args=[\"x3\", \"vmax_3\", \"km_3\"],\n",
    "        )\n",
    "    )\n",
    "    label_variables: dict[str, int] = {\"x1\": 2, \"x2\": 2, \"x3\": 2}\n",
    "    label_maps: dict[str, list[int]] = {\n",
    "        \"v1\": [0, 1],\n",
    "        \"v2a\": [0, 1],\n",
    "        \"v2b\": [1, 0],\n",
    "        \"v3\": [0, 1],\n",
    "    }\n",
    "    return model, label_variables, label_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _worker(\n",
    "    x: tuple[tuple[int, pd.Series], tuple[int, pd.Series]],\n",
    "    mapper: LinearLabelMapper,\n",
    "    time: float,\n",
    "    initial_labels: dict[str, int | list[int]],\n",
    ") -> pd.Series:\n",
    "    (_, y_ss), (_, v_ss) = x\n",
    "    return unwrap(\n",
    "        Simulator(mapper.build_model(y_ss, v_ss, initial_labels=initial_labels))\n",
    "        .simulate(time)\n",
    "        .get_result()\n",
    "    ).variables.iloc[-1]\n",
    "\n",
    "\n",
    "def get_label_distribution_at_time(\n",
    "    model: Model,\n",
    "    label_variables: dict[str, int],\n",
    "    label_maps: dict[str, list[int]],\n",
    "    time: float,\n",
    "    initial_labels: dict[str, int | list[int]],\n",
    "    ss_concs: pd.DataFrame,\n",
    "    ss_fluxes: pd.DataFrame,\n",
    ") -> pd.DataFrame:\n",
    "    mapper = LinearLabelMapper(\n",
    "        model,\n",
    "        label_variables=label_variables,\n",
    "        label_maps=label_maps,\n",
    "    )\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        dict(\n",
    "            parallelise(\n",
    "                partial(\n",
    "                    _worker, mapper=mapper, time=time, initial_labels=initial_labels\n",
    "                ),\n",
    "                inputs=list(\n",
    "                    enumerate(\n",
    "                        zip(\n",
    "                            ss_concs.iterrows(),\n",
    "                            ss_fluxes.iterrows(),\n",
    "                            strict=True,\n",
    "                        )\n",
    "                    )\n",
    "                ),  # type: ignore\n",
    "                cache=None,\n",
    "            )\n",
    "        ),\n",
    "        dtype=float,\n",
    "    ).T\n",
    "\n",
    "\n",
    "def inverse_parameter_elasticity(\n",
    "    estimator: AbstractEstimator,\n",
    "    datum: pd.Series,\n",
    "    *,\n",
    "    normalized: bool = True,\n",
    "    displacement: float = 1e-4,\n",
    ") -> pd.DataFrame:\n",
    "    ref = estimator.predict(datum).iloc[0, :]\n",
    "\n",
    "    coefs = {}\n",
    "    for name, value in datum.items():\n",
    "        up = coefs[name] = estimator.predict(\n",
    "            pd.Series(datum.to_dict() | {name: value * 1 + displacement})\n",
    "        ).iloc[0, :]\n",
    "        down = coefs[name] = estimator.predict(\n",
    "            pd.Series(datum.to_dict() | {name: value * 1 - displacement})\n",
    "        ).iloc[0, :]\n",
    "        coefs[name] = (up - down) / (2 * displacement * value)\n",
    "\n",
    "    coefs = pd.DataFrame(coefs)\n",
    "    if normalized:\n",
    "        coefs *= datum / ref.to_numpy()\n",
    "\n",
    "    return coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, label_variables, label_maps = get_closed_cycle()\n",
    "\n",
    "ss_concs, ss_fluxes = unwrap(\n",
    "    Simulator(model)\n",
    "    .update_parameters({\"vmax_2a\": 1.0, \"vmax_2b\": 0.5})\n",
    "    .simulate_to_steady_state()\n",
    "    .get_result()\n",
    ")\n",
    "mapper = LinearLabelMapper(\n",
    "    model,\n",
    "    label_variables=label_variables,\n",
    "    label_maps=label_maps,\n",
    ")\n",
    "\n",
    "_, axs = plot.relative_label_distribution(\n",
    "    mapper,\n",
    "    unwrap(\n",
    "        Simulator(\n",
    "            mapper.build_model(\n",
    "                ss_concs.iloc[-1], ss_fluxes.iloc[-1], initial_labels={\"x1\": 0}\n",
    "            )\n",
    "        )\n",
    "        .simulate(5)\n",
    "        .get_result()\n",
    "    ).variables,\n",
    "    sharey=True,\n",
    "    n_cols=3,\n",
    ")\n",
    "\n",
    "axs[0, 0].set_ylabel(\"Relative label distribution\")\n",
    "axs[0, 1].set_xlabel(\"Time / s\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_targets = sample(\n",
    "    {\n",
    "        \"vmax_2b\": Normal(0.5, 0.1),\n",
    "    },\n",
    "    n=1000,\n",
    ").clip(lower=0)\n",
    "\n",
    "ax = sns.kdeplot(surrogate_targets, fill=True)\n",
    "ax.set_title(\"Prior\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_concs, ss_fluxes = scan.steady_state(\n",
    "    model,\n",
    "    to_scan=surrogate_targets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "_, ax = plot.violins(ss_concs, ax=ax1)\n",
    "ax.set_ylabel(\"Concentration / a.u.\")\n",
    "_, ax = plot.violins(ss_fluxes, ax=ax2)\n",
    "ax.set_ylabel(\"Flux / a.u.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_features = get_label_distribution_at_time(\n",
    "    model=model,\n",
    "    label_variables=label_variables,\n",
    "    label_maps=label_maps,\n",
    "    time=5,\n",
    "    ss_concs=ss_concs,\n",
    "    ss_fluxes=ss_fluxes,\n",
    "    initial_labels={\"x1\": 0},\n",
    ")\n",
    "_, ax = plot.violins(surrogate_features)\n",
    "ax.set_ylabel(\"Relative label distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator, losses = npe.torch.train_steady_state(\n",
    "    features=surrogate_features,\n",
    "    targets=surrogate_targets,\n",
    "    batch_size=100,\n",
    "    epochs=250,\n",
    ")\n",
    "\n",
    "ax = losses.plot()\n",
    "ax.set_ylim(0, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(\n",
    "    1,\n",
    "    2,\n",
    "    figsize=(8, 3),\n",
    "    layout=\"constrained\",\n",
    "    sharex=True,\n",
    "    sharey=False,\n",
    ")\n",
    "\n",
    "ax = sns.kdeplot(surrogate_targets, fill=True, ax=ax1)\n",
    "ax.set_title(\"Prior\")\n",
    "\n",
    "posterior = estimator.predict(surrogate_features)\n",
    "\n",
    "ax = sns.kdeplot(posterior, fill=True, ax=ax2)\n",
    "ax.set_title(\"Posterior\")\n",
    "ax2.set_ylim(*ax1.get_ylim())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverse parameter sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot.heatmap(inverse_parameter_elasticity(estimator, surrogate_features.iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elasticities = pd.DataFrame(\n",
    "    {\n",
    "        k: inverse_parameter_elasticity(estimator, i).loc[\"vmax_2b\"]\n",
    "        for k, i in surrogate_features.iterrows()\n",
    "    }\n",
    ").T\n",
    "\n",
    "_ = plot.violins(elasticities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mxlpy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
