Metadata-Version: 2.4
Name: camel-py
Version: 0.1.1
Summary: A lightweight Python client for Ollama with context management and streaming support.
Project-URL: Homepage, https://github.com/Narendrakumar-Suresh/camel-py
Project-URL: Repository, https://github.com/Narendrakumar-Suresh/camel-py
Project-URL: Issues, https://github.com/Narendrakumar-Suresh/camel-py/issues
Author-email: Narendrakumar S <narendrkumarsuresh@gmail.com>
License: MIT
Requires-Python: >=3.12
Requires-Dist: httpx>=0.28.1
Requires-Dist: yaspin>=3.1.0
Description-Content-Type: text/markdown

# ğŸª Camel-ollama

A lightweight Python client for [Ollama](https://ollama.ai/) that makes it easy to:

- Chat with models (supports streaming or one-shot responses).
- Persist and reload conversation context.
- Manage models (list, pull, delete).
- Generate embeddings.

---

## ğŸš€ Installation

```bash
pip install camel-py
```

## Quickstart example

```Python
from camel import CamelClient

with CamelClient(model="llama3") as client:
    # Simple one-shot response
    resp = client.chat("Hello, who are you?")
    print(resp.text)

    # Streaming response
    print("Assistant: ", end="")
    resp = client.stream("Tell me a joke about camels")
    print(resp.text)

```

## ğŸ“‚ Examples

See the [examples/](examples/) directory for more examples.
- [advanced_chat.py](examples/advanced_chat.py) â†’ context persistence across sessions

## ğŸ”§ Features

- Streaming API: real-time token streaming (.stream()).
- Context management: save/load/clear conversation history.
- Model management: list, pull, and delete Ollama models.
- Embeddings: generate embeddings for text.
