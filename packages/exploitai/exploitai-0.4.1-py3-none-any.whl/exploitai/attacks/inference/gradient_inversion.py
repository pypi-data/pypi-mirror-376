import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from typing import Tuple, List, Optional, Dict, Any
import numpy as np


class GradientInversionAttack:
    """
    Gradient Inversion Attack - Advanced reconstruction of training data from gradients
    
    Implements state-of-the-art gradient inversion techniques with:
    - Cosine similarity loss for better gradient matching
    - Multiple optimization strategies (Adam, SGD, LBFGS)
    - Batch reconstruction capabilities
    - Multiple restart strategies for better convergence
    - Advanced regularization techniques
    
    Reference: Geiping et al., "Inverting Gradients - How easy is it to break privacy in federated learning?" (NeurIPS 2020)
    """
    
    def __init__(
        self,
        model: nn.Module,
        loss_fn: Optional[nn.Module] = None,
        device: str = 'cpu',
        cost_fn: str = 'cosine',
        optimizer_name: str = 'adam',
        learning_rate: float = 0.1,
        max_iterations: int = 4800,
        num_restarts: int = 1,
        tv_reg: float = 1e-4,
        l2_reg: float = 0.0,
        bn_reg: float = 0.0,
        group_reg: float = 0.0,
        lr_decay: bool = True,
        use_signed: bool = False,
        boxed: bool = True,
        mean_std: Tuple[float, float] = (0.0, 1.0),
        verbose: bool = True
    ):
        """
        Initialize Gradient Inversion Attack
        
        Args:
            model: Target model to attack
            loss_fn: Loss function used in training (default: CrossEntropyLoss)
            device: Device to run computations ('cpu' or 'cuda')
            cost_fn: Cost function for gradient matching ('cosine', 'l2', 'l1', 'max')
            optimizer_name: Optimizer to use ('adam', 'sgd', 'lbfgs', 'signed')
            learning_rate: Initial learning rate for optimization
            max_iterations: Maximum optimization iterations per restart
            num_restarts: Number of random restarts to try
            tv_reg: Total variation regularization weight
            l2_reg: L2 regularization on reconstructed data
            bn_reg: Batch norm regularization weight
            group_reg: Group consistency regularization for batch reconstruction
            lr_decay: Whether to use learning rate decay
            use_signed: Whether to use signed gradients (sign only, as in invertinggradients)
            boxed: Whether to clamp reconstructed data to valid range
            mean_std: Mean and std for data normalization (dm, ds)
            verbose: Print progress during attack
        """
        self.model = model.to(device)
        self.loss_fn = loss_fn or nn.CrossEntropyLoss(reduction='mean')
        self.device = device
        self.cost_fn = cost_fn.lower()
        self.optimizer_name = optimizer_name.lower()
        self.lr = learning_rate
        self.max_iterations = max_iterations
        self.num_restarts = num_restarts
        self.tv_reg = tv_reg
        self.l2_reg = l2_reg
        self.bn_reg = bn_reg
        self.group_reg = group_reg
        self.lr_decay = lr_decay
        self.use_signed = use_signed
        self.boxed = boxed
        self.mean_std = mean_std
        self.verbose = verbose
        
        self.model.eval()
        
        # Store model statistics for batch norm regularization
        self.bn_layers = []
        self.bn_stats = {}
        self._collect_bn_layers()
    
    def _collect_bn_layers(self):
        """Collect batch normalization layers and their statistics"""
        for module in self.model.modules():
            if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
                self.bn_layers.append(module)
                if module.running_mean is not None:
                    self.bn_stats[module] = {
                        'mean': module.running_mean.clone(),
                        'var': module.running_var.clone()
                    }
    
    def _compute_gradient_loss(
        self,
        dummy_gradients: List[torch.Tensor],
        real_gradients: List[torch.Tensor],
        cost_fn: Optional[str] = None
    ) -> torch.Tensor:
        """
        Compute loss between dummy and real gradients using specified cost function
        
        Args:
            dummy_gradients: Gradients from reconstructed data
            real_gradients: Original gradients to match
            cost_fn: Override the default cost function
            
        Returns:
            Gradient matching loss
        """
        cost_fn = cost_fn or self.cost_fn
        
        if cost_fn == 'cosine':
            # Standard cosine similarity loss (1 - cosine_similarity)
            loss = 0.0
            for dummy_g, real_g in zip(dummy_gradients, real_gradients):
                if len(dummy_g.shape) == 0:  # Skip scalar gradients
                    continue
                dummy_g_flat = dummy_g.view(-1)
                real_g_flat = real_g.view(-1)
                
                # Compute cosine similarity
                cos_sim = F.cosine_similarity(
                    dummy_g_flat.unsqueeze(0),
                    real_g_flat.unsqueeze(0),
                    dim=1
                )
                loss += (1 - cos_sim)
            return loss
            
        elif cost_fn == 'sim':
            # InvertingGradients similarity: 1 - (dot product) / (norm1 * norm2)
            # This is their specific implementation
            costs = 0
            pnorm_dummy = 0
            pnorm_real = 0
            
            for dummy_g, real_g in zip(dummy_gradients, real_gradients):
                # Compute dot product (negative for minimization)
                costs -= (dummy_g * real_g).sum()
                # Accumulate squared norms
                pnorm_dummy += dummy_g.pow(2).sum()
                pnorm_real += real_g.pow(2).sum()
            
            # InvertingGradients formula: 1 + (dot product) / (sqrt(norm1) * sqrt(norm2))
            # Since costs is negative dot product, we use: 1 + costs / ...
            loss = 1 + costs / (pnorm_dummy.sqrt() * pnorm_real.sqrt())
            return loss
            
        elif cost_fn == 'l2':
            # L2 (MSE) loss
            loss = 0.0
            for dummy_g, real_g in zip(dummy_gradients, real_gradients):
                loss += ((dummy_g - real_g) ** 2).sum()
            return loss
            
        elif cost_fn == 'l1':
            # L1 (MAE) loss
            loss = 0.0
            for dummy_g, real_g in zip(dummy_gradients, real_gradients):
                loss += (dummy_g - real_g).abs().sum()
            return loss
            
        elif cost_fn == 'max':
            # Maximum absolute difference
            loss = 0.0
            for dummy_g, real_g in zip(dummy_gradients, real_gradients):
                loss += (dummy_g - real_g).abs().max()
            return loss
            
        else:
            raise ValueError(f"Unknown cost function: {cost_fn}")
    
    def _total_variation(self, x: torch.Tensor) -> torch.Tensor:
        """
        Compute total variation for image regularization
        Encourages spatial smoothness in reconstructed images
        """
        if len(x.shape) != 4:  # Not an image tensor
            return torch.tensor(0.0, device=x.device)
        
        batch_size = x.shape[0]
        tv_h = torch.pow(x[:, :, 1:, :] - x[:, :, :-1, :], 2).sum()
        tv_w = torch.pow(x[:, :, :, 1:] - x[:, :, :, :-1], 2).sum()
        return (tv_h + tv_w) / (batch_size * x.shape[1] * x.shape[2] * x.shape[3])
    
    def _group_consistency_loss(self, x: torch.Tensor) -> torch.Tensor:
        """
        Group consistency regularization for batch reconstruction
        Encourages diversity in batch while maintaining quality
        """
        if x.shape[0] <= 1:
            return torch.tensor(0.0, device=x.device)
        
        # Compute pairwise distances
        x_flat = x.view(x.shape[0], -1)
        distances = torch.cdist(x_flat, x_flat, p=2)
        
        # Penalize too similar reconstructions (encourage diversity)
        similarity_penalty = torch.exp(-distances).sum() - x.shape[0]
        
        return similarity_penalty / (x.shape[0] * (x.shape[0] - 1))
    
    def _bn_regularization(self, dummy_data: torch.Tensor) -> torch.Tensor:
        """
        Batch normalization regularization
        Ensures reconstructed batch statistics match model's running statistics
        """
        if not self.bn_layers or dummy_data.shape[0] <= 1:
            return torch.tensor(0.0, device=self.device)
        
        bn_loss = 0.0
        
        # Forward pass to get intermediate features
        x = dummy_data
        for module in self.model.modules():
            if isinstance(module, (nn.Conv2d, nn.Linear, nn.ReLU, nn.MaxPool2d, nn.AvgPool2d)):
                x = module(x)
            elif isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)) and module in self.bn_stats:
                # Compare batch statistics with running statistics
                batch_mean = x.mean(dim=[0, 2, 3] if len(x.shape) == 4 else [0])
                batch_var = x.var(dim=[0, 2, 3] if len(x.shape) == 4 else [0])
                
                running_mean = self.bn_stats[module]['mean']
                running_var = self.bn_stats[module]['var']
                
                bn_loss += ((batch_mean - running_mean) ** 2).sum()
                bn_loss += ((batch_var - running_var) ** 2).sum()
                
                x = module(x)
        
        return bn_loss
    
    def _create_optimizer(
        self,
        parameters: List[torch.Tensor],
        iteration: int = 0
    ) -> torch.optim.Optimizer:
        """Create optimizer based on configuration"""
        
        if self.optimizer_name == 'adam':
            return optim.Adam(parameters, lr=self.lr, betas=(0.9, 0.999), eps=1e-8)
        elif self.optimizer_name == 'sgd':
            # InvertingGradients uses lr=0.01 for SGD, not the config lr
            return optim.SGD(parameters, lr=0.01, momentum=0.9, nesterov=True)
        elif self.optimizer_name == 'lbfgs':
            return optim.LBFGS(parameters, lr=self.lr, max_iter=20, history_size=100)
        elif self.optimizer_name == 'signed':
            # Signed gradient descent (sign of gradient only)
            return optim.SGD(parameters, lr=self.lr)
        else:
            raise ValueError(f"Unknown optimizer: {self.optimizer_name}")
    
    def _reconstruct_labels(
        self,
        gradients: List[torch.Tensor],
        num_classes: int,
        batch_size: int
    ) -> torch.Tensor:
        """
        Attempt to reconstruct labels from gradient information
        Uses the insight that gradient patterns reveal label information
        """
        # Get gradient of last layer
        last_layer_grad = gradients[-1] if gradients[-1].dim() == 1 else gradients[-2]
        
        if last_layer_grad.shape[0] == num_classes:
            # For cross-entropy, smallest gradient magnitude often indicates true class
            if batch_size == 1:
                predicted_label = torch.argmin(last_layer_grad.abs()).unsqueeze(0)
            else:
                # For batch, this is more complex - use gradient patterns
                # This is a simplified heuristic
                grad_norms = last_layer_grad.abs()
                _, indices = torch.topk(grad_norms, k=batch_size, largest=False)
                predicted_label = indices[:batch_size]
        else:
            # Fallback to random labels
            predicted_label = torch.randint(0, num_classes, (batch_size,), device=self.device)
        
        return predicted_label
    
    def attack(
        self,
        real_gradients: List[torch.Tensor],
        input_shape: Tuple[int, ...],
        num_classes: int,
        labels: Optional[torch.Tensor] = None,
        batch_size: int = 1,
        initialization: str = 'randn',
        return_all: bool = False
    ) -> Dict[str, Any]:
        """
        Execute gradient inversion attack to recover training data
        
        Args:
            real_gradients: List of gradient tensors from real training
            input_shape: Shape of input data to recover (e.g., (3, 32, 32))
            num_classes: Number of output classes
            labels: Known labels (if available)
            batch_size: Number of samples to recover
            initialization: How to initialize dummy data ('randn', 'rand', 'zeros')
            return_all: Return all restart attempts or just the best
        
        Returns:
            Dictionary containing:
                - 'data': Best recovered input data
                - 'labels': Recovered or provided labels
                - 'losses': Loss history during optimization
                - 'scores': Final reconstruction scores
                - 'all_attempts': All reconstruction attempts (if return_all=True)
        """
        
        best_reconstruction = None
        best_loss = float('inf')
        all_attempts = []
        
        # Try to reconstruct labels if not provided
        if labels is None and num_classes is not None:
            labels = self._reconstruct_labels(real_gradients, num_classes, batch_size)
            if self.verbose:
                print(f"Reconstructed labels: {labels.tolist()}")
        elif labels is not None:
            labels = labels.to(self.device)
            if self.verbose:
                print(f"Using provided labels: {labels.tolist()}")
        
        for restart_idx in range(self.num_restarts):
            if self.verbose and self.num_restarts > 1:
                print(f"\n=== Restart {restart_idx + 1}/{self.num_restarts} ===")
            
            # Initialize dummy data
            if initialization == 'randn':
                dummy_data = torch.randn((batch_size, *input_shape), device=self.device, requires_grad=True)
            elif initialization == 'rand':
                dummy_data = torch.rand((batch_size, *input_shape), device=self.device, requires_grad=True)
            elif initialization == 'zeros':
                dummy_data = torch.zeros((batch_size, *input_shape), device=self.device, requires_grad=True)
            else:
                # Default with small random noise
                dummy_data = torch.randn((batch_size, *input_shape), device=self.device) * 0.01
                dummy_data.requires_grad = True
            
            # Optimize labels if not provided
            if labels is None:
                dummy_labels = torch.randn((batch_size, num_classes), device=self.device, requires_grad=True)
                parameters = [dummy_data, dummy_labels]
            else:
                dummy_labels = labels
                parameters = [dummy_data]
            
            losses = []
            
            # Create optimizer once
            optimizer = self._create_optimizer(parameters, 0)
            
            # Setup learning rate scheduler if decay is enabled
            if self.lr_decay:
                # InvertingGradients decay schedule
                milestones = [
                    self.max_iterations // 2.667,
                    self.max_iterations // 1.6,
                    self.max_iterations // 1.142
                ]
                scheduler = torch.optim.lr_scheduler.MultiStepLR(
                    optimizer, milestones=milestones, gamma=0.1
                )
            
            # Optimization loop
            for iteration in range(self.max_iterations):
                
                # Closure for LBFGS
                def closure():
                    optimizer.zero_grad()
                    
                    # Clamp to valid range if boxed
                    if self.boxed:
                        # Use mean_std normalization if provided, else default [0,1]
                        dm, ds = self.mean_std
                        if not isinstance(dm, torch.Tensor):
                            dm = torch.tensor(dm, device=dummy_data.device)
                        if not isinstance(ds, torch.Tensor):
                            ds = torch.tensor(ds, device=dummy_data.device)
                        if dm.dim() == 1:
                            dm = dm.view(-1, 1, 1)
                        if ds.dim() == 1:
                            ds = ds.view(-1, 1, 1)
                        dummy_data_clamped = torch.max(torch.min(dummy_data, (1 - dm) / ds), -dm / ds)
                    else:
                        dummy_data_clamped = dummy_data
                    
                    # Forward pass
                    dummy_output = self.model(dummy_data_clamped)
                    
                    # Compute loss
                    if labels is None:
                        dummy_loss = self.loss_fn(dummy_output, F.softmax(dummy_labels, dim=-1))
                    else:
                        dummy_loss = self.loss_fn(dummy_output, dummy_labels)
                    
                    # Compute dummy gradients
                    dummy_gradients = torch.autograd.grad(
                        dummy_loss,
                        self.model.parameters(),
                        retain_graph=True,
                        create_graph=True
                    )
                    
                    # Gradient matching loss
                    grad_loss = self._compute_gradient_loss(dummy_gradients, real_gradients)
                    
                    # Total loss with regularization
                    total_loss = grad_loss
                    
                    if self.tv_reg > 0:
                        total_loss = total_loss + self.tv_reg * self._total_variation(dummy_data)
                    
                    if self.l2_reg > 0:
                        total_loss = total_loss + self.l2_reg * torch.norm(dummy_data, 2)
                    
                    if self.bn_reg > 0 and self.bn_layers:
                        total_loss = total_loss + self.bn_reg * self._bn_regularization(dummy_data)
                    
                    if self.group_reg > 0 and batch_size > 1:
                        total_loss = total_loss + self.group_reg * self._group_consistency_loss(dummy_data)
                    
                    total_loss.backward()
                    
                    return total_loss
                
                # Optimization step
                if self.optimizer_name == 'lbfgs':
                    loss = optimizer.step(closure)
                else:
                    loss = closure()
                    
                    # Signed gradient descent modification (InvertingGradients style)
                    # Only apply to dummy_data gradient, not labels
                    if self.use_signed and dummy_data.grad is not None:
                        dummy_data.grad.sign_()  # In-place sign operation
                    
                    optimizer.step()
                
                # Step learning rate scheduler
                if self.lr_decay and 'scheduler' in locals():
                    scheduler.step()
                
                losses.append(loss.item())
                
                # Verbose output
                if self.verbose and iteration % 100 == 0:
                    print(f"Iteration {iteration}: Loss = {loss.item():.6f}")
                
                # Early stopping
                if len(losses) > 100:
                    recent_losses = losses[-100:]
                    if np.std(recent_losses) < 1e-6:
                        if self.verbose:
                            print(f"Early stopping at iteration {iteration}")
                        break
                
                # Clamp dummy data to valid range if boxed
                if self.boxed:
                    with torch.no_grad():
                        # InvertingGradients formula: torch.max(torch.min(x, (1 - dm) / ds), -dm / ds)
                        dm, ds = self.mean_std
                        # Convert to tensors if needed
                        if not isinstance(dm, torch.Tensor):
                            dm = torch.tensor(dm, device=dummy_data.device)
                        if not isinstance(ds, torch.Tensor):
                            ds = torch.tensor(ds, device=dummy_data.device)
                        # Ensure correct shape for broadcasting
                        if dm.dim() == 1:
                            dm = dm.view(-1, 1, 1)
                        if ds.dim() == 1:
                            ds = ds.view(-1, 1, 1)
                        dummy_data.data = torch.max(torch.min(dummy_data, (1 - dm) / ds), -dm / ds)
            
            # Store attempt results
            final_labels = dummy_labels.argmax(dim=-1) if labels is None else labels
            
            attempt_result = {
                'data': dummy_data.detach().cpu(),
                'labels': final_labels.detach().cpu(),
                'final_loss': losses[-1] if losses else float('inf'),
                'losses': losses,
                'restart_idx': restart_idx
            }
            
            all_attempts.append(attempt_result)
            
            # Update best reconstruction
            if losses and losses[-1] < best_loss:
                best_loss = losses[-1]
                best_reconstruction = attempt_result
        
        # Prepare return dictionary
        result = {
            'data': best_reconstruction['data'],
            'labels': best_reconstruction['labels'],
            'losses': best_reconstruction['losses'],
            'final_loss': best_loss,
            'success': best_loss < 0.1,
            'num_restarts': self.num_restarts
        }
        
        if return_all:
            result['all_attempts'] = all_attempts
        
        # Compute reconstruction quality metrics
        result['scores'] = self.compute_metrics(None, best_reconstruction['data'])
        
        return result
    
    def compute_metrics(
        self,
        original: Optional[torch.Tensor],
        reconstructed: torch.Tensor
    ) -> Dict[str, float]:
        """
        Compute reconstruction quality metrics
        
        Args:
            original: Original data (if available)
            reconstructed: Reconstructed data
            
        Returns:
            Dictionary of quality metrics
        """
        metrics = {}
        
        # Basic statistics
        metrics['mean'] = reconstructed.mean().item()
        metrics['std'] = reconstructed.std().item()
        metrics['min'] = reconstructed.min().item()
        metrics['max'] = reconstructed.max().item()
        
        if original is not None:
            original = original.to(reconstructed.device)
            
            # MSE
            metrics['mse'] = F.mse_loss(reconstructed, original).item()
            
            # PSNR (Peak Signal-to-Noise Ratio)
            mse = metrics['mse']
            if mse > 0:
                metrics['psnr'] = 20 * np.log10(1.0 / np.sqrt(mse))
            else:
                metrics['psnr'] = float('inf')
            
            # SSIM (Structural Similarity Index) - simplified version
            if len(reconstructed.shape) == 4:  # Image data
                # Simplified SSIM calculation
                mu1 = F.avg_pool2d(original, 3, 1, padding=1)
                mu2 = F.avg_pool2d(reconstructed, 3, 1, padding=1)
                
                mu1_sq = mu1.pow(2)
                mu2_sq = mu2.pow(2)
                mu1_mu2 = mu1 * mu2
                
                sigma1_sq = F.avg_pool2d(original * original, 3, 1, padding=1) - mu1_sq
                sigma2_sq = F.avg_pool2d(reconstructed * reconstructed, 3, 1, padding=1) - mu2_sq
                sigma12 = F.avg_pool2d(original * reconstructed, 3, 1, padding=1) - mu1_mu2
                
                C1 = 0.01 ** 2
                C2 = 0.03 ** 2
                
                ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / \
                          ((mu1_sq + mu2_sq + C1) * (sigma1_sq + sigma2_sq + C2))
                
                metrics['ssim'] = ssim_map.mean().item()
            
            # L1 distance
            metrics['l1_distance'] = F.l1_loss(reconstructed, original).item()
            
            # Cosine similarity
            if reconstructed.numel() > 0:
                cos_sim = F.cosine_similarity(
                    reconstructed.view(-1).unsqueeze(0),
                    original.view(-1).unsqueeze(0)
                )
                metrics['cosine_similarity'] = cos_sim.item()
        
        return metrics