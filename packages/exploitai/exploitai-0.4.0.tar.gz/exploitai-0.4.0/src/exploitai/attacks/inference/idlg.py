import torch
import torch.nn as nn
import torch.optim as optim
from typing import Tuple, List, Optional, Dict, Any


class iDLGAttack:
    """
    Improved Deep Leakage from Gradients (iDLG) Attack
    
    Key improvements over DLG:
    1. Predicts ground truth labels from gradient information
    2. Only optimizes dummy data (not labels)
    3. More efficient and accurate reconstruction
    
    Reference: Zhao et al., "iDLG: Improved Deep Leakage from Gradients" (2020)
    """
    
    def __init__(
        self,
        model: nn.Module,
        loss_fn: Optional[nn.Module] = None,
        device: str = 'cpu',
        learning_rate: float = 1.0,
        max_iterations: int = 300,
        tv_reg: float = 0.0,
        l2_reg: float = 0.0,
        verbose: bool = True
    ):
        """
        Initialize iDLG Attack
        
        Args:
            model: Target model to attack
            loss_fn: Loss function used in training (default: CrossEntropyLoss)
            device: Device to run computations ('cpu' or 'cuda')
            learning_rate: Learning rate for optimization
            max_iterations: Maximum optimization iterations
            tv_reg: Total variation regularization weight
            l2_reg: L2 regularization on dummy data
            verbose: Print progress during attack
        """
        self.model = model.to(device)
        self.loss_fn = loss_fn or nn.CrossEntropyLoss()
        self.device = device
        self.lr = learning_rate
        self.max_iterations = max_iterations
        self.tv_reg = tv_reg
        self.l2_reg = l2_reg
        self.verbose = verbose
        
        self.model.eval()
    
    def _predict_label_from_gradients(self, gradients: List[torch.Tensor]) -> torch.Tensor:
        """
        Predict ground truth label from gradient information
        
        Key insight: For classification tasks with cross-entropy loss,
        the gradient of the last layer contains label information
        
        Args:
            gradients: List of gradient tensors
            
        Returns:
            Predicted label tensor with batch dimension
        """
        # Get gradient of the last layer (bias of final linear layer)
        # This contains the most direct label information
        last_weight_gradient = gradients[-2]  # Weight of last layer
        
        # For cross-entropy loss, the gradient of the last layer bias
        # has a specific pattern that reveals the true label
        if len(gradients) >= 2:
            last_bias_gradient = gradients[-1]  # Bias of last layer
            
            # The true label corresponds to the position with minimum gradient
            # (due to cross-entropy gradient structure)
            predicted_label_idx = torch.argmin(last_bias_gradient, dim=-1)
        else:
            # Fallback: use weight gradients
            predicted_label_idx = torch.argmin(torch.sum(last_weight_gradient, dim=-1), dim=-1)
        
        # Ensure proper shape for batch processing (add batch dimension if needed)
        if predicted_label_idx.dim() == 0:
            predicted_label = predicted_label_idx.unsqueeze(0)
        else:
            predicted_label = predicted_label_idx
        
        return predicted_label
    
    def _total_variation(self, x: torch.Tensor) -> torch.Tensor:
        """Compute total variation for image regularization"""
        if len(x.shape) != 4:
            return torch.tensor(0.0, device=x.device)
        
        batch_size = x.shape[0]
        tv_h = torch.pow(x[:, :, 1:, :] - x[:, :, :-1, :], 2).sum()
        tv_w = torch.pow(x[:, :, :, 1:] - x[:, :, :, :-1], 2).sum()
        return (tv_h + tv_w) / batch_size
    
    def _compute_gradient_loss(
        self,
        dummy_gradients: List[torch.Tensor],
        real_gradients: List[torch.Tensor]
    ) -> torch.Tensor:
        """Compute loss between dummy and real gradients"""
        grad_loss = 0.0
        
        for dummy_g, real_g in zip(dummy_gradients, real_gradients):
            if dummy_g.shape != real_g.shape:
                continue
            grad_loss += ((dummy_g - real_g) ** 2).sum()
        
        return grad_loss
    
    def attack(
        self,
        real_gradients: List[torch.Tensor],
        input_shape: Tuple[int, ...],
        num_classes: int,
        initialization: str = 'random',
        batch_size: int = 1
    ) -> Dict[str, Any]:
        """
        Execute iDLG attack to recover training data from gradients
        
        Args:
            real_gradients: List of gradient tensors from real training
            input_shape: Shape of input data to recover (e.g., (3, 32, 32) for CIFAR)
            num_classes: Number of output classes
            initialization: How to initialize dummy data ('random', 'randn', 'uniform')
            batch_size: Number of samples to recover (default: 1, higher values reduce success rate)
        
        Returns:
            Dictionary containing:
                - 'data': Recovered input data
                - 'predicted_label': Predicted ground truth label
                - 'losses': Loss history during optimization
                - 'success': Whether attack converged
        """
        
        # Step 1: Predict ground truth label from gradients (key iDLG improvement)
        predicted_label = self._predict_label_from_gradients(real_gradients)
        predicted_label = predicted_label.to(self.device)
        
        # Handle batch dimension for predicted labels  
        if batch_size > 1:
            # For batch_size > 1, replicate the predicted label
            # This is a limitation - iDLG's label prediction works best for single samples
            if predicted_label.dim() == 0:
                predicted_label = predicted_label.unsqueeze(0)
            predicted_label = predicted_label.expand(batch_size)
            if self.verbose:
                print(f"Warning: iDLG label prediction is most accurate for batch_size=1")
                print(f"Using same predicted label for all samples: {predicted_label[0].item()}")
        elif self.verbose and batch_size == 1:
            if predicted_label.dim() == 0:
                predicted_label = predicted_label.unsqueeze(0)
            print(f"iDLG predicted label: {predicted_label.item()}")
        
        # Step 2: Initialize dummy data
        if initialization == 'randn':
            dummy_data = torch.randn((batch_size, *input_shape), requires_grad=True, device=self.device)
        elif initialization == 'uniform':
            dummy_data = torch.rand((batch_size, *input_shape), requires_grad=True, device=self.device)
        else:
            dummy_data = torch.randn((batch_size, *input_shape), requires_grad=True, device=self.device)
            dummy_data.data = dummy_data.data * 0.1
        
        # Step 3: Optimize only dummy data (not label!)
        optimizer = optim.LBFGS([dummy_data], lr=self.lr)
        
        losses = []
        reconstruction_history = []
        
        for iteration in range(self.max_iterations):
            def closure():
                optimizer.zero_grad()
                
                # Forward pass with dummy data and predicted label
                dummy_output = self.model(dummy_data)
                dummy_loss = self.loss_fn(dummy_output, predicted_label)
                
                # Compute dummy gradients
                dummy_gradients = torch.autograd.grad(
                    dummy_loss,
                    self.model.parameters(),
                    retain_graph=True,
                    create_graph=True
                )
                
                # Gradient matching loss
                grad_loss = self._compute_gradient_loss(dummy_gradients, real_gradients)
                
                # Total loss with regularization
                total_loss = grad_loss
                
                if self.tv_reg > 0:
                    total_loss += self.tv_reg * self._total_variation(dummy_data)
                
                if self.l2_reg > 0:
                    total_loss += self.l2_reg * torch.norm(dummy_data, 2)
                
                total_loss.backward()
                
                return total_loss
            
            loss = optimizer.step(closure)
            losses.append(loss.item())
            
            # Save reconstruction every 10 iterations
            if iteration % 10 == 0:
                reconstruction_history.append({
                    'iteration': iteration,
                    'data': dummy_data.detach().cpu().clone(),
                    'loss': loss.item()
                })
            
            if self.verbose and iteration % 50 == 0:
                print(f"Iteration {iteration}: Loss = {loss.item():.6f}")
            
            # Early stopping if converged
            if len(losses) > 10 and abs(losses[-1] - losses[-2]) < 1e-7:
                if self.verbose:
                    print(f"Converged at iteration {iteration}")
                break
        
        return {
            'data': dummy_data.detach().cpu(),
            'predicted_label': predicted_label.detach().cpu(),
            'losses': losses,
            'success': losses[-1] < 0.1,
            'iterations': iteration + 1,
            'reconstruction_history': reconstruction_history
        }
    
    def attack_with_known_label(
        self,
        real_gradients: List[torch.Tensor],
        input_shape: Tuple[int, ...],
        true_label: torch.Tensor,
        initialization: str = 'random',
        batch_size: int = 1
    ) -> Dict[str, Any]:
        """
        Execute iDLG attack with known ground truth label (for comparison)
        
        Args:
            real_gradients: List of gradient tensors from real training
            input_shape: Shape of input data to recover
            true_label: Known ground truth label
            initialization: How to initialize dummy data
            batch_size: Number of samples to recover (default: 1)
        
        Returns:
            Dictionary containing attack results
        """
        true_label = true_label.to(self.device)
        
        # Ensure true_label has correct shape for batch
        if true_label.dim() == 0:
            true_label = true_label.unsqueeze(0)
        if true_label.shape[0] == 1 and batch_size > 1:
            true_label = true_label.expand(batch_size)
            
        if self.verbose:
            if batch_size == 1:
                print(f"Using known true label: {true_label[0].item()}")
            else:
                print(f"Using known true labels: {true_label.tolist()}")
        
        # Initialize dummy data
        if initialization == 'randn':
            dummy_data = torch.randn((batch_size, *input_shape), requires_grad=True, device=self.device)
        elif initialization == 'uniform':
            dummy_data = torch.rand((batch_size, *input_shape), requires_grad=True, device=self.device)
        else:
            dummy_data = torch.randn((batch_size, *input_shape), requires_grad=True, device=self.device)
            dummy_data.data = dummy_data.data * 0.1
        
        # Optimize dummy data with known label
        optimizer = optim.LBFGS([dummy_data], lr=self.lr)
        
        losses = []
        reconstruction_history = []
        
        for iteration in range(self.max_iterations):
            def closure():
                optimizer.zero_grad()
                
                # Forward pass with dummy data and true label
                dummy_output = self.model(dummy_data)
                dummy_loss = self.loss_fn(dummy_output, true_label)
                
                # Compute dummy gradients
                dummy_gradients = torch.autograd.grad(
                    dummy_loss,
                    self.model.parameters(),
                    retain_graph=True,
                    create_graph=True
                )
                
                # Gradient matching loss
                grad_loss = self._compute_gradient_loss(dummy_gradients, real_gradients)
                
                # Total loss with regularization
                total_loss = grad_loss
                
                if self.tv_reg > 0:
                    total_loss += self.tv_reg * self._total_variation(dummy_data)
                
                if self.l2_reg > 0:
                    total_loss += self.l2_reg * torch.norm(dummy_data, 2)
                
                total_loss.backward()
                
                return total_loss
            
            loss = optimizer.step(closure)
            losses.append(loss.item())
            
            # Save reconstruction every 10 iterations
            if iteration % 10 == 0:
                reconstruction_history.append({
                    'iteration': iteration,
                    'data': dummy_data.detach().cpu().clone(),
                    'loss': loss.item()
                })
            
            if self.verbose and iteration % 50 == 0:
                print(f"Iteration {iteration}: Loss = {loss.item():.6f}")
            
            # Early stopping if converged
            if len(losses) > 10 and abs(losses[-1] - losses[-2]) < 1e-7:
                if self.verbose:
                    print(f"Converged at iteration {iteration}")
                break
        
        return {
            'data': dummy_data.detach().cpu(),
            'predicted_label': true_label.detach().cpu(),
            'losses': losses,
            'success': losses[-1] < 0.1,
            'iterations': iteration + 1,
            'reconstruction_history': reconstruction_history,
            'used_true_label': True
        }