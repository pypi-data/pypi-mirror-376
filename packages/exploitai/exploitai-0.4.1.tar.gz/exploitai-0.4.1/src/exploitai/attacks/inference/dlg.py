import torch
import torch.nn as nn
import torch.optim as optim
from typing import Tuple, List, Optional, Dict, Any


class DLGAttack:
    """
    Deep Leakage from Gradients (DLG) Attack
    
    Recovers training data from shared gradients by optimizing dummy inputs
    to produce gradients matching the observed ones.
    
    Reference: Zhu et al., "Deep Leakage from Gradients" (NeurIPS 2019)
    """
    
    def __init__(
        self,
        model: nn.Module,
        loss_fn: Optional[nn.Module] = None,
        device: str = 'cpu',
        learning_rate: float = 0.1,
        max_iterations: int = 300,
        gradient_reg: float = 0.0,
        tv_reg: float = 0.0,
        l2_reg: float = 0.0,
        verbose: bool = True
    ):
        """
        Initialize DLG Attack
        
        Args:
            model: Target model to attack
            loss_fn: Loss function used in training (default: CrossEntropyLoss)
            device: Device to run computations ('cpu' or 'cuda')
            learning_rate: Learning rate for optimization
            max_iterations: Maximum optimization iterations
            gradient_reg: Gradient matching regularization weight
            tv_reg: Total variation regularization (for images)
            l2_reg: L2 regularization on dummy data
            verbose: Print progress during attack
        """
        self.model = model.to(device)
        self.loss_fn = loss_fn or nn.CrossEntropyLoss()
        self.device = device
        self.lr = learning_rate
        self.max_iterations = max_iterations
        self.gradient_reg = gradient_reg
        self.tv_reg = tv_reg
        self.l2_reg = l2_reg
        self.verbose = verbose
        
        self.model.eval()
    
    def _total_variation(self, x: torch.Tensor) -> torch.Tensor:
        """Compute total variation for image regularization"""
        if len(x.shape) != 4:
            return torch.tensor(0.0)
        
        batch_size = x.shape[0]
        tv_h = torch.pow(x[:, :, 1:, :] - x[:, :, :-1, :], 2).sum()
        tv_w = torch.pow(x[:, :, :, 1:] - x[:, :, :, :-1], 2).sum()
        return (tv_h + tv_w) / batch_size
    
    def _compute_gradient_loss(
        self,
        dummy_gradients: List[torch.Tensor],
        real_gradients: List[torch.Tensor]
    ) -> torch.Tensor:
        """Compute loss between dummy and real gradients"""
        grad_loss = 0.0
        
        for dummy_g, real_g in zip(dummy_gradients, real_gradients):
            if dummy_g.shape != real_g.shape:
                continue
            grad_loss += ((dummy_g - real_g) ** 2).sum()
        
        return grad_loss
    
    def attack(
        self,
        real_gradients: List[torch.Tensor],
        input_shape: Tuple[int, ...],
        num_classes: int,
        labels: Optional[torch.Tensor] = None,
        initialization: str = 'random',
        batch_size: int = 1
    ) -> Dict[str, Any]:
        """
        Execute DLG attack to recover training data from gradients
        
        Args:
            real_gradients: List of gradient tensors from real training
            input_shape: Shape of input data to recover (e.g., (3, 32, 32) for CIFAR)
            num_classes: Number of output classes
            labels: Known labels (if available, makes attack easier)
            initialization: How to initialize dummy data ('random', 'randn', 'uniform')
            batch_size: Number of samples to recover (default: 1, higher values reduce success rate)
        
        Returns:
            Dictionary containing:
                - 'data': Recovered input data
                - 'labels': Recovered or provided labels
                - 'losses': Loss history during optimization
                - 'success': Whether attack converged
        """
        
        # Initialize dummy data
        if initialization == 'randn':
            dummy_data = torch.randn((batch_size, *input_shape), requires_grad=True, device=self.device)
        elif initialization == 'uniform':
            dummy_data = torch.rand((batch_size, *input_shape), requires_grad=True, device=self.device)
        else:
            dummy_data = torch.randn((batch_size, *input_shape), requires_grad=True, device=self.device)
            dummy_data.data = dummy_data.data * 0.1
        
        # Initialize or use provided labels
        if labels is None:
            dummy_labels = torch.randn(
                (batch_size, num_classes),
                requires_grad=True,
                device=self.device
            )
        else:
            dummy_labels = labels.to(self.device)
        
        # Optimizer for dummy data (and labels if unknown)
        if labels is None:
            optimizer = optim.LBFGS([dummy_data, dummy_labels], lr=self.lr)
        else:
            optimizer = optim.LBFGS([dummy_data], lr=self.lr)
        
        losses = []
        reconstruction_history = []  # Store reconstruction every 10 iterations
        
        for iteration in range(self.max_iterations):
            def closure():
                optimizer.zero_grad()
                
                # Forward pass with dummy data
                dummy_output = self.model(dummy_data)
                
                # Compute loss
                if labels is None:
                    # Use soft labels with cross entropy (following original DLG)
                    dummy_onehot = torch.nn.functional.softmax(dummy_labels, dim=-1)
                    dummy_loss = self.loss_fn(dummy_output, dummy_onehot)
                else:
                    dummy_loss = self.loss_fn(dummy_output, dummy_labels)
                
                # Compute dummy gradients
                dummy_gradients = torch.autograd.grad(
                    dummy_loss,
                    self.model.parameters(),
                    retain_graph=True,
                    create_graph=True
                )
                
                # Gradient matching loss
                grad_loss = self._compute_gradient_loss(dummy_gradients, real_gradients)
                
                # Total loss with regularization
                total_loss = grad_loss
                
                if self.tv_reg > 0:
                    total_loss += self.tv_reg * self._total_variation(dummy_data)
                
                if self.l2_reg > 0:
                    total_loss += self.l2_reg * torch.norm(dummy_data, 2)
                
                total_loss.backward()
                
                return total_loss
            
            loss = optimizer.step(closure)
            losses.append(loss.item())
            
            # Save reconstruction every 10 iterations
            if iteration % 10 == 0:
                reconstruction_history.append({
                    'iteration': iteration,
                    'data': dummy_data.detach().cpu().clone(),
                    'loss': loss.item()
                })
            
            if self.verbose and iteration % 50 == 0:
                print(f"Iteration {iteration}: Loss = {loss.item():.6f}")
            
            # Early stopping if converged
            if len(losses) > 10 and abs(losses[-1] - losses[-2]) < 1e-7:
                if self.verbose:
                    print(f"Converged at iteration {iteration}")
                break
        
        # Recover labels if they were unknown
        if labels is None:
            recovered_labels = dummy_labels.argmax(dim=-1)
        else:
            recovered_labels = labels
        
        return {
            'data': dummy_data.detach().cpu(),
            'labels': recovered_labels.detach().cpu(),
            'losses': losses,
            'success': losses[-1] < 0.1,
            'iterations': iteration + 1,
            'reconstruction_history': reconstruction_history
        }
    
