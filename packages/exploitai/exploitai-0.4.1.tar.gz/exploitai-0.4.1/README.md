# ExploitAI

AI Model Security Testing Framework for research and education.

## Overview

ExploitAI is a Python library designed to demonstrate and test security vulnerabilities in machine learning models. It focuses on:

- **Gradient Inversion Attacks** (DLG, iDLG, GIAS)
- **Membership Inference Attacks** (Shokri et al., variations)  
- **Model Poisoning Attacks** (data poisoning, Byzantine attacks)

## Installation

```bash
pip install exploitai
```

## Quick Start

```python
import exploitai as ea

# Load a pre-trained model
model = ea.models.load_model("resnet18", pretrained=True)

# Run gradient inversion attack
attack = ea.GradientInversion(method="idlg")
results = attack.run(model, target_gradients)

# Generate security report
ea.generate_report(results, output="security_audit.html")
```

## Purpose

This library is developed for:

- **Security Research**: Understanding ML model vulnerabilities
- **Educational Use**: Teaching AI security concepts  
- **Defensive Testing**: Evaluating model robustness

**Note**: This tool is intended for defensive security research and education only.

## Development Status

ðŸš§ **Alpha Release** - Core framework and basic attacks in development

## License

MIT License - see LICENSE file for details.