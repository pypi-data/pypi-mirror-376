# Copyright (c) 2025 PaddlePaddle Authors. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import shutil
import time
from typing import Any, Optional

import paddle
from paddleformers.mergekit import MergeConfig, MergeModel
from paddleformers.trainer import get_last_checkpoint
from paddleformers.utils.download import resolve_file_path
from paddleformers.utils.env import SAFE_WEIGHTS_INDEX_NAME, SAFE_WEIGHTS_NAME
from paddleformers.utils.log import logger

from ernie.utils.download_utils import check_download_repo

from ..hparams import get_export_args, read_args
from ..utils.process import is_valid_model_dir


def logger_merge_config(merge_config, lora_merge):
    """
    Logs the merge configuration details to debug output, with different formatting
    for LoRA merges versus standard model merges.

    Args:
        merge_config (object): Configuration object containing merge parameters.
                              Expected to have attributes accessible via __dict__.
        lora_merge (bool): Flag indicating whether this is a LoRA merge operation.
                           When True, logs only LoRA-specific parameters.
                           When False, logs standard merge parameters.

    Outputs:
        Writes formatted configuration details to the logger at DEBUG level.
        For LoRA merges: Displays centered "LoRA Merge Info" header and specific paths.
        For standard merges: Displays centered "Mergekit Config Info" header and all
        parameters except excluded ones.
    """
    if lora_merge:
        logger.debug("{:^40}".format("LoRA Merge Info"))
        for k, v in merge_config.__dict__.items():
            if k in ["lora_model_path", "base_model_path"]:
                logger.debug(f"{k:30}: {v}")
    else:
        logger.debug("{:^40}".format("Mergekit Config Info"))
        for k, v in merge_config.__dict__.items():
            if k in ["model_path_str", "device", "tensor_type", "merge_preifx"]:
                continue
            logger.debug(f"{k:30}: {v}")


def run_export(args: Optional[dict[str, Any]] = None) -> None:
    """_summary_

    Args:
        args (Optional[dict[str, Any]], optional): _description_. Defaults to None.
    """

    args = read_args(args)
    model_args, data_args, generating_args, finetuning_args, export_args = (
        get_export_args(args)
    )

    paddle.set_device(finetuning_args.device)

    last_checkpoint = None
    if os.path.isdir(finetuning_args.output_dir):
        # Check if the output directory is a valid model directory (contains .safetensors or .pdparams files)
        if is_valid_model_dir(finetuning_args.output_dir):
            last_checkpoint = finetuning_args.output_dir
        # If not a model directory but still a valid path, try to find the latest checkpoint
        else:
            last_checkpoint = get_last_checkpoint(finetuning_args.output_dir)
    if last_checkpoint is not None:
        logger.info(f"Starting model export from checkpoint: {last_checkpoint}")
    else:
        raise FileNotFoundError(
            f"No valid checkpoint found in: {finetuning_args.output_dir}"
        )

    if model_args.lora:
        start = time.time()
        logger.info("***** Start merging LoRA model *****")

        model_args.model_name_or_path = check_download_repo(
            model_args.model_name_or_path,
            from_hf_hub=model_args.from_hf_hub,
            from_aistudio=model_args.from_aistudio,
            from_modelscope=model_args.from_modelscope,
        )
        if getattr(model_args, "from_modelscope", False):
            os.environ["from_modelscope"] = "True"

        resolve_result = resolve_file_path(
            model_args.model_name_or_path,
            [SAFE_WEIGHTS_INDEX_NAME, SAFE_WEIGHTS_NAME],
            from_hf_hub=model_args.from_hf_hub,
            from_aistudio=model_args.from_aistudio,
        )
        if resolve_result is not None:
            resolve_path = os.path.dirname(resolve_result)
            logger.info(f"base model path parsed:{resolve_path}")
        else:
            logger.error(f"{model_args.model_name_or_path} does not found.")

        config = {}
        config["base_model_path"] = resolve_path
        config["lora_model_path"] = last_checkpoint
        config["output_path"] = os.path.join(finetuning_args.output_dir, "export")
        if export_args.copy_tokenizer:
            config["copy_file_list"] = [
                "tokenizer.model",
                "tokenizer_config.json",
                "special_tokens_map.json",
                # "config.json",
            ]

        merge_config = MergeConfig(**config)
        mergekit = MergeModel(merge_config)
        logger_merge_config(merge_config, model_args.lora)
        mergekit.merge_model()
        src_file = os.path.join(config["base_model_path"], "config.json")
        dst_file = os.path.join(config["output_path"], "config.json")
        if os.path.isfile(src_file):
            shutil.copy2(src_file, dst_file)
        else:
            logger.debug(
                f'Copy failed: "config.json" not found in {config["base_model_path"]}'
            )
        src_file = os.path.join(config["base_model_path"], "preprocessor_config.json")
        dst_file = os.path.join(config["output_path"], "preprocessor_config.json")
        if os.path.isfile(src_file):
            shutil.copy2(src_file, dst_file)
        else:
            logger.debug(
                f'Copy failed: "preprocessor_config.json" not found in {config["base_model_path"]}'
            )
        logger.info(
            f"***** Successfully finished merging LoRA model. Time cost: {time.time() - start} s *****"
        )
    else:
        raise ValueError(
            "Only support merge lora checkpoint, but get model_args.lora is False."
        )
