### data
train_dataset_type: "erniekit"
eval_dataset_type: "erniekit"
train_dataset_path: "./examples/data/sft-train.jsonl"
train_dataset_prob: "1.0"
eval_dataset_path: "./examples/data/sft-eval.jsonl"
eval_dataset_prob: "1.0"
max_seq_len: 8192
max_prompt_len: 2048
num_samples_each_epoch: 6000000

### model
model_name_or_path: ./baidu/ERNIE-4.5-0.3B
moe_group: dummy
fine_tuning: LoRA
lora_rank: 32
lora_alpha: -1
lora_plus_scale: 1.0
rslora: False
fuse_rope: True
fuse_linear: False
use_sparse_head_and_loss_fn: True

### finetuning
# base
stage: SFT
seed: 23
do_train: True
do_eval: True
distributed_dataloader: False
dataloader_num_workers: 1
batch_size: 1
num_train_epochs: 1
max_steps: 100
max_evaluate_steps: 10000
eval_steps: 10000
evaluation_strategy: steps
save_steps: 10
save_total_limit: 5
save_strategy: steps
logging_steps: 1
release_grads: True
gradient_accumulation_steps: 8
logging_dir: ./vdl_log
output_dir: ./output
disable_tqdm: True

# train
warmup_steps: 20
learning_rate: 3.0e-4
lr_scheduler_type: cosine
min_lr: 0
layerwise_lr_decay_bound: 1.0

# loss
offset_alpha: 0.0
scale_loss: 32768

# optimizer
optim: adamw
attention_probs_dropout_prob: 0.0
dropout_warmup_steps: 0
weight_decay: 0.01
adam_epsilon: 1.0e-8
adam_beta1: 0.9
adam_beta2: 0.95
offload_optim: True

# performance
use_sp_callback: False
tensor_parallel_degree: 1
tensor_parallel_config: ""
pipeline_parallel_degree: 1
sharding_parallel_degree: 1
sharding: stage1
sequence_parallel: True
pp_seg_method: layer:Ernie4_5_DecoderLayer|EmptyLayer
pipeline_parallel_config: disable_partial_send_recv enable_clear_every_step_cache
recompute: False
recompute_use_reentrant: True
compute_type: bf16
fp16_opt_level: O2
disable_ckpt_quant: True
amp_master_grad: True
amp_custom_white_list:
  - lookup_table
  - lookup_table_v2
  - flash_attn
  - matmul
  - matmul_v2
  - fused_gemm_epilogue
amp_custom_black_list:
  - reduce_sum
  - softmax_with_cross_entropy
  - c_softmax_with_cross_entropy
  - elementwise_div
  - sin
  - cos
unified_checkpoint: True
unified_checkpoint_config: ""
