from dataclasses import dataclass
from typing import List, Optional, Dict, Any, Union

from gen_ai_hub.orchestration.models.message import Message, FunctionCall
from gen_ai_hub.orchestration.models.multimodal_items import ContentPart


@dataclass
class ToolCallChunk:
    index: int
    id: Optional[str] = None
    type: Optional[str] = None
    function: Optional[FunctionCall] = None


@dataclass
class ChatDelta:
    """
    Represents a partial update in a streaming chat response.

    Attributes:
        content: The text content of the chat delta.
        role: Optional role identifier (e.g., 'assistant', 'user') for the message delta.
    """
    content: Union[str, List[ContentPart]]
    role: Optional[str] = None
    refusal: Optional[str] = None
    tool_calls: Optional[List[ToolCallChunk]] = None


@dataclass
class LLMUsage:
    """
    Represents the token usage statistics for an LLM (Large Language Model) operation.

    Attributes:
        completion_tokens: The number of tokens generated by the model in the response.
        prompt_tokens: The number of tokens in the input prompt.
        total_tokens: The total number of tokens used, including both prompt and completion tokens.
    """
    completion_tokens: int
    prompt_tokens: int
    total_tokens: int


@dataclass
class LLMChoice:
    """
    Represents an individual choice or response generated by the LLM.
    
    Attributes:
        index: The index of this particular choice in the list of possible choices.
        message: The message object containing the role and content of the response.
        finish_reason: The reason why the model stopped generating tokens.
        logprobs: Optional dictionary containing token log probabilities.
    """
    index: int
    message: Message
    finish_reason: str
    logprobs: Optional[Dict[str, float]] = None


@dataclass
class LLMChoiceStreaming:
    """
        Represents a streaming choice or partial response generated by the LLM.

        Attributes:
            index: The index of this particular choice in the list of possible choices.
            delta: The partial update (ChatDelta) for this choice.
            finish_reason: Optional reason for why the generation stopped, may be None during streaming.
            logprobs: Optional dictionary containing token log probabilities.
    """
    index: int
    delta: ChatDelta
    finish_reason: Optional[str] = None
    logprobs: Optional[Dict[str, float]] = None


@dataclass
class BaseLLMResult:
    """
    Base class for LLM results containing common attributes.

    Attributes:
        id: Unique identifier for the LLM operation.
        object: Type of object returned (e.g., "chat.completion").
        created: Timestamp when this result was created.
        model: Name or identifier of the model used.
    """
    id: str
    object: str
    created: int
    model: str


@dataclass
class LLMResult(BaseLLMResult):
    """
    Represents the complete result from an LLM operation.

    Attributes:
        id: The unique identifier for this LLM operation.
        object: The type of object returned (typically "chat.completion").
        created: The timestamp when this result was created.
        model: The name or identifier of the model used for generating the result.
        choices: A list of possible choices generated by the LLM.
        usage: The token usage statistics for this operation.
        system_fingerprint: An optional system fingerprint for tracking the model used.
    """
    choices: List[LLMChoice]
    usage: LLMUsage
    system_fingerprint: Optional[str] = None


@dataclass
class LLMResultStreaming(BaseLLMResult):
    """
        Represents a streaming result from an LLM operation.

        Attributes:
            id: The unique identifier for this LLM operation.
            object: The type of object returned (typically "chat.completion.chunk").
            created: The timestamp when this result was created.
            model: The name or identifier of the model used.
            choices: A list of streaming choices generated by the LLM.
            usage: optional token usage statistics for this operation.
            system_fingerprint: An optional system fingerprint for tracking the model used.
    """
    choices: List[LLMChoiceStreaming]
    usage: Optional[LLMUsage] = None
    system_fingerprint: Optional[str] = None


@dataclass
class GenericModuleResult:
    """
    Represents a generic module result in the orchestration process.

    Attributes:
        message: A message or description generated by the module.
        data: Additional data relevant to the module result.
    """
    message: str
    data: Optional[Dict[str, Any]] = None


@dataclass
class BaseModuleResults:
    """
        Base class for module results containing grounding, common filtering and masking attributes.

        Attributes:
            input_filtering: Results from the input filtering module.
            output_filtering: Results from the output filtering module.
            input_masking: Results from the input masking module.
            grounding: A list of extracted text to be provided as grounding context.
            input_translation: Results from the input translation module.
            output_translation: Results from the output translation module.
    """
    input_filtering: Optional[GenericModuleResult] = None
    output_filtering: Optional[GenericModuleResult] = None
    input_masking: Optional[GenericModuleResult] = None
    grounding: Optional[GenericModuleResult] = None
    input_translation: Optional[GenericModuleResult] = None
    output_translation: Optional[GenericModuleResult] = None


@dataclass
class ModuleResults(BaseModuleResults):
    """
    Represents the results of various modules used in processing an orchestration request.

    Attributes:
        templating: A list of messages that define the conversation's context or template.
        llm: The result from the LLM operation.
        input_filtering: The result of any input filtering, if applicable.
        output_filtering: The result of any output filtering, if applicable.
        input_masking: The result of input masking, if applicable.
        output_unmasking: The result of output unmasking, if applicable.
    """
    llm: Optional[LLMResult] = None
    templating: Optional[List[Message]] = None
    output_unmasking: Optional[List[LLMChoice]] = None


@dataclass
class ModuleResultsStreaming(BaseModuleResults):
    """
        Represents the streaming results of various modules used in processing an orchestration request.

        Attributes:
            llm: The streaming result from the LLM operation.
            templating: A list of chat deltas that define the conversation's context or template.
            input_filtering: The result of any input filtering, if applicable.
            output_filtering: The result of any output filtering, if applicable.
            input_masking: The result of input masking, if applicable.
            output_unmasking: The result of output unmasking for streaming responses.
    """
    llm: Optional[LLMResultStreaming] = None
    templating: Optional[List[ChatDelta]] = None
    output_unmasking: Optional[List[LLMChoiceStreaming]] = None


@dataclass
class OrchestrationResponse:
    """
    Represents the complete response from an orchestration process.

    Attributes:
        request_id: The unique identifier for the request being processed.
        module_results: The results from the various modules involved in processing the request.
        orchestration_result: The final result from the orchestration, typically mirroring the LLM result.
    """
    request_id: str
    module_results: ModuleResults
    orchestration_result: LLMResult

    @property
    def content(self) -> str:
        """
        Returns the content of the first choice in the orchestration result.
        """

        if not self.orchestration_result.choices:
            raise ValueError("No choices available in the orchestration result.")

        return self.orchestration_result.choices[0].message.content

@dataclass
class OrchestrationResponseStreaming:
    """
        Represents the streaming response from an orchestration process.

        Attributes:
            request_id: The unique identifier for the request being processed.
            module_results: The streaming results from the various modules involved in processing the request.
            orchestration_result: The streaming result from the orchestration.
    """
    request_id: str
    module_results: ModuleResultsStreaming
    orchestration_result: LLMResultStreaming
