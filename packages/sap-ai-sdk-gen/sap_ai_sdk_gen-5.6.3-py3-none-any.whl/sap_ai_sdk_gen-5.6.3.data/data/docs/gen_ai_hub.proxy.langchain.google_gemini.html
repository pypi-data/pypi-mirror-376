<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN">
<html><head><title>Python: module gen_ai_hub.proxy.langchain.google_gemini</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
</head><body bgcolor="#f0f0f8">

<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="heading">
<tr bgcolor="#7799ee">
<td valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial">&nbsp;<br><big><big><strong><a href="gen_ai_hub.html"><font color="#ffffff">gen_ai_hub</font></a>.<a href="gen_ai_hub.proxy.html"><font color="#ffffff">proxy</font></a>.<a href="gen_ai_hub.proxy.langchain.html"><font color="#ffffff">langchain</font></a>.google_gemini</strong></big></big></font></td
><td align=right valign=bottom
><font color="#ffffff" face="helvetica, arial"><a href=".">index</a><br><a href="file:/home/jenkins/agent/workspace/ation_generative-ai-hub-sdk_main/gen_ai_hub/proxy/langchain/google_gemini.py">/home/jenkins/agent/workspace/ation_generative-ai-hub-sdk_main/gen_ai_hub/proxy/langchain/google_gemini.py</a></font></td></tr></table>
    <p></p>
<p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ee77aa">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Classes</strong></big></font></td></tr>
    
<tr><td bgcolor="#ee77aa"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><dl>
<dt><font face="helvetica, arial"><a href="langchain_google_genai.chat_models.html#ChatGoogleGenerativeAI">langchain_google_genai.chat_models.ChatGoogleGenerativeAI</a>(<a href="langchain_google_genai.llms.html#_BaseGoogleGenerativeAI">langchain_google_genai.llms._BaseGoogleGenerativeAI</a>, <a href="langchain_core.language_models.chat_models.html#BaseChatModel">langchain_core.language_models.chat_models.BaseChatModel</a>)
</font></dt><dd>
<dl>
<dt><font face="helvetica, arial"><a href="gen_ai_hub.proxy.langchain.google_gemini.html#ChatGoogleGenerativeAI">ChatGoogleGenerativeAI</a>
</font></dt></dl>
</dd>
</dl>
 <p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#ffc8d8">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#000000" face="helvetica, arial"><a name="ChatGoogleGenerativeAI">class <strong>ChatGoogleGenerativeAI</strong></a>(<a href="langchain_google_genai.chat_models.html#ChatGoogleGenerativeAI">langchain_google_genai.chat_models.ChatGoogleGenerativeAI</a>)</font></td></tr>
    
<tr bgcolor="#ffc8d8"><td rowspan=2><tt>&nbsp;&nbsp;&nbsp;</tt></td>
<td colspan=2><tt><a href="#ChatGoogleGenerativeAI">ChatGoogleGenerativeAI</a>(*args,&nbsp;model:&nbsp;str&nbsp;=&nbsp;'',&nbsp;proxy_model_name:&nbsp;str&nbsp;=&nbsp;'',&nbsp;model_id:&nbsp;str&nbsp;=&nbsp;'',&nbsp;deployment_id:&nbsp;str&nbsp;=&nbsp;'',&nbsp;model_name:&nbsp;str&nbsp;=&nbsp;'',&nbsp;config_id:&nbsp;str&nbsp;=&nbsp;'',&nbsp;config_name:&nbsp;str&nbsp;=&nbsp;'',&nbsp;proxy_client:&nbsp;Optional[gen_ai_hub.proxy.core.base.BaseProxyClient]&nbsp;=&nbsp;None,&nbsp;name:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;cache:&nbsp;ForwardRef('Union[BaseCache,&nbsp;bool,&nbsp;None]')&nbsp;=&nbsp;None,&nbsp;verbose:&nbsp;bool&nbsp;=&nbsp;None,&nbsp;callbacks:&nbsp;ForwardRef('Callbacks')&nbsp;=&nbsp;None,&nbsp;tags:&nbsp;Optional[List[str]]&nbsp;=&nbsp;None,&nbsp;metadata:&nbsp;Optional[Dict[str,&nbsp;Any]]&nbsp;=&nbsp;None,&nbsp;custom_get_token_ids:&nbsp;Optional[Callable[[str],&nbsp;List[int]]]&nbsp;=&nbsp;None,&nbsp;callback_manager:&nbsp;Optional[langchain_core.callbacks.base.BaseCallbackManager]&nbsp;=&nbsp;None,&nbsp;rate_limiter:&nbsp;Optional[langchain_core.rate_limiters.BaseRateLimiter]&nbsp;=&nbsp;None,&nbsp;google_api_key:&nbsp;Optional[pydantic.v1.types.SecretStr]&nbsp;=&nbsp;None,&nbsp;credentials:&nbsp;Any&nbsp;=&nbsp;None,&nbsp;temperature:&nbsp;float&nbsp;=&nbsp;0.7,&nbsp;top_p:&nbsp;Optional[float]&nbsp;=&nbsp;None,&nbsp;top_k:&nbsp;Optional[int]&nbsp;=&nbsp;None,&nbsp;max_output_tokens:&nbsp;Optional[int]&nbsp;=&nbsp;None,&nbsp;n:&nbsp;int&nbsp;=&nbsp;1,&nbsp;max_retries:&nbsp;int&nbsp;=&nbsp;6,&nbsp;timeout:&nbsp;Optional[float]&nbsp;=&nbsp;None,&nbsp;client_options:&nbsp;Optional[Dict]&nbsp;=&nbsp;None,&nbsp;transport:&nbsp;Optional[str]&nbsp;=&nbsp;None,&nbsp;additional_headers:&nbsp;Optional[Dict[str,&nbsp;str]]&nbsp;=&nbsp;None,&nbsp;safety_settings:&nbsp;Optional[Dict[google.ai.generativelanguage_v1beta.types.safety.HarmCategory,&nbsp;google.ai.generativelanguage_v1beta.types.safety.SafetySetting.HarmBlockThreshold]]&nbsp;=&nbsp;None,&nbsp;client:&nbsp;Any&nbsp;=&nbsp;None,&nbsp;async_client:&nbsp;Any&nbsp;=&nbsp;None,&nbsp;default_metadata:&nbsp;Sequence[Tuple[str,&nbsp;str]]&nbsp;=&nbsp;None,&nbsp;convert_system_message_to_human:&nbsp;bool&nbsp;=&nbsp;False)&nbsp;-&amp;gt;&nbsp;None<br>
&nbsp;<br>
Drop-in&nbsp;replacement&nbsp;for&nbsp;langchain_google_genai.<a href="#ChatGoogleGenerativeAI">ChatGoogleGenerativeAI</a>.<br>&nbsp;</tt></td></tr>
<tr><td>&nbsp;</td>
<td width="100%"><dl><dt>Method resolution order:</dt>
<dd><a href="gen_ai_hub.proxy.langchain.google_gemini.html#ChatGoogleGenerativeAI">ChatGoogleGenerativeAI</a></dd>
<dd><a href="langchain_google_genai.chat_models.html#ChatGoogleGenerativeAI">langchain_google_genai.chat_models.ChatGoogleGenerativeAI</a></dd>
<dd><a href="langchain_google_genai.llms.html#_BaseGoogleGenerativeAI">langchain_google_genai.llms._BaseGoogleGenerativeAI</a></dd>
<dd><a href="langchain_core.language_models.chat_models.html#BaseChatModel">langchain_core.language_models.chat_models.BaseChatModel</a></dd>
<dd><a href="langchain_core.language_models.base.html#BaseLanguageModel">langchain_core.language_models.base.BaseLanguageModel</a></dd>
<dd><a href="langchain_core.runnables.base.html#RunnableSerializable">langchain_core.runnables.base.RunnableSerializable</a></dd>
<dd><a href="langchain_core.load.serializable.html#Serializable">langchain_core.load.serializable.Serializable</a></dd>
<dd><a href="pydantic.v1.main.html#BaseModel">pydantic.v1.main.BaseModel</a></dd>
<dd><a href="pydantic.v1.utils.html#Representation">pydantic.v1.utils.Representation</a></dd>
<dd><a href="langchain_core.runnables.base.html#Runnable">langchain_core.runnables.base.Runnable</a></dd>
<dd><a href="typing.html#Generic">typing.Generic</a></dd>
<dd><a href="abc.html#ABC">abc.ABC</a></dd>
<dd><a href="builtins.html#object">builtins.object</a></dd>
</dl>
<hr>
Methods defined here:<br>
<dl><dt><a name="ChatGoogleGenerativeAI-__init__"><strong>__init__</strong></a>(self, *args, model: str = '', proxy_model_name: str = '', model_id: str = '', deployment_id: str = '', model_name: str = '', config_id: str = '', config_name: str = '', proxy_client: Optional[gen_ai_hub.proxy.core.base.BaseProxyClient] = None, **kwargs)</dt><dd><tt>Create&nbsp;a&nbsp;new&nbsp;model&nbsp;by&nbsp;parsing&nbsp;and&nbsp;validating&nbsp;input&nbsp;data&nbsp;from&nbsp;keyword&nbsp;arguments.<br>
&nbsp;<br>
Raises&nbsp;ValidationError&nbsp;if&nbsp;the&nbsp;input&nbsp;data&nbsp;cannot&nbsp;be&nbsp;parsed&nbsp;to&nbsp;form&nbsp;a&nbsp;valid&nbsp;model.</tt></dd></dl>

<hr>
Class methods defined here:<br>
<dl><dt><a name="ChatGoogleGenerativeAI-validate_environment"><strong>validate_environment</strong></a>(values: Dict) -&gt; Dict<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Validates&nbsp;params&nbsp;and&nbsp;passes&nbsp;them&nbsp;to&nbsp;google-generativeai&nbsp;package.</tt></dd></dl>

<hr>
Static methods defined here:<br>
<dl><dt><a name="ChatGoogleGenerativeAI-__json_encoder__"><strong>__json_encoder__</strong></a> = pydantic_encoder(obj: Any) -&gt; Any</dt></dl>

<hr>
Data and other attributes defined here:<br>
<dl><dt><strong>__abstractmethods__</strong> = frozenset()</dl>

<dl><dt><strong>__class_vars__</strong> = set()</dl>

<dl><dt><strong>__config__</strong> = &lt;class 'pydantic.v1.config.Config'&gt;</dl>

<dl><dt><strong>__custom_root_type__</strong> = False</dl>

<dl><dt><strong>__exclude_fields__</strong> = {'callback_manager': True, 'callbacks': True, 'custom_get_token_ids': True, 'metadata': True, 'rate_limiter': True, 'tags': True}</dl>

<dl><dt><strong>__fields__</strong> = {'additional_headers': ModelField(name='additional_headers', type=Optio...Mapping[str, str]], required=False, default=None), 'async_client': ModelField(name='async_client', type=Optional[Any], required=False, default=None), 'cache': ModelField(name='cache', type=Union[BaseCache, bool, NoneType], required=False, default=None), 'callback_manager': ModelField(name='callback_manager', type=Optiona...seCallbackManager], required=False, default=None), 'callbacks': ModelField(name='callbacks', type=Union[List[lan...Manager, NoneType], required=False, default=None), 'client': ModelField(name='client', type=Optional[Any], required=False, default=None), 'client_options': ModelField(name='client_options', type=Optional[Mapping[Any, Any]], required=False, default=None), 'convert_system_message_to_human': ModelField(name='convert_system_message_to_human', type=bool, required=False, default=False), 'credentials': ModelField(name='credentials', type=Optional[Any], required=False, default=None), 'custom_get_token_ids': ModelField(name='custom_get_token_ids', type=Opt...[str], List[int]]], required=False, default=None), ...}</dl>

<dl><dt><strong>__hash__</strong> = None</dl>

<dl><dt><strong>__include_fields__</strong> = None</dl>

<dl><dt><strong>__parameters__</strong> = ()</dl>

<dl><dt><strong>__post_root_validators__</strong> = [(False, &lt;function ChatGoogleGenerativeAI.validate_environment&gt;)]</dl>

<dl><dt><strong>__pre_root_validators__</strong> = [&lt;function BaseChatModel.raise_deprecation&gt;]</dl>

<dl><dt><strong>__private_attributes__</strong> = {}</dl>

<dl><dt><strong>__schema_cache__</strong> = {}</dl>

<dl><dt><strong>__signature__</strong> = &lt;Signature (*args, model: str = '', proxy_model_...t_system_message_to_human: bool = False) -&gt; None&gt;</dl>

<dl><dt><strong>__validators__</strong> = {'verbose': [&lt;pydantic.v1.class_validators.Validator object&gt;]}</dl>

<hr>
Methods inherited from <a href="langchain_google_genai.chat_models.html#ChatGoogleGenerativeAI">langchain_google_genai.chat_models.ChatGoogleGenerativeAI</a>:<br>
<dl><dt><a name="ChatGoogleGenerativeAI-bind_tools"><strong>bind_tools</strong></a>(self, tools: 'Sequence[Union[ToolDict, GoogleTool]]', tool_config: 'Optional[Union[Dict, _ToolConfigDict]]' = None, *, tool_choice: 'Optional[Union[_ToolChoiceType, bool]]' = None, **kwargs: 'Any') -&gt; 'Runnable[LanguageModelInput, BaseMessage]'</dt><dd><tt>Bind&nbsp;tool-like&nbsp;objects&nbsp;to&nbsp;this&nbsp;chat&nbsp;model.<br>
&nbsp;<br>
Assumes&nbsp;model&nbsp;is&nbsp;compatible&nbsp;with&nbsp;google-generativeAI&nbsp;tool-calling&nbsp;API.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;tools:&nbsp;A&nbsp;list&nbsp;of&nbsp;tool&nbsp;definitions&nbsp;to&nbsp;bind&nbsp;to&nbsp;this&nbsp;chat&nbsp;model.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Can&nbsp;be&nbsp;a&nbsp;pydantic&nbsp;model,&nbsp;callable,&nbsp;or&nbsp;BaseTool.&nbsp;Pydantic<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;models,&nbsp;callables,&nbsp;and&nbsp;BaseTools&nbsp;will&nbsp;be&nbsp;automatically&nbsp;converted&nbsp;to<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;their&nbsp;schema&nbsp;dictionary&nbsp;representation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Any&nbsp;additional&nbsp;parameters&nbsp;to&nbsp;pass&nbsp;to&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;:class:`~langchain.runnable.Runnable`&nbsp;constructor.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-get_num_tokens"><strong>get_num_tokens</strong></a>(self, text: 'str') -&gt; 'int'</dt><dd><tt>Get&nbsp;the&nbsp;number&nbsp;of&nbsp;tokens&nbsp;present&nbsp;in&nbsp;the&nbsp;text.<br>
&nbsp;<br>
Useful&nbsp;for&nbsp;checking&nbsp;if&nbsp;an&nbsp;input&nbsp;will&nbsp;fit&nbsp;in&nbsp;a&nbsp;model's&nbsp;context&nbsp;window.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;text:&nbsp;The&nbsp;string&nbsp;input&nbsp;to&nbsp;tokenize.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;integer&nbsp;number&nbsp;of&nbsp;tokens&nbsp;in&nbsp;the&nbsp;text.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-with_structured_output"><strong>with_structured_output</strong></a>(self, schema: 'Union[Dict, Type[BaseModel]]', *, include_raw: 'bool' = False, **kwargs: 'Any') -&gt; 'Runnable[LanguageModelInput, Union[Dict, BaseModel]]'</dt><dd><tt>Model&nbsp;wrapper&nbsp;that&nbsp;returns&nbsp;outputs&nbsp;formatted&nbsp;to&nbsp;match&nbsp;the&nbsp;given&nbsp;schema.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;schema:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;output&nbsp;schema.&nbsp;Can&nbsp;be&nbsp;passed&nbsp;in&nbsp;as:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;an&nbsp;OpenAI&nbsp;function/tool&nbsp;schema,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;a&nbsp;JSON&nbsp;Schema,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;a&nbsp;TypedDict&nbsp;class&nbsp;(support&nbsp;added&nbsp;in&nbsp;0.2.26),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;or&nbsp;a&nbsp;Pydantic&nbsp;class.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;``schema``&nbsp;is&nbsp;a&nbsp;Pydantic&nbsp;class&nbsp;then&nbsp;the&nbsp;model&nbsp;output&nbsp;will&nbsp;be&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Pydantic&nbsp;instance&nbsp;of&nbsp;that&nbsp;class,&nbsp;and&nbsp;the&nbsp;model-generated&nbsp;fields&nbsp;will&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;validated&nbsp;by&nbsp;the&nbsp;Pydantic&nbsp;class.&nbsp;Otherwise&nbsp;the&nbsp;model&nbsp;output&nbsp;will&nbsp;be&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dict&nbsp;and&nbsp;will&nbsp;not&nbsp;be&nbsp;validated.&nbsp;See&nbsp;:meth:`langchain_core.utils.function_calling.convert_to_openai_tool`<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;more&nbsp;on&nbsp;how&nbsp;to&nbsp;properly&nbsp;specify&nbsp;types&nbsp;and&nbsp;descriptions&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;schema&nbsp;fields&nbsp;when&nbsp;specifying&nbsp;a&nbsp;Pydantic&nbsp;or&nbsp;TypedDict&nbsp;class.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;versionchanged::&nbsp;0.2.26<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Added&nbsp;support&nbsp;for&nbsp;TypedDict&nbsp;class.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_raw:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;False&nbsp;then&nbsp;only&nbsp;the&nbsp;parsed&nbsp;structured&nbsp;output&nbsp;is&nbsp;returned.&nbsp;If<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;an&nbsp;error&nbsp;occurs&nbsp;during&nbsp;model&nbsp;output&nbsp;parsing&nbsp;it&nbsp;will&nbsp;be&nbsp;raised.&nbsp;If&nbsp;True<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;then&nbsp;both&nbsp;the&nbsp;raw&nbsp;model&nbsp;response&nbsp;(a&nbsp;BaseMessage)&nbsp;and&nbsp;the&nbsp;parsed&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;response&nbsp;will&nbsp;be&nbsp;returned.&nbsp;If&nbsp;an&nbsp;error&nbsp;occurs&nbsp;during&nbsp;output&nbsp;parsing&nbsp;it<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;will&nbsp;be&nbsp;caught&nbsp;and&nbsp;returned&nbsp;as&nbsp;well.&nbsp;The&nbsp;final&nbsp;output&nbsp;is&nbsp;always&nbsp;a&nbsp;dict<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;with&nbsp;keys&nbsp;"raw",&nbsp;"parsed",&nbsp;and&nbsp;"parsing_error".<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;Runnable&nbsp;that&nbsp;takes&nbsp;same&nbsp;inputs&nbsp;as&nbsp;a&nbsp;:class:`langchain_core.language_models.chat.BaseChatModel`.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;``include_raw``&nbsp;is&nbsp;False&nbsp;and&nbsp;``schema``&nbsp;is&nbsp;a&nbsp;Pydantic&nbsp;class,&nbsp;Runnable&nbsp;outputs<br>
&nbsp;&nbsp;&nbsp;&nbsp;an&nbsp;instance&nbsp;of&nbsp;``schema``&nbsp;(i.e.,&nbsp;a&nbsp;Pydantic&nbsp;object).<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;Otherwise,&nbsp;if&nbsp;``include_raw``&nbsp;is&nbsp;False&nbsp;then&nbsp;Runnable&nbsp;outputs&nbsp;a&nbsp;dict.<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;If&nbsp;``include_raw``&nbsp;is&nbsp;True,&nbsp;then&nbsp;Runnable&nbsp;outputs&nbsp;a&nbsp;dict&nbsp;with&nbsp;keys:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;``"raw"``:&nbsp;BaseMessage<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;``"parsed"``:&nbsp;None&nbsp;if&nbsp;there&nbsp;was&nbsp;a&nbsp;parsing&nbsp;error,&nbsp;otherwise&nbsp;the&nbsp;type&nbsp;depends&nbsp;on&nbsp;the&nbsp;``schema``&nbsp;as&nbsp;described&nbsp;above.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-&nbsp;``"parsing_error"``:&nbsp;Optional[BaseException]<br>
&nbsp;<br>
Example:&nbsp;Pydantic&nbsp;schema&nbsp;(include_raw=False):<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.pydantic_v1&nbsp;import&nbsp;BaseModel<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;AnswerWithJustification(BaseModel):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'''An&nbsp;answer&nbsp;to&nbsp;the&nbsp;user&nbsp;question&nbsp;along&nbsp;with&nbsp;justification&nbsp;for&nbsp;the&nbsp;answer.'''<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;answer:&nbsp;str<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;justification:&nbsp;str<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;llm&nbsp;=&nbsp;ChatModel(model="model-name",&nbsp;temperature=0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;structured_llm&nbsp;=&nbsp;llm.<a href="#ChatGoogleGenerativeAI-with_structured_output">with_structured_output</a>(AnswerWithJustification)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;structured_llm.<a href="#ChatGoogleGenerativeAI-invoke">invoke</a>("What&nbsp;weighs&nbsp;more&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;or&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers")<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;AnswerWithJustification(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;answer='They&nbsp;weigh&nbsp;the&nbsp;same',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;justification='Both&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;and&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers&nbsp;weigh&nbsp;one&nbsp;pound.&nbsp;The&nbsp;weight&nbsp;is&nbsp;the&nbsp;same,&nbsp;but&nbsp;the&nbsp;volume&nbsp;or&nbsp;density&nbsp;of&nbsp;the&nbsp;objects&nbsp;may&nbsp;differ.'<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;)<br>
&nbsp;<br>
Example:&nbsp;Pydantic&nbsp;schema&nbsp;(include_raw=True):<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.pydantic_v1&nbsp;import&nbsp;BaseModel<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;AnswerWithJustification(BaseModel):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'''An&nbsp;answer&nbsp;to&nbsp;the&nbsp;user&nbsp;question&nbsp;along&nbsp;with&nbsp;justification&nbsp;for&nbsp;the&nbsp;answer.'''<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;answer:&nbsp;str<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;justification:&nbsp;str<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;llm&nbsp;=&nbsp;ChatModel(model="model-name",&nbsp;temperature=0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;structured_llm&nbsp;=&nbsp;llm.<a href="#ChatGoogleGenerativeAI-with_structured_output">with_structured_output</a>(AnswerWithJustification,&nbsp;include_raw=True)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;structured_llm.<a href="#ChatGoogleGenerativeAI-invoke">invoke</a>("What&nbsp;weighs&nbsp;more&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;or&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'raw':&nbsp;AIMessage(content='',&nbsp;additional_kwargs={'tool_calls':&nbsp;[{'id':&nbsp;'call_Ao02pnFYXD6GN1yzc0uXPsvF',&nbsp;'function':&nbsp;{'arguments':&nbsp;'{"answer":"They&nbsp;weigh&nbsp;the&nbsp;same.","justification":"Both&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;and&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers&nbsp;weigh&nbsp;one&nbsp;pound.&nbsp;The&nbsp;weight&nbsp;is&nbsp;the&nbsp;same,&nbsp;but&nbsp;the&nbsp;volume&nbsp;or&nbsp;density&nbsp;of&nbsp;the&nbsp;objects&nbsp;may&nbsp;differ."}',&nbsp;'name':&nbsp;'AnswerWithJustification'},&nbsp;'type':&nbsp;'function'}]}),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'parsed':&nbsp;AnswerWithJustification(answer='They&nbsp;weigh&nbsp;the&nbsp;same.',&nbsp;justification='Both&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;and&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers&nbsp;weigh&nbsp;one&nbsp;pound.&nbsp;The&nbsp;weight&nbsp;is&nbsp;the&nbsp;same,&nbsp;but&nbsp;the&nbsp;volume&nbsp;or&nbsp;density&nbsp;of&nbsp;the&nbsp;objects&nbsp;may&nbsp;differ.'),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'parsing_error':&nbsp;None<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;}<br>
&nbsp;<br>
Example:&nbsp;Dict&nbsp;schema&nbsp;(include_raw=False):<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.pydantic_v1&nbsp;import&nbsp;BaseModel<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.utils.function_calling&nbsp;import&nbsp;convert_to_openai_tool<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;AnswerWithJustification(BaseModel):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'''An&nbsp;answer&nbsp;to&nbsp;the&nbsp;user&nbsp;question&nbsp;along&nbsp;with&nbsp;justification&nbsp;for&nbsp;the&nbsp;answer.'''<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;answer:&nbsp;str<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;justification:&nbsp;str<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;dict_schema&nbsp;=&nbsp;convert_to_openai_tool(AnswerWithJustification)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;llm&nbsp;=&nbsp;ChatModel(model="model-name",&nbsp;temperature=0)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;structured_llm&nbsp;=&nbsp;llm.<a href="#ChatGoogleGenerativeAI-with_structured_output">with_structured_output</a>(dict_schema)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;structured_llm.<a href="#ChatGoogleGenerativeAI-invoke">invoke</a>("What&nbsp;weighs&nbsp;more&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;or&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'answer':&nbsp;'They&nbsp;weigh&nbsp;the&nbsp;same',<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'justification':&nbsp;'Both&nbsp;a&nbsp;pound&nbsp;of&nbsp;bricks&nbsp;and&nbsp;a&nbsp;pound&nbsp;of&nbsp;feathers&nbsp;weigh&nbsp;one&nbsp;pound.&nbsp;The&nbsp;weight&nbsp;is&nbsp;the&nbsp;same,&nbsp;but&nbsp;the&nbsp;volume&nbsp;and&nbsp;density&nbsp;of&nbsp;the&nbsp;two&nbsp;substances&nbsp;differ.'<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;}</tt></dd></dl>

<hr>
Class methods inherited from <a href="langchain_google_genai.chat_models.html#ChatGoogleGenerativeAI">langchain_google_genai.chat_models.ChatGoogleGenerativeAI</a>:<br>
<dl><dt><a name="ChatGoogleGenerativeAI-is_lc_serializable"><strong>is_lc_serializable</strong></a>() -&gt; 'bool'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Is&nbsp;this&nbsp;class&nbsp;serializable?<br>
&nbsp;<br>
By&nbsp;design,&nbsp;even&nbsp;if&nbsp;a&nbsp;class&nbsp;inherits&nbsp;from&nbsp;Serializable,&nbsp;it&nbsp;is&nbsp;not&nbsp;serializable&nbsp;by<br>
default.&nbsp;This&nbsp;is&nbsp;to&nbsp;prevent&nbsp;accidental&nbsp;serialization&nbsp;of&nbsp;objects&nbsp;that&nbsp;should&nbsp;not<br>
be&nbsp;serialized.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Whether&nbsp;the&nbsp;class&nbsp;is&nbsp;serializable.&nbsp;Default&nbsp;is&nbsp;False.</tt></dd></dl>

<hr>
Readonly properties inherited from <a href="langchain_google_genai.chat_models.html#ChatGoogleGenerativeAI">langchain_google_genai.chat_models.ChatGoogleGenerativeAI</a>:<br>
<dl><dt><strong>lc_secrets</strong></dt>
<dd><tt>A&nbsp;map&nbsp;of&nbsp;constructor&nbsp;argument&nbsp;names&nbsp;to&nbsp;secret&nbsp;ids.<br>
&nbsp;<br>
For&nbsp;example,<br>
&nbsp;&nbsp;&nbsp;&nbsp;{"openai_api_key":&nbsp;"OPENAI_API_KEY"}</tt></dd>
</dl>
<hr>
Data descriptors inherited from <a href="langchain_google_genai.chat_models.html#ChatGoogleGenerativeAI">langchain_google_genai.chat_models.ChatGoogleGenerativeAI</a>:<br>
<dl><dt><strong>__weakref__</strong></dt>
<dd><tt>list&nbsp;of&nbsp;weak&nbsp;references&nbsp;to&nbsp;the&nbsp;object&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="langchain_google_genai.chat_models.html#ChatGoogleGenerativeAI">langchain_google_genai.chat_models.ChatGoogleGenerativeAI</a>:<br>
<dl><dt><strong>Config</strong> = &lt;class 'langchain_google_genai.chat_models.ChatGoogleGenerativeAI.Config'&gt;</dl>

<dl><dt><strong>__annotations__</strong> = {'async_client': 'Any', 'client': 'Any', 'convert_system_message_to_human': 'bool', 'default_metadata': 'Sequence[Tuple[str, str]]'}</dl>

<hr>
Methods inherited from <a href="langchain_core.language_models.chat_models.html#BaseChatModel">langchain_core.language_models.chat_models.BaseChatModel</a>:<br>
<dl><dt><a name="ChatGoogleGenerativeAI-__call__"><strong>__call__</strong></a>(self, messages: 'List[BaseMessage]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -&gt; 'BaseMessage'</dt><dd><tt>[*Deprecated*]&nbsp;<br>
&nbsp;<br>
Notes<br>
-----<br>
..&nbsp;deprecated::&nbsp;langchain-core==0.1.7<br>
&nbsp;&nbsp;&nbsp;Use&nbsp;invoke&nbsp;instead.</tt></dd></dl>

<dl><dt>async <a name="ChatGoogleGenerativeAI-agenerate"><strong>agenerate</strong></a>(self, messages: 'List[List[BaseMessage]]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, run_id: 'Optional[uuid.UUID]' = None, **kwargs: 'Any') -&gt; 'LLMResult'</dt><dd><tt>Asynchronously&nbsp;pass&nbsp;a&nbsp;sequence&nbsp;of&nbsp;prompts&nbsp;to&nbsp;a&nbsp;model&nbsp;and&nbsp;return&nbsp;generations.<br>
&nbsp;<br>
This&nbsp;method&nbsp;should&nbsp;make&nbsp;use&nbsp;of&nbsp;batched&nbsp;calls&nbsp;for&nbsp;models&nbsp;that&nbsp;expose&nbsp;a&nbsp;batched<br>
API.<br>
&nbsp;<br>
Use&nbsp;this&nbsp;method&nbsp;when&nbsp;you&nbsp;want&nbsp;to:<br>
&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;take&nbsp;advantage&nbsp;of&nbsp;batched&nbsp;calls,<br>
&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;need&nbsp;more&nbsp;output&nbsp;from&nbsp;the&nbsp;model&nbsp;than&nbsp;just&nbsp;the&nbsp;top&nbsp;generated&nbsp;value,<br>
&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;are&nbsp;building&nbsp;chains&nbsp;that&nbsp;are&nbsp;agnostic&nbsp;to&nbsp;the&nbsp;underlying&nbsp;language&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;type&nbsp;(e.g.,&nbsp;pure&nbsp;text&nbsp;completion&nbsp;models&nbsp;vs&nbsp;chat&nbsp;models).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;messages:&nbsp;List&nbsp;of&nbsp;list&nbsp;of&nbsp;messages.<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop:&nbsp;Stop&nbsp;words&nbsp;to&nbsp;use&nbsp;when&nbsp;generating.&nbsp;Model&nbsp;output&nbsp;is&nbsp;cut&nbsp;off&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;occurrence&nbsp;of&nbsp;any&nbsp;of&nbsp;these&nbsp;substrings.<br>
&nbsp;&nbsp;&nbsp;&nbsp;callbacks:&nbsp;Callbacks&nbsp;to&nbsp;pass&nbsp;through.&nbsp;Used&nbsp;for&nbsp;executing&nbsp;additional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality,&nbsp;such&nbsp;as&nbsp;logging&nbsp;or&nbsp;streaming,&nbsp;throughout&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Arbitrary&nbsp;additional&nbsp;keyword&nbsp;arguments.&nbsp;These&nbsp;are&nbsp;usually&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;model&nbsp;provider&nbsp;API&nbsp;call.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;LLMResult,&nbsp;which&nbsp;contains&nbsp;a&nbsp;list&nbsp;of&nbsp;candidate&nbsp;Generations&nbsp;for&nbsp;each&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;and&nbsp;additional&nbsp;model&nbsp;provider-specific&nbsp;output.</tt></dd></dl>

<dl><dt>async <a name="ChatGoogleGenerativeAI-agenerate_prompt"><strong>agenerate_prompt</strong></a>(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -&gt; 'LLMResult'</dt><dd><tt>Asynchronously&nbsp;pass&nbsp;a&nbsp;sequence&nbsp;of&nbsp;prompts&nbsp;and&nbsp;return&nbsp;model&nbsp;generations.<br>
&nbsp;<br>
This&nbsp;method&nbsp;should&nbsp;make&nbsp;use&nbsp;of&nbsp;batched&nbsp;calls&nbsp;for&nbsp;models&nbsp;that&nbsp;expose&nbsp;a&nbsp;batched<br>
API.<br>
&nbsp;<br>
Use&nbsp;this&nbsp;method&nbsp;when&nbsp;you&nbsp;want&nbsp;to:<br>
&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;take&nbsp;advantage&nbsp;of&nbsp;batched&nbsp;calls,<br>
&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;need&nbsp;more&nbsp;output&nbsp;from&nbsp;the&nbsp;model&nbsp;than&nbsp;just&nbsp;the&nbsp;top&nbsp;generated&nbsp;value,<br>
&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;are&nbsp;building&nbsp;chains&nbsp;that&nbsp;are&nbsp;agnostic&nbsp;to&nbsp;the&nbsp;underlying&nbsp;language&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;type&nbsp;(e.g.,&nbsp;pure&nbsp;text&nbsp;completion&nbsp;models&nbsp;vs&nbsp;chat&nbsp;models).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompts:&nbsp;List&nbsp;of&nbsp;PromptValues.&nbsp;A&nbsp;PromptValue&nbsp;is&nbsp;an&nbsp;object&nbsp;that&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;converted&nbsp;to&nbsp;match&nbsp;the&nbsp;format&nbsp;of&nbsp;any&nbsp;language&nbsp;model&nbsp;(string&nbsp;for&nbsp;pure<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text&nbsp;generation&nbsp;models&nbsp;and&nbsp;BaseMessages&nbsp;for&nbsp;chat&nbsp;models).<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop:&nbsp;Stop&nbsp;words&nbsp;to&nbsp;use&nbsp;when&nbsp;generating.&nbsp;Model&nbsp;output&nbsp;is&nbsp;cut&nbsp;off&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;occurrence&nbsp;of&nbsp;any&nbsp;of&nbsp;these&nbsp;substrings.<br>
&nbsp;&nbsp;&nbsp;&nbsp;callbacks:&nbsp;Callbacks&nbsp;to&nbsp;pass&nbsp;through.&nbsp;Used&nbsp;for&nbsp;executing&nbsp;additional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality,&nbsp;such&nbsp;as&nbsp;logging&nbsp;or&nbsp;streaming,&nbsp;throughout&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Arbitrary&nbsp;additional&nbsp;keyword&nbsp;arguments.&nbsp;These&nbsp;are&nbsp;usually&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;model&nbsp;provider&nbsp;API&nbsp;call.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;LLMResult,&nbsp;which&nbsp;contains&nbsp;a&nbsp;list&nbsp;of&nbsp;candidate&nbsp;Generations&nbsp;for&nbsp;each&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;and&nbsp;additional&nbsp;model&nbsp;provider-specific&nbsp;output.</tt></dd></dl>

<dl><dt>async <a name="ChatGoogleGenerativeAI-ainvoke"><strong>ainvoke</strong></a>(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -&gt; 'BaseMessage'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;ainvoke,&nbsp;calls&nbsp;invoke&nbsp;from&nbsp;a&nbsp;thread.<br>
&nbsp;<br>
The&nbsp;default&nbsp;implementation&nbsp;allows&nbsp;usage&nbsp;of&nbsp;async&nbsp;code&nbsp;even&nbsp;if<br>
the&nbsp;Runnable&nbsp;did&nbsp;not&nbsp;implement&nbsp;a&nbsp;native&nbsp;async&nbsp;version&nbsp;of&nbsp;invoke.<br>
&nbsp;<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;run&nbsp;asynchronously.</tt></dd></dl>

<dl><dt>async <a name="ChatGoogleGenerativeAI-apredict"><strong>apredict</strong></a>(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'str'</dt><dd><tt>[*Deprecated*]&nbsp;<br>
&nbsp;<br>
Notes<br>
-----<br>
..&nbsp;deprecated::&nbsp;langchain-core==0.1.7<br>
&nbsp;&nbsp;&nbsp;Use&nbsp;ainvoke&nbsp;instead.</tt></dd></dl>

<dl><dt>async <a name="ChatGoogleGenerativeAI-apredict_messages"><strong>apredict_messages</strong></a>(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'BaseMessage'</dt><dd><tt>[*Deprecated*]&nbsp;<br>
&nbsp;<br>
Notes<br>
-----<br>
..&nbsp;deprecated::&nbsp;langchain-core==0.1.7<br>
&nbsp;&nbsp;&nbsp;Use&nbsp;ainvoke&nbsp;instead.</tt></dd></dl>

<dl><dt>async <a name="ChatGoogleGenerativeAI-astream"><strong>astream</strong></a>(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -&gt; 'AsyncIterator[BaseMessageChunk]'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;astream,&nbsp;which&nbsp;calls&nbsp;ainvoke.<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;support&nbsp;streaming&nbsp;output.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;Runnable.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;output&nbsp;of&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-call_as_llm"><strong>call_as_llm</strong></a>(self, message: 'str', stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -&gt; 'str'</dt><dd><tt>[*Deprecated*]&nbsp;<br>
&nbsp;<br>
Notes<br>
-----<br>
..&nbsp;deprecated::&nbsp;langchain-core==0.1.7<br>
&nbsp;&nbsp;&nbsp;Use&nbsp;invoke&nbsp;instead.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-dict"><strong>dict</strong></a>(self, **kwargs: 'Any') -&gt; 'Dict'</dt><dd><tt>Return&nbsp;a&nbsp;dictionary&nbsp;of&nbsp;the&nbsp;LLM.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-generate"><strong>generate</strong></a>(self, messages: 'List[List[BaseMessage]]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[List[str]]' = None, metadata: 'Optional[Dict[str, Any]]' = None, run_name: 'Optional[str]' = None, run_id: 'Optional[uuid.UUID]' = None, **kwargs: 'Any') -&gt; 'LLMResult'</dt><dd><tt>Pass&nbsp;a&nbsp;sequence&nbsp;of&nbsp;prompts&nbsp;to&nbsp;the&nbsp;model&nbsp;and&nbsp;return&nbsp;model&nbsp;generations.<br>
&nbsp;<br>
This&nbsp;method&nbsp;should&nbsp;make&nbsp;use&nbsp;of&nbsp;batched&nbsp;calls&nbsp;for&nbsp;models&nbsp;that&nbsp;expose&nbsp;a&nbsp;batched<br>
API.<br>
&nbsp;<br>
Use&nbsp;this&nbsp;method&nbsp;when&nbsp;you&nbsp;want&nbsp;to:<br>
&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;take&nbsp;advantage&nbsp;of&nbsp;batched&nbsp;calls,<br>
&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;need&nbsp;more&nbsp;output&nbsp;from&nbsp;the&nbsp;model&nbsp;than&nbsp;just&nbsp;the&nbsp;top&nbsp;generated&nbsp;value,<br>
&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;are&nbsp;building&nbsp;chains&nbsp;that&nbsp;are&nbsp;agnostic&nbsp;to&nbsp;the&nbsp;underlying&nbsp;language&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;type&nbsp;(e.g.,&nbsp;pure&nbsp;text&nbsp;completion&nbsp;models&nbsp;vs&nbsp;chat&nbsp;models).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;messages:&nbsp;List&nbsp;of&nbsp;list&nbsp;of&nbsp;messages.<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop:&nbsp;Stop&nbsp;words&nbsp;to&nbsp;use&nbsp;when&nbsp;generating.&nbsp;Model&nbsp;output&nbsp;is&nbsp;cut&nbsp;off&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;occurrence&nbsp;of&nbsp;any&nbsp;of&nbsp;these&nbsp;substrings.<br>
&nbsp;&nbsp;&nbsp;&nbsp;callbacks:&nbsp;Callbacks&nbsp;to&nbsp;pass&nbsp;through.&nbsp;Used&nbsp;for&nbsp;executing&nbsp;additional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality,&nbsp;such&nbsp;as&nbsp;logging&nbsp;or&nbsp;streaming,&nbsp;throughout&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Arbitrary&nbsp;additional&nbsp;keyword&nbsp;arguments.&nbsp;These&nbsp;are&nbsp;usually&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;model&nbsp;provider&nbsp;API&nbsp;call.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;LLMResult,&nbsp;which&nbsp;contains&nbsp;a&nbsp;list&nbsp;of&nbsp;candidate&nbsp;Generations&nbsp;for&nbsp;each&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;and&nbsp;additional&nbsp;model&nbsp;provider-specific&nbsp;output.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-generate_prompt"><strong>generate_prompt</strong></a>(self, prompts: 'List[PromptValue]', stop: 'Optional[List[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -&gt; 'LLMResult'</dt><dd><tt>Pass&nbsp;a&nbsp;sequence&nbsp;of&nbsp;prompts&nbsp;to&nbsp;the&nbsp;model&nbsp;and&nbsp;return&nbsp;model&nbsp;generations.<br>
&nbsp;<br>
This&nbsp;method&nbsp;should&nbsp;make&nbsp;use&nbsp;of&nbsp;batched&nbsp;calls&nbsp;for&nbsp;models&nbsp;that&nbsp;expose&nbsp;a&nbsp;batched<br>
API.<br>
&nbsp;<br>
Use&nbsp;this&nbsp;method&nbsp;when&nbsp;you&nbsp;want&nbsp;to:<br>
&nbsp;&nbsp;&nbsp;&nbsp;1.&nbsp;take&nbsp;advantage&nbsp;of&nbsp;batched&nbsp;calls,<br>
&nbsp;&nbsp;&nbsp;&nbsp;2.&nbsp;need&nbsp;more&nbsp;output&nbsp;from&nbsp;the&nbsp;model&nbsp;than&nbsp;just&nbsp;the&nbsp;top&nbsp;generated&nbsp;value,<br>
&nbsp;&nbsp;&nbsp;&nbsp;3.&nbsp;are&nbsp;building&nbsp;chains&nbsp;that&nbsp;are&nbsp;agnostic&nbsp;to&nbsp;the&nbsp;underlying&nbsp;language&nbsp;model<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;type&nbsp;(e.g.,&nbsp;pure&nbsp;text&nbsp;completion&nbsp;models&nbsp;vs&nbsp;chat&nbsp;models).<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompts:&nbsp;List&nbsp;of&nbsp;PromptValues.&nbsp;A&nbsp;PromptValue&nbsp;is&nbsp;an&nbsp;object&nbsp;that&nbsp;can&nbsp;be<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;converted&nbsp;to&nbsp;match&nbsp;the&nbsp;format&nbsp;of&nbsp;any&nbsp;language&nbsp;model&nbsp;(string&nbsp;for&nbsp;pure<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;text&nbsp;generation&nbsp;models&nbsp;and&nbsp;BaseMessages&nbsp;for&nbsp;chat&nbsp;models).<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop:&nbsp;Stop&nbsp;words&nbsp;to&nbsp;use&nbsp;when&nbsp;generating.&nbsp;Model&nbsp;output&nbsp;is&nbsp;cut&nbsp;off&nbsp;at&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;first&nbsp;occurrence&nbsp;of&nbsp;any&nbsp;of&nbsp;these&nbsp;substrings.<br>
&nbsp;&nbsp;&nbsp;&nbsp;callbacks:&nbsp;Callbacks&nbsp;to&nbsp;pass&nbsp;through.&nbsp;Used&nbsp;for&nbsp;executing&nbsp;additional<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;functionality,&nbsp;such&nbsp;as&nbsp;logging&nbsp;or&nbsp;streaming,&nbsp;throughout&nbsp;generation.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Arbitrary&nbsp;additional&nbsp;keyword&nbsp;arguments.&nbsp;These&nbsp;are&nbsp;usually&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;the&nbsp;model&nbsp;provider&nbsp;API&nbsp;call.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;LLMResult,&nbsp;which&nbsp;contains&nbsp;a&nbsp;list&nbsp;of&nbsp;candidate&nbsp;Generations&nbsp;for&nbsp;each&nbsp;input<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;and&nbsp;additional&nbsp;model&nbsp;provider-specific&nbsp;output.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-invoke"><strong>invoke</strong></a>(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -&gt; 'BaseMessage'</dt><dd><tt>Transform&nbsp;a&nbsp;single&nbsp;input&nbsp;into&nbsp;an&nbsp;output.&nbsp;Override&nbsp;to&nbsp;implement.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;invoking&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;config&nbsp;supports&nbsp;standard&nbsp;keys&nbsp;like&nbsp;'tags',&nbsp;'metadata'&nbsp;for&nbsp;tracing<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;purposes,&nbsp;'max_concurrency'&nbsp;for&nbsp;controlling&nbsp;how&nbsp;much&nbsp;work&nbsp;to&nbsp;do<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;parallel,&nbsp;and&nbsp;other&nbsp;keys.&nbsp;Please&nbsp;refer&nbsp;to&nbsp;the&nbsp;RunnableConfig<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;more&nbsp;details.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;output&nbsp;of&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-predict"><strong>predict</strong></a>(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'str'</dt><dd><tt>[*Deprecated*]&nbsp;<br>
&nbsp;<br>
Notes<br>
-----<br>
..&nbsp;deprecated::&nbsp;langchain-core==0.1.7<br>
&nbsp;&nbsp;&nbsp;Use&nbsp;invoke&nbsp;instead.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-predict_messages"><strong>predict_messages</strong></a>(self, messages: 'List[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'BaseMessage'</dt><dd><tt>[*Deprecated*]&nbsp;<br>
&nbsp;<br>
Notes<br>
-----<br>
..&nbsp;deprecated::&nbsp;langchain-core==0.1.7<br>
&nbsp;&nbsp;&nbsp;Use&nbsp;invoke&nbsp;instead.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-stream"><strong>stream</strong></a>(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[List[str]]' = None, **kwargs: 'Any') -&gt; 'Iterator[BaseMessageChunk]'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;stream,&nbsp;which&nbsp;calls&nbsp;invoke.<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;support&nbsp;streaming&nbsp;output.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;Runnable.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;output&nbsp;of&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<hr>
Class methods inherited from <a href="langchain_core.language_models.chat_models.html#BaseChatModel">langchain_core.language_models.chat_models.BaseChatModel</a>:<br>
<dl><dt><a name="ChatGoogleGenerativeAI-raise_deprecation"><strong>raise_deprecation</strong></a>(values: 'Dict') -&gt; 'Dict'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Raise&nbsp;deprecation&nbsp;warning&nbsp;if&nbsp;callback_manager&nbsp;is&nbsp;used.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;values&nbsp;(Dict):&nbsp;Values&nbsp;to&nbsp;validate.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;Dict:&nbsp;Validated&nbsp;values.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;DeprecationWarning:&nbsp;If&nbsp;callback_manager&nbsp;is&nbsp;used.</tt></dd></dl>

<hr>
Readonly properties inherited from <a href="langchain_core.language_models.chat_models.html#BaseChatModel">langchain_core.language_models.chat_models.BaseChatModel</a>:<br>
<dl><dt><strong>OutputType</strong></dt>
<dd><tt>Get&nbsp;the&nbsp;output&nbsp;type&nbsp;for&nbsp;this&nbsp;runnable.</tt></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="langchain_core.language_models.chat_models.html#BaseChatModel">langchain_core.language_models.chat_models.BaseChatModel</a>:<br>
<dl><dt><strong>__orig_bases__</strong> = (langchain_core.language_models.base.BaseLanguageModel[langchain_core.messages.base.BaseMessage], &lt;class 'abc.ABC'&gt;)</dl>

<hr>
Methods inherited from <a href="langchain_core.language_models.base.html#BaseLanguageModel">langchain_core.language_models.base.BaseLanguageModel</a>:<br>
<dl><dt><a name="ChatGoogleGenerativeAI-get_num_tokens_from_messages"><strong>get_num_tokens_from_messages</strong></a>(self, messages: 'List[BaseMessage]') -&gt; 'int'</dt><dd><tt>Get&nbsp;the&nbsp;number&nbsp;of&nbsp;tokens&nbsp;in&nbsp;the&nbsp;messages.<br>
&nbsp;<br>
Useful&nbsp;for&nbsp;checking&nbsp;if&nbsp;an&nbsp;input&nbsp;fits&nbsp;in&nbsp;a&nbsp;model's&nbsp;context&nbsp;window.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;messages:&nbsp;The&nbsp;message&nbsp;inputs&nbsp;to&nbsp;tokenize.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;sum&nbsp;of&nbsp;the&nbsp;number&nbsp;of&nbsp;tokens&nbsp;across&nbsp;the&nbsp;messages.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-get_token_ids"><strong>get_token_ids</strong></a>(self, text: 'str') -&gt; 'List[int]'</dt><dd><tt>Return&nbsp;the&nbsp;ordered&nbsp;ids&nbsp;of&nbsp;the&nbsp;tokens&nbsp;in&nbsp;a&nbsp;text.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;text:&nbsp;The&nbsp;string&nbsp;input&nbsp;to&nbsp;tokenize.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;list&nbsp;of&nbsp;ids&nbsp;corresponding&nbsp;to&nbsp;the&nbsp;tokens&nbsp;in&nbsp;the&nbsp;text,&nbsp;in&nbsp;order&nbsp;they&nbsp;occur<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;the&nbsp;text.</tt></dd></dl>

<hr>
Class methods inherited from <a href="langchain_core.language_models.base.html#BaseLanguageModel">langchain_core.language_models.base.BaseLanguageModel</a>:<br>
<dl><dt><a name="ChatGoogleGenerativeAI-set_verbose"><strong>set_verbose</strong></a>(verbose: 'Optional[bool]') -&gt; 'bool'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>If&nbsp;verbose&nbsp;is&nbsp;None,&nbsp;set&nbsp;it.<br>
&nbsp;<br>
This&nbsp;allows&nbsp;users&nbsp;to&nbsp;pass&nbsp;in&nbsp;None&nbsp;as&nbsp;verbose&nbsp;to&nbsp;access&nbsp;the&nbsp;global&nbsp;setting.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;verbose:&nbsp;The&nbsp;verbosity&nbsp;setting&nbsp;to&nbsp;use.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;verbosity&nbsp;setting&nbsp;to&nbsp;use.</tt></dd></dl>

<hr>
Readonly properties inherited from <a href="langchain_core.language_models.base.html#BaseLanguageModel">langchain_core.language_models.base.BaseLanguageModel</a>:<br>
<dl><dt><strong>InputType</strong></dt>
<dd><tt>Get&nbsp;the&nbsp;input&nbsp;type&nbsp;for&nbsp;this&nbsp;runnable.</tt></dd>
</dl>
<hr>
Methods inherited from <a href="langchain_core.runnables.base.html#RunnableSerializable">langchain_core.runnables.base.RunnableSerializable</a>:<br>
<dl><dt><a name="ChatGoogleGenerativeAI-configurable_alternatives"><strong>configurable_alternatives</strong></a>(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]') -&gt; 'RunnableSerializable[Input, Output]'</dt><dd><tt>Configure&nbsp;alternatives&nbsp;for&nbsp;Runnables&nbsp;that&nbsp;can&nbsp;be&nbsp;set&nbsp;at&nbsp;runtime.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;which:&nbsp;The&nbsp;ConfigurableField&nbsp;instance&nbsp;that&nbsp;will&nbsp;be&nbsp;used&nbsp;to&nbsp;select&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;alternative.<br>
&nbsp;&nbsp;&nbsp;&nbsp;default_key:&nbsp;The&nbsp;default&nbsp;key&nbsp;to&nbsp;use&nbsp;if&nbsp;no&nbsp;alternative&nbsp;is&nbsp;selected.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;"default".<br>
&nbsp;&nbsp;&nbsp;&nbsp;prefix_keys:&nbsp;Whether&nbsp;to&nbsp;prefix&nbsp;the&nbsp;keys&nbsp;with&nbsp;the&nbsp;ConfigurableField&nbsp;id.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;False.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;A&nbsp;dictionary&nbsp;of&nbsp;keys&nbsp;to&nbsp;Runnable&nbsp;instances&nbsp;or&nbsp;callables&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;Runnable&nbsp;instances.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;with&nbsp;the&nbsp;alternatives&nbsp;configured.<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_anthropic&nbsp;import&nbsp;ChatAnthropic<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables.utils&nbsp;import&nbsp;ConfigurableField<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_openai&nbsp;import&nbsp;ChatOpenAI<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;ChatAnthropic(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model_name="claude-3-sonnet-20240229"<br>
&nbsp;&nbsp;&nbsp;&nbsp;).<a href="#ChatGoogleGenerativeAI-configurable_alternatives">configurable_alternatives</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ConfigurableField(id="llm"),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;default_key="anthropic",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;openai=ChatOpenAI()<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;uses&nbsp;the&nbsp;default&nbsp;model&nbsp;ChatAnthropic<br>
&nbsp;&nbsp;&nbsp;&nbsp;print(model.<a href="#ChatGoogleGenerativeAI-invoke">invoke</a>("which&nbsp;organization&nbsp;created&nbsp;you?").content)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;uses&nbsp;ChatOpenAI<br>
&nbsp;&nbsp;&nbsp;&nbsp;print(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model.<a href="#ChatGoogleGenerativeAI-with_config">with_config</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;configurable={"llm":&nbsp;"openai"}<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;).<a href="#ChatGoogleGenerativeAI-invoke">invoke</a>("which&nbsp;organization&nbsp;created&nbsp;you?").content<br>
&nbsp;&nbsp;&nbsp;&nbsp;)</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-configurable_fields"><strong>configurable_fields</strong></a>(self, **kwargs: 'AnyConfigurableField') -&gt; 'RunnableSerializable[Input, Output]'</dt><dd><tt>Configure&nbsp;particular&nbsp;Runnable&nbsp;fields&nbsp;at&nbsp;runtime.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;A&nbsp;dictionary&nbsp;of&nbsp;ConfigurableField&nbsp;instances&nbsp;to&nbsp;configure.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;with&nbsp;the&nbsp;fields&nbsp;configured.<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;ConfigurableField<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_openai&nbsp;import&nbsp;ChatOpenAI<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;model&nbsp;=&nbsp;ChatOpenAI(max_tokens=20).<a href="#ChatGoogleGenerativeAI-configurable_fields">configurable_fields</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;max_tokens=ConfigurableField(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;id="output_token_number",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;name="Max&nbsp;tokens&nbsp;in&nbsp;the&nbsp;output",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;description="The&nbsp;maximum&nbsp;number&nbsp;of&nbsp;tokens&nbsp;in&nbsp;the&nbsp;output",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;max_tokens&nbsp;=&nbsp;20<br>
&nbsp;&nbsp;&nbsp;&nbsp;print(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"max_tokens_20:&nbsp;",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;model.<a href="#ChatGoogleGenerativeAI-invoke">invoke</a>("tell&nbsp;me&nbsp;something&nbsp;about&nbsp;chess").content<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;max_tokens&nbsp;=&nbsp;200<br>
&nbsp;&nbsp;&nbsp;&nbsp;print("max_tokens_200:&nbsp;",&nbsp;model.<a href="#ChatGoogleGenerativeAI-with_config">with_config</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;configurable={"output_token_number":&nbsp;200}<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;).<a href="#ChatGoogleGenerativeAI-invoke">invoke</a>("tell&nbsp;me&nbsp;something&nbsp;about&nbsp;chess").content<br>
&nbsp;&nbsp;&nbsp;&nbsp;)</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-to_json"><strong>to_json</strong></a>(self) -&gt; 'Union[SerializedConstructor, SerializedNotImplemented]'</dt><dd><tt>Serialize&nbsp;the&nbsp;Runnable&nbsp;to&nbsp;JSON.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;JSON-serializable&nbsp;representation&nbsp;of&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<hr>
Methods inherited from <a href="langchain_core.load.serializable.html#Serializable">langchain_core.load.serializable.Serializable</a>:<br>
<dl><dt><a name="ChatGoogleGenerativeAI-__repr_args__"><strong>__repr_args__</strong></a>(self) -&gt; Any</dt><dd><tt>Returns&nbsp;the&nbsp;attributes&nbsp;to&nbsp;show&nbsp;in&nbsp;__str__,&nbsp;__repr__,&nbsp;and&nbsp;__pretty__&nbsp;this&nbsp;is&nbsp;generally&nbsp;overridden.<br>
&nbsp;<br>
Can&nbsp;either&nbsp;return:<br>
*&nbsp;name&nbsp;-&nbsp;value&nbsp;pairs,&nbsp;e.g.:&nbsp;`[('foo_name',&nbsp;'foo'),&nbsp;('bar_name',&nbsp;['b',&nbsp;'a',&nbsp;'r'])]`<br>
*&nbsp;or,&nbsp;just&nbsp;values,&nbsp;e.g.:&nbsp;`[(None,&nbsp;'foo'),&nbsp;(None,&nbsp;['b',&nbsp;'a',&nbsp;'r'])]`</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-to_json_not_implemented"><strong>to_json_not_implemented</strong></a>(self) -&gt; langchain_core.load.serializable.SerializedNotImplemented</dt></dl>

<hr>
Class methods inherited from <a href="langchain_core.load.serializable.html#Serializable">langchain_core.load.serializable.Serializable</a>:<br>
<dl><dt><a name="ChatGoogleGenerativeAI-get_lc_namespace"><strong>get_lc_namespace</strong></a>() -&gt; List[str]<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Get&nbsp;the&nbsp;namespace&nbsp;of&nbsp;the&nbsp;langchain&nbsp;object.<br>
&nbsp;<br>
For&nbsp;example,&nbsp;if&nbsp;the&nbsp;class&nbsp;is&nbsp;`langchain.llms.openai.OpenAI`,&nbsp;then&nbsp;the<br>
namespace&nbsp;is&nbsp;["langchain",&nbsp;"llms",&nbsp;"openai"]</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-lc_id"><strong>lc_id</strong></a>() -&gt; List[str]<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>A&nbsp;unique&nbsp;identifier&nbsp;for&nbsp;this&nbsp;class&nbsp;for&nbsp;serialization&nbsp;purposes.<br>
&nbsp;<br>
The&nbsp;unique&nbsp;identifier&nbsp;is&nbsp;a&nbsp;list&nbsp;of&nbsp;strings&nbsp;that&nbsp;describes&nbsp;the&nbsp;path<br>
to&nbsp;the&nbsp;object.<br>
For&nbsp;example,&nbsp;for&nbsp;the&nbsp;class&nbsp;`langchain.llms.openai.OpenAI`,&nbsp;the&nbsp;id&nbsp;is<br>
["langchain",&nbsp;"llms",&nbsp;"openai",&nbsp;"OpenAI"].</tt></dd></dl>

<hr>
Readonly properties inherited from <a href="langchain_core.load.serializable.html#Serializable">langchain_core.load.serializable.Serializable</a>:<br>
<dl><dt><strong>lc_attributes</strong></dt>
<dd><tt>List&nbsp;of&nbsp;attribute&nbsp;names&nbsp;that&nbsp;should&nbsp;be&nbsp;included&nbsp;in&nbsp;the&nbsp;serialized&nbsp;kwargs.<br>
&nbsp;<br>
These&nbsp;attributes&nbsp;must&nbsp;be&nbsp;accepted&nbsp;by&nbsp;the&nbsp;constructor.<br>
Default&nbsp;is&nbsp;an&nbsp;empty&nbsp;dictionary.</tt></dd>
</dl>
<hr>
Methods inherited from <a href="pydantic.v1.main.html#BaseModel">pydantic.v1.main.BaseModel</a>:<br>
<dl><dt><a name="ChatGoogleGenerativeAI-__eq__"><strong>__eq__</strong></a>(self, other: Any) -&gt; bool</dt><dd><tt>Return&nbsp;self==value.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-__getstate__"><strong>__getstate__</strong></a>(self) -&gt; 'DictAny'</dt></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-__iter__"><strong>__iter__</strong></a>(self) -&gt; 'TupleGenerator'</dt><dd><tt>so&nbsp;`<a href="#ChatGoogleGenerativeAI-dict">dict</a>(model)`&nbsp;works</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-__setattr__"><strong>__setattr__</strong></a>(self, name, value)</dt><dd><tt>Implement&nbsp;setattr(self,&nbsp;name,&nbsp;value).</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-__setstate__"><strong>__setstate__</strong></a>(self, state: 'DictAny') -&gt; None</dt></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-copy"><strong>copy</strong></a>(self: 'Model', *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, update: Optional[ForwardRef('DictStrAny')] = None, deep: bool = False) -&gt; 'Model'</dt><dd><tt>Duplicate&nbsp;a&nbsp;model,&nbsp;optionally&nbsp;choose&nbsp;which&nbsp;fields&nbsp;to&nbsp;include,&nbsp;exclude&nbsp;and&nbsp;change.<br>
&nbsp;<br>
:param&nbsp;include:&nbsp;fields&nbsp;to&nbsp;include&nbsp;in&nbsp;new&nbsp;model<br>
:param&nbsp;exclude:&nbsp;fields&nbsp;to&nbsp;exclude&nbsp;from&nbsp;new&nbsp;model,&nbsp;as&nbsp;with&nbsp;values&nbsp;this&nbsp;takes&nbsp;precedence&nbsp;over&nbsp;include<br>
:param&nbsp;update:&nbsp;values&nbsp;to&nbsp;change/add&nbsp;in&nbsp;the&nbsp;new&nbsp;model.&nbsp;Note:&nbsp;the&nbsp;data&nbsp;is&nbsp;not&nbsp;validated&nbsp;before&nbsp;creating<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;new&nbsp;model:&nbsp;you&nbsp;should&nbsp;trust&nbsp;this&nbsp;data<br>
:param&nbsp;deep:&nbsp;set&nbsp;to&nbsp;`True`&nbsp;to&nbsp;make&nbsp;a&nbsp;deep&nbsp;copy&nbsp;of&nbsp;the&nbsp;model<br>
:return:&nbsp;new&nbsp;model&nbsp;instance</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-json"><strong>json</strong></a>(self, *, include: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, exclude: Union[ForwardRef('AbstractSetIntStr'), ForwardRef('MappingIntStrAny'), NoneType] = None, by_alias: bool = False, skip_defaults: Optional[bool] = None, exclude_unset: bool = False, exclude_defaults: bool = False, exclude_none: bool = False, encoder: Optional[Callable[[Any], Any]] = None, models_as_dict: bool = True, **dumps_kwargs: Any) -&gt; str</dt><dd><tt>Generate&nbsp;a&nbsp;JSON&nbsp;representation&nbsp;of&nbsp;the&nbsp;model,&nbsp;`include`&nbsp;and&nbsp;`exclude`&nbsp;arguments&nbsp;as&nbsp;per&nbsp;`<a href="#ChatGoogleGenerativeAI-dict">dict</a>()`.<br>
&nbsp;<br>
`encoder`&nbsp;is&nbsp;an&nbsp;optional&nbsp;function&nbsp;to&nbsp;supply&nbsp;as&nbsp;`default`&nbsp;to&nbsp;json.dumps(),&nbsp;other&nbsp;arguments&nbsp;as&nbsp;per&nbsp;`json.dumps()`.</tt></dd></dl>

<hr>
Class methods inherited from <a href="pydantic.v1.main.html#BaseModel">pydantic.v1.main.BaseModel</a>:<br>
<dl><dt><a name="ChatGoogleGenerativeAI-__get_validators__"><strong>__get_validators__</strong></a>() -&gt; 'CallableGenerator'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-__try_update_forward_refs__"><strong>__try_update_forward_refs__</strong></a>(**localns: Any) -&gt; None<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Same&nbsp;as&nbsp;update_forward_refs&nbsp;but&nbsp;will&nbsp;not&nbsp;raise&nbsp;exception<br>
when&nbsp;forward&nbsp;references&nbsp;are&nbsp;not&nbsp;defined.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-construct"><strong>construct</strong></a>(_fields_set: Optional[ForwardRef('SetStr')] = None, **values: Any) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Creates&nbsp;a&nbsp;new&nbsp;model&nbsp;setting&nbsp;__dict__&nbsp;and&nbsp;__fields_set__&nbsp;from&nbsp;trusted&nbsp;or&nbsp;pre-validated&nbsp;data.<br>
Default&nbsp;values&nbsp;are&nbsp;respected,&nbsp;but&nbsp;no&nbsp;other&nbsp;validation&nbsp;is&nbsp;performed.<br>
Behaves&nbsp;as&nbsp;if&nbsp;`Config.extra&nbsp;=&nbsp;'allow'`&nbsp;was&nbsp;set&nbsp;since&nbsp;it&nbsp;adds&nbsp;all&nbsp;passed&nbsp;values</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-from_orm"><strong>from_orm</strong></a>(obj: Any) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-parse_file"><strong>parse_file</strong></a>(path: Union[str, pathlib.Path], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-parse_obj"><strong>parse_obj</strong></a>(obj: Any) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-parse_raw"><strong>parse_raw</strong></a>(b: Union[str, bytes], *, content_type: str = None, encoding: str = 'utf8', proto: pydantic.v1.parse.Protocol = None, allow_pickle: bool = False) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-schema"><strong>schema</strong></a>(by_alias: bool = True, ref_template: str = '#/definitions/{model}') -&gt; 'DictStrAny'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-schema_json"><strong>schema_json</strong></a>(*, by_alias: bool = True, ref_template: str = '#/definitions/{model}', **dumps_kwargs: Any) -&gt; str<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-update_forward_refs"><strong>update_forward_refs</strong></a>(**localns: Any) -&gt; None<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>Try&nbsp;to&nbsp;update&nbsp;ForwardRefs&nbsp;on&nbsp;fields&nbsp;based&nbsp;on&nbsp;this&nbsp;Model,&nbsp;globalns&nbsp;and&nbsp;localns.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-validate"><strong>validate</strong></a>(value: Any) -&gt; 'Model'<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<hr>
Data descriptors inherited from <a href="pydantic.v1.main.html#BaseModel">pydantic.v1.main.BaseModel</a>:<br>
<dl><dt><strong>__dict__</strong></dt>
<dd><tt>dictionary&nbsp;for&nbsp;instance&nbsp;variables&nbsp;(if&nbsp;defined)</tt></dd>
</dl>
<dl><dt><strong>__fields_set__</strong></dt>
</dl>
<hr>
Methods inherited from <a href="pydantic.v1.utils.html#Representation">pydantic.v1.utils.Representation</a>:<br>
<dl><dt><a name="ChatGoogleGenerativeAI-__pretty__"><strong>__pretty__</strong></a>(self, fmt: Callable[[Any], Any], **kwargs: Any) -&gt; Generator[Any, NoneType, NoneType]</dt><dd><tt>Used&nbsp;by&nbsp;devtools&nbsp;(<a href="https://python-devtools.helpmanual.io/">https://python-devtools.helpmanual.io/</a>)&nbsp;to&nbsp;provide&nbsp;a&nbsp;human&nbsp;readable&nbsp;representations&nbsp;of&nbsp;objects</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-__repr__"><strong>__repr__</strong></a>(self) -&gt; str</dt><dd><tt>Return&nbsp;repr(self).</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-__repr_name__"><strong>__repr_name__</strong></a>(self) -&gt; str</dt><dd><tt>Name&nbsp;of&nbsp;the&nbsp;instance's&nbsp;class,&nbsp;used&nbsp;in&nbsp;__repr__.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-__repr_str__"><strong>__repr_str__</strong></a>(self, join_str: str) -&gt; str</dt></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-__rich_repr__"><strong>__rich_repr__</strong></a>(self) -&gt; 'RichReprResult'</dt><dd><tt>Get&nbsp;fields&nbsp;for&nbsp;Rich&nbsp;library</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-__str__"><strong>__str__</strong></a>(self) -&gt; str</dt><dd><tt>Return&nbsp;str(self).</tt></dd></dl>

<hr>
Methods inherited from <a href="langchain_core.runnables.base.html#Runnable">langchain_core.runnables.base.Runnable</a>:<br>
<dl><dt><a name="ChatGoogleGenerativeAI-__or__"><strong>__or__</strong></a>(self, other: 'Union[Runnable[Any, Other], Callable[[Any], Other], Callable[[Iterator[Any]], Iterator[Other]], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -&gt; 'RunnableSerializable[Input, Other]'</dt><dd><tt>Compose&nbsp;this&nbsp;Runnable&nbsp;with&nbsp;another&nbsp;object&nbsp;to&nbsp;create&nbsp;a&nbsp;RunnableSequence.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-__ror__"><strong>__ror__</strong></a>(self, other: 'Union[Runnable[Other, Any], Callable[[Other], Any], Callable[[Iterator[Other]], Iterator[Any]], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -&gt; 'RunnableSerializable[Other, Output]'</dt><dd><tt>Compose&nbsp;this&nbsp;Runnable&nbsp;with&nbsp;another&nbsp;object&nbsp;to&nbsp;create&nbsp;a&nbsp;RunnableSequence.</tt></dd></dl>

<dl><dt>async <a name="ChatGoogleGenerativeAI-abatch"><strong>abatch</strong></a>(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -&gt; 'List[Output]'</dt><dd><tt>Default&nbsp;implementation&nbsp;runs&nbsp;ainvoke&nbsp;in&nbsp;parallel&nbsp;using&nbsp;asyncio.gather.<br>
&nbsp;<br>
The&nbsp;default&nbsp;implementation&nbsp;of&nbsp;batch&nbsp;works&nbsp;well&nbsp;for&nbsp;IO&nbsp;bound&nbsp;runnables.<br>
&nbsp;<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;batch&nbsp;more&nbsp;efficiently;<br>
e.g.,&nbsp;if&nbsp;the&nbsp;underlying&nbsp;Runnable&nbsp;uses&nbsp;an&nbsp;API&nbsp;which&nbsp;supports&nbsp;a&nbsp;batch&nbsp;mode.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;inputs:&nbsp;A&nbsp;list&nbsp;of&nbsp;inputs&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;invoking&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;config&nbsp;supports&nbsp;standard&nbsp;keys&nbsp;like&nbsp;'tags',&nbsp;'metadata'&nbsp;for&nbsp;tracing<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;purposes,&nbsp;'max_concurrency'&nbsp;for&nbsp;controlling&nbsp;how&nbsp;much&nbsp;work&nbsp;to&nbsp;do<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;parallel,&nbsp;and&nbsp;other&nbsp;keys.&nbsp;Please&nbsp;refer&nbsp;to&nbsp;the&nbsp;RunnableConfig<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;more&nbsp;details.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;return_exceptions:&nbsp;Whether&nbsp;to&nbsp;return&nbsp;exceptions&nbsp;instead&nbsp;of&nbsp;raising&nbsp;them.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;False.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;list&nbsp;of&nbsp;outputs&nbsp;from&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<dl><dt>async <a name="ChatGoogleGenerativeAI-abatch_as_completed"><strong>abatch_as_completed</strong></a>(self, inputs: 'Sequence[Input]', config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -&gt; 'AsyncIterator[Tuple[int, Union[Output, Exception]]]'</dt><dd><tt>Run&nbsp;ainvoke&nbsp;in&nbsp;parallel&nbsp;on&nbsp;a&nbsp;list&nbsp;of&nbsp;inputs,<br>
yielding&nbsp;results&nbsp;as&nbsp;they&nbsp;complete.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;inputs:&nbsp;A&nbsp;list&nbsp;of&nbsp;inputs&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;invoking&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;config&nbsp;supports&nbsp;standard&nbsp;keys&nbsp;like&nbsp;'tags',&nbsp;'metadata'&nbsp;for&nbsp;tracing<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;purposes,&nbsp;'max_concurrency'&nbsp;for&nbsp;controlling&nbsp;how&nbsp;much&nbsp;work&nbsp;to&nbsp;do<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;parallel,&nbsp;and&nbsp;other&nbsp;keys.&nbsp;Please&nbsp;refer&nbsp;to&nbsp;the&nbsp;RunnableConfig<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;for&nbsp;more&nbsp;details.&nbsp;Defaults&nbsp;to&nbsp;None.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;return_exceptions:&nbsp;Whether&nbsp;to&nbsp;return&nbsp;exceptions&nbsp;instead&nbsp;of&nbsp;raising&nbsp;them.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;False.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;tuple&nbsp;of&nbsp;the&nbsp;index&nbsp;of&nbsp;the&nbsp;input&nbsp;and&nbsp;the&nbsp;output&nbsp;from&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-as_tool"><strong>as_tool</strong></a>(self, args_schema: 'Optional[Type[BaseModel]]' = None, *, name: 'Optional[str]' = None, description: 'Optional[str]' = None, arg_types: 'Optional[Dict[str, Type]]' = None) -&gt; 'BaseTool'</dt><dd><tt>[*Beta*]&nbsp;Create&nbsp;a&nbsp;BaseTool&nbsp;from&nbsp;a&nbsp;Runnable.<br>
&nbsp;<br>
``as_tool``&nbsp;will&nbsp;instantiate&nbsp;a&nbsp;BaseTool&nbsp;with&nbsp;a&nbsp;name,&nbsp;description,&nbsp;and<br>
``args_schema``&nbsp;from&nbsp;a&nbsp;Runnable.&nbsp;Where&nbsp;possible,&nbsp;schemas&nbsp;are&nbsp;inferred<br>
from&nbsp;``runnable.get_input_schema``.&nbsp;Alternatively&nbsp;(e.g.,&nbsp;if&nbsp;the<br>
Runnable&nbsp;takes&nbsp;a&nbsp;dict&nbsp;as&nbsp;input&nbsp;and&nbsp;the&nbsp;specific&nbsp;dict&nbsp;keys&nbsp;are&nbsp;not&nbsp;typed),<br>
the&nbsp;schema&nbsp;can&nbsp;be&nbsp;specified&nbsp;directly&nbsp;with&nbsp;``args_schema``.&nbsp;You&nbsp;can&nbsp;also<br>
pass&nbsp;``arg_types``&nbsp;to&nbsp;just&nbsp;specify&nbsp;the&nbsp;required&nbsp;arguments&nbsp;and&nbsp;their&nbsp;types.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;args_schema:&nbsp;The&nbsp;schema&nbsp;for&nbsp;the&nbsp;tool.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;name:&nbsp;The&nbsp;name&nbsp;of&nbsp;the&nbsp;tool.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;description:&nbsp;The&nbsp;description&nbsp;of&nbsp;the&nbsp;tool.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;arg_types:&nbsp;A&nbsp;dictionary&nbsp;of&nbsp;argument&nbsp;names&nbsp;to&nbsp;types.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;BaseTool&nbsp;instance.<br>
&nbsp;<br>
Typed&nbsp;dict&nbsp;input:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;typing&nbsp;import&nbsp;List<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;typing_extensions&nbsp;import&nbsp;TypedDict<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;Args(TypedDict):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a:&nbsp;int<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b:&nbsp;List[int]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;f(x:&nbsp;Args)&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;str(x["a"]&nbsp;*&nbsp;max(x["b"]))<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(f)<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool&nbsp;=&nbsp;runnable.<a href="#ChatGoogleGenerativeAI-as_tool">as_tool</a>()<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool.<a href="#ChatGoogleGenerativeAI-invoke">invoke</a>({"a":&nbsp;3,&nbsp;"b":&nbsp;[1,&nbsp;2]})<br>
&nbsp;<br>
``dict``&nbsp;input,&nbsp;specifying&nbsp;schema&nbsp;via&nbsp;``args_schema``:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;typing&nbsp;import&nbsp;Any,&nbsp;Dict,&nbsp;List<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.pydantic_v1&nbsp;import&nbsp;BaseModel,&nbsp;Field<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;f(x:&nbsp;Dict[str,&nbsp;Any])&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;str(x["a"]&nbsp;*&nbsp;max(x["b"]))<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;class&nbsp;FSchema(BaseModel):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Apply&nbsp;a&nbsp;function&nbsp;to&nbsp;an&nbsp;integer&nbsp;and&nbsp;list&nbsp;of&nbsp;integers."""<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;a:&nbsp;int&nbsp;=&nbsp;Field(...,&nbsp;description="Integer")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;b:&nbsp;List[int]&nbsp;=&nbsp;Field(...,&nbsp;description="List&nbsp;of&nbsp;ints")<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(f)<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool&nbsp;=&nbsp;runnable.<a href="#ChatGoogleGenerativeAI-as_tool">as_tool</a>(FSchema)<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool.<a href="#ChatGoogleGenerativeAI-invoke">invoke</a>({"a":&nbsp;3,&nbsp;"b":&nbsp;[1,&nbsp;2]})<br>
&nbsp;<br>
``dict``&nbsp;input,&nbsp;specifying&nbsp;schema&nbsp;via&nbsp;``arg_types``:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;typing&nbsp;import&nbsp;Any,&nbsp;Dict,&nbsp;List<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;f(x:&nbsp;Dict[str,&nbsp;Any])&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;str(x["a"]&nbsp;*&nbsp;max(x["b"]))<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(f)<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool&nbsp;=&nbsp;runnable.<a href="#ChatGoogleGenerativeAI-as_tool">as_tool</a>(arg_types={"a":&nbsp;int,&nbsp;"b":&nbsp;List[int]})<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool.<a href="#ChatGoogleGenerativeAI-invoke">invoke</a>({"a":&nbsp;3,&nbsp;"b":&nbsp;[1,&nbsp;2]})<br>
&nbsp;<br>
String&nbsp;input:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;f(x:&nbsp;str)&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;x&nbsp;+&nbsp;"a"<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;g(x:&nbsp;str)&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;x&nbsp;+&nbsp;"z"<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(f)&nbsp;|&nbsp;g<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool&nbsp;=&nbsp;runnable.<a href="#ChatGoogleGenerativeAI-as_tool">as_tool</a>()<br>
&nbsp;&nbsp;&nbsp;&nbsp;as_tool.<a href="#ChatGoogleGenerativeAI-invoke">invoke</a>("b")<br>
&nbsp;<br>
..&nbsp;versionadded::&nbsp;0.2.14<br>
&nbsp;<br>
Notes<br>
-----<br>
..&nbsp;beta::<br>
&nbsp;&nbsp;&nbsp;This&nbsp;API&nbsp;is&nbsp;in&nbsp;beta&nbsp;and&nbsp;may&nbsp;change&nbsp;in&nbsp;the&nbsp;future.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-assign"><strong>assign</strong></a>(self, **kwargs: 'Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any], Mapping[str, Union[Runnable[Dict[str, Any], Any], Callable[[Dict[str, Any]], Any]]]]') -&gt; 'RunnableSerializable[Any, Any]'</dt><dd><tt>Assigns&nbsp;new&nbsp;fields&nbsp;to&nbsp;the&nbsp;dict&nbsp;output&nbsp;of&nbsp;this&nbsp;Runnable.<br>
Returns&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_community.llms.fake&nbsp;import&nbsp;FakeStreamingListLLM<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.output_parsers&nbsp;import&nbsp;StrOutputParser<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.prompts&nbsp;import&nbsp;SystemMessagePromptTemplate<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;Runnable<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;operator&nbsp;import&nbsp;itemgetter<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;prompt&nbsp;=&nbsp;(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;SystemMessagePromptTemplate.from_template("You&nbsp;are&nbsp;a&nbsp;nice&nbsp;assistant.")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;+&nbsp;"{question}"<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;llm&nbsp;=&nbsp;FakeStreamingListLLM(responses=["foo-lish"])<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain:&nbsp;Runnable&nbsp;=&nbsp;prompt&nbsp;|&nbsp;llm&nbsp;|&nbsp;{"str":&nbsp;StrOutputParser()}<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain_with_assign&nbsp;=&nbsp;chain.<a href="#ChatGoogleGenerativeAI-assign">assign</a>(hello=itemgetter("str")&nbsp;|&nbsp;llm)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;print(chain_with_assign.input_schema.<a href="#ChatGoogleGenerativeAI-schema">schema</a>())<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;{'title':&nbsp;'PromptInput',&nbsp;'type':&nbsp;'object',&nbsp;'properties':<br>
&nbsp;&nbsp;&nbsp;&nbsp;{'question':&nbsp;{'title':&nbsp;'Question',&nbsp;'type':&nbsp;'string'}}}<br>
&nbsp;&nbsp;&nbsp;&nbsp;print(chain_with_assign.output_schema.<a href="#ChatGoogleGenerativeAI-schema">schema</a>())&nbsp;#<br>
&nbsp;&nbsp;&nbsp;&nbsp;{'title':&nbsp;'RunnableSequenceOutput',&nbsp;'type':&nbsp;'object',&nbsp;'properties':<br>
&nbsp;&nbsp;&nbsp;&nbsp;{'str':&nbsp;{'title':&nbsp;'Str',<br>
&nbsp;&nbsp;&nbsp;&nbsp;'type':&nbsp;'string'},&nbsp;'hello':&nbsp;{'title':&nbsp;'Hello',&nbsp;'type':&nbsp;'string'}}}</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-astream_events"><strong>astream_events</strong></a>(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, version: "Literal['v1', 'v2']", include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'AsyncIterator[StreamEvent]'</dt><dd><tt>[*Beta*]&nbsp;Generate&nbsp;a&nbsp;stream&nbsp;of&nbsp;events.<br>
&nbsp;<br>
Use&nbsp;to&nbsp;create&nbsp;an&nbsp;iterator&nbsp;over&nbsp;StreamEvents&nbsp;that&nbsp;provide&nbsp;real-time&nbsp;information<br>
about&nbsp;the&nbsp;progress&nbsp;of&nbsp;the&nbsp;Runnable,&nbsp;including&nbsp;StreamEvents&nbsp;from&nbsp;intermediate<br>
results.<br>
&nbsp;<br>
A&nbsp;StreamEvent&nbsp;is&nbsp;a&nbsp;dictionary&nbsp;with&nbsp;the&nbsp;following&nbsp;schema:<br>
&nbsp;<br>
-&nbsp;``event``:&nbsp;**str**&nbsp;-&nbsp;Event&nbsp;names&nbsp;are&nbsp;of&nbsp;the<br>
&nbsp;&nbsp;&nbsp;&nbsp;format:&nbsp;on_[runnable_type]_(start|stream|end).<br>
-&nbsp;``name``:&nbsp;**str**&nbsp;-&nbsp;The&nbsp;name&nbsp;of&nbsp;the&nbsp;Runnable&nbsp;that&nbsp;generated&nbsp;the&nbsp;event.<br>
-&nbsp;``run_id``:&nbsp;**str**&nbsp;-&nbsp;randomly&nbsp;generated&nbsp;ID&nbsp;associated&nbsp;with&nbsp;the&nbsp;given&nbsp;execution&nbsp;of<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;Runnable&nbsp;that&nbsp;emitted&nbsp;the&nbsp;event.<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;child&nbsp;Runnable&nbsp;that&nbsp;gets&nbsp;invoked&nbsp;as&nbsp;part&nbsp;of&nbsp;the&nbsp;execution&nbsp;of&nbsp;a<br>
&nbsp;&nbsp;&nbsp;&nbsp;parent&nbsp;Runnable&nbsp;is&nbsp;assigned&nbsp;its&nbsp;own&nbsp;unique&nbsp;ID.<br>
-&nbsp;``parent_ids``:&nbsp;**List[str]**&nbsp;-&nbsp;The&nbsp;IDs&nbsp;of&nbsp;the&nbsp;parent&nbsp;runnables&nbsp;that<br>
&nbsp;&nbsp;&nbsp;&nbsp;generated&nbsp;the&nbsp;event.&nbsp;The&nbsp;root&nbsp;Runnable&nbsp;will&nbsp;have&nbsp;an&nbsp;empty&nbsp;list.<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;order&nbsp;of&nbsp;the&nbsp;parent&nbsp;IDs&nbsp;is&nbsp;from&nbsp;the&nbsp;root&nbsp;to&nbsp;the&nbsp;immediate&nbsp;parent.<br>
&nbsp;&nbsp;&nbsp;&nbsp;Only&nbsp;available&nbsp;for&nbsp;v2&nbsp;version&nbsp;of&nbsp;the&nbsp;API.&nbsp;The&nbsp;v1&nbsp;version&nbsp;of&nbsp;the&nbsp;API<br>
&nbsp;&nbsp;&nbsp;&nbsp;will&nbsp;return&nbsp;an&nbsp;empty&nbsp;list.<br>
-&nbsp;``tags``:&nbsp;**Optional[List[str]]**&nbsp;-&nbsp;The&nbsp;tags&nbsp;of&nbsp;the&nbsp;Runnable&nbsp;that&nbsp;generated<br>
&nbsp;&nbsp;&nbsp;&nbsp;the&nbsp;event.<br>
-&nbsp;``metadata``:&nbsp;**Optional[Dict[str,&nbsp;Any]]**&nbsp;-&nbsp;The&nbsp;metadata&nbsp;of&nbsp;the&nbsp;Runnable<br>
&nbsp;&nbsp;&nbsp;&nbsp;that&nbsp;generated&nbsp;the&nbsp;event.<br>
-&nbsp;``data``:&nbsp;**Dict[str,&nbsp;Any]**<br>
&nbsp;<br>
&nbsp;<br>
Below&nbsp;is&nbsp;a&nbsp;table&nbsp;that&nbsp;illustrates&nbsp;some&nbsp;evens&nbsp;that&nbsp;might&nbsp;be&nbsp;emitted&nbsp;by&nbsp;various<br>
chains.&nbsp;Metadata&nbsp;fields&nbsp;have&nbsp;been&nbsp;omitted&nbsp;from&nbsp;the&nbsp;table&nbsp;for&nbsp;brevity.<br>
Chain&nbsp;definitions&nbsp;have&nbsp;been&nbsp;included&nbsp;after&nbsp;the&nbsp;table.<br>
&nbsp;<br>
**ATTENTION**&nbsp;This&nbsp;reference&nbsp;table&nbsp;is&nbsp;for&nbsp;the&nbsp;V2&nbsp;version&nbsp;of&nbsp;the&nbsp;schema.<br>
&nbsp;<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;event&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;chunk&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;input&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;output&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+======================+==================+=================================+===============================================+=================================================+<br>
|&nbsp;on_chat_model_start&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"messages":&nbsp;[[SystemMessage,&nbsp;HumanMessage]]}&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_chat_model_stream&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;AIMessageChunk(content="hello")&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_chat_model_end&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"messages":&nbsp;[[SystemMessage,&nbsp;HumanMessage]]}&nbsp;|&nbsp;AIMessageChunk(content="hello&nbsp;world")&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_llm_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{'input':&nbsp;'hello'}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_llm_stream&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;'Hello'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_llm_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[model&nbsp;name]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;'Hello&nbsp;human!'&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_chain_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;format_docs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_chain_stream&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;format_docs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;"hello&nbsp;world!,&nbsp;goodbye&nbsp;world!"&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_chain_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;format_docs&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[Document(...)]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;"hello&nbsp;world!,&nbsp;goodbye&nbsp;world!"&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_tool_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;some_tool&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"x":&nbsp;1,&nbsp;"y":&nbsp;"2"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_tool_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;some_tool&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"x":&nbsp;1,&nbsp;"y":&nbsp;"2"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_retriever_start&nbsp;&nbsp;&nbsp;|&nbsp;[retriever&nbsp;name]&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"query":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_retriever_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[retriever&nbsp;name]&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"query":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[Document(...),&nbsp;..]&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_prompt_start&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[template_name]&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"question":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
|&nbsp;on_prompt_end&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;[template_name]&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;{"question":&nbsp;"hello"}&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;ChatPromptValue(messages:&nbsp;[SystemMessage,&nbsp;...])&nbsp;|<br>
+----------------------+------------------+---------------------------------+-----------------------------------------------+-------------------------------------------------+<br>
&nbsp;<br>
In&nbsp;addition&nbsp;to&nbsp;the&nbsp;standard&nbsp;events,&nbsp;users&nbsp;can&nbsp;also&nbsp;dispatch&nbsp;custom&nbsp;events&nbsp;(see&nbsp;example&nbsp;below).<br>
&nbsp;<br>
Custom&nbsp;events&nbsp;will&nbsp;be&nbsp;only&nbsp;be&nbsp;surfaced&nbsp;with&nbsp;in&nbsp;the&nbsp;`v2`&nbsp;version&nbsp;of&nbsp;the&nbsp;API!<br>
&nbsp;<br>
A&nbsp;custom&nbsp;event&nbsp;has&nbsp;following&nbsp;format:<br>
&nbsp;<br>
+-----------+------+-----------------------------------------------------------------------------------------------------------+<br>
|&nbsp;Attribute&nbsp;|&nbsp;Type&nbsp;|&nbsp;Description&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+===========+======+===========================================================================================================+<br>
|&nbsp;name&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;str&nbsp;&nbsp;|&nbsp;A&nbsp;user&nbsp;defined&nbsp;name&nbsp;for&nbsp;the&nbsp;event.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|<br>
+-----------+------+-----------------------------------------------------------------------------------------------------------+<br>
|&nbsp;data&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;Any&nbsp;&nbsp;|&nbsp;The&nbsp;data&nbsp;associated&nbsp;with&nbsp;the&nbsp;event.&nbsp;This&nbsp;can&nbsp;be&nbsp;anything,&nbsp;though&nbsp;we&nbsp;suggest&nbsp;making&nbsp;it&nbsp;JSON&nbsp;serializable.&nbsp;&nbsp;|<br>
+-----------+------+-----------------------------------------------------------------------------------------------------------+<br>
&nbsp;<br>
Here&nbsp;are&nbsp;declarations&nbsp;associated&nbsp;with&nbsp;the&nbsp;standard&nbsp;events&nbsp;shown&nbsp;above:<br>
&nbsp;<br>
`format_docs`:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;format_docs(docs:&nbsp;List[Document])&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'''Format&nbsp;the&nbsp;docs.'''<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;",&nbsp;".join([doc.page_content&nbsp;for&nbsp;doc&nbsp;in&nbsp;docs])<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;format_docs&nbsp;=&nbsp;RunnableLambda(format_docs)<br>
&nbsp;<br>
`some_tool`:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;@tool<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;some_tool(x:&nbsp;int,&nbsp;y:&nbsp;str)&nbsp;-&gt;&nbsp;dict:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;'''Some_tool.'''<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;{"x":&nbsp;x,&nbsp;"y":&nbsp;y}<br>
&nbsp;<br>
`prompt`:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;template&nbsp;=&nbsp;ChatPromptTemplate.from_messages(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[("system",&nbsp;"You&nbsp;are&nbsp;Cat&nbsp;Agent&nbsp;007"),&nbsp;("human",&nbsp;"{question}")]<br>
&nbsp;&nbsp;&nbsp;&nbsp;).<a href="#ChatGoogleGenerativeAI-with_config">with_config</a>({"run_name":&nbsp;"my_template",&nbsp;"tags":&nbsp;["my_template"]})<br>
&nbsp;<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;def&nbsp;reverse(s:&nbsp;str)&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;s[::-1]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain&nbsp;=&nbsp;RunnableLambda(func=reverse)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;events&nbsp;=&nbsp;[<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;event&nbsp;async&nbsp;for&nbsp;event&nbsp;in&nbsp;chain.<a href="#ChatGoogleGenerativeAI-astream_events">astream_events</a>("hello",&nbsp;version="v2")<br>
&nbsp;&nbsp;&nbsp;&nbsp;]<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;will&nbsp;produce&nbsp;the&nbsp;following&nbsp;events&nbsp;(run_id,&nbsp;and&nbsp;parent_ids<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;has&nbsp;been&nbsp;omitted&nbsp;for&nbsp;brevity):<br>
&nbsp;&nbsp;&nbsp;&nbsp;[<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"data":&nbsp;{"input":&nbsp;"hello"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"event":&nbsp;"on_chain_start",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata":&nbsp;{},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"reverse",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tags":&nbsp;[],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"data":&nbsp;{"chunk":&nbsp;"olleh"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"event":&nbsp;"on_chain_stream",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata":&nbsp;{},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"reverse",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tags":&nbsp;[],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"data":&nbsp;{"output":&nbsp;"olleh"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"event":&nbsp;"on_chain_end",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"metadata":&nbsp;{},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"name":&nbsp;"reverse",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"tags":&nbsp;[],<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},<br>
&nbsp;&nbsp;&nbsp;&nbsp;]<br>
&nbsp;<br>
&nbsp;<br>
Example:&nbsp;Dispatch&nbsp;Custom&nbsp;Event<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.callbacks.manager&nbsp;import&nbsp;(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;adispatch_custom_event,<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda,&nbsp;RunnableConfig<br>
&nbsp;&nbsp;&nbsp;&nbsp;import&nbsp;asyncio<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;def&nbsp;slow_thing(some_input:&nbsp;str,&nbsp;config:&nbsp;RunnableConfig)&nbsp;-&gt;&nbsp;str:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"""Do&nbsp;something&nbsp;that&nbsp;takes&nbsp;a&nbsp;long&nbsp;time."""<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;asyncio.sleep(1)&nbsp;#&nbsp;Placeholder&nbsp;for&nbsp;some&nbsp;slow&nbsp;operation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;adispatch_custom_event(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"progress_event",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{"message":&nbsp;"Finished&nbsp;step&nbsp;1&nbsp;of&nbsp;3"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;config=config&nbsp;#&nbsp;Must&nbsp;be&nbsp;included&nbsp;for&nbsp;python&nbsp;&lt;&nbsp;3.10<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;asyncio.sleep(1)&nbsp;#&nbsp;Placeholder&nbsp;for&nbsp;some&nbsp;slow&nbsp;operation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;adispatch_custom_event(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;"progress_event",<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{"message":&nbsp;"Finished&nbsp;step&nbsp;2&nbsp;of&nbsp;3"},<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;config=config&nbsp;#&nbsp;Must&nbsp;be&nbsp;included&nbsp;for&nbsp;python&nbsp;&lt;&nbsp;3.10<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;asyncio.sleep(1)&nbsp;#&nbsp;Placeholder&nbsp;for&nbsp;some&nbsp;slow&nbsp;operation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;"Done"<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;slow_thing&nbsp;=&nbsp;RunnableLambda(slow_thing)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;for&nbsp;event&nbsp;in&nbsp;slow_thing.<a href="#ChatGoogleGenerativeAI-astream_events">astream_events</a>("some_input",&nbsp;version="v2"):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(event)<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;version:&nbsp;The&nbsp;version&nbsp;of&nbsp;the&nbsp;schema&nbsp;to&nbsp;use&nbsp;either&nbsp;`v2`&nbsp;or&nbsp;`v1`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Users&nbsp;should&nbsp;use&nbsp;`v2`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;`v1`&nbsp;is&nbsp;for&nbsp;backwards&nbsp;compatibility&nbsp;and&nbsp;will&nbsp;be&nbsp;deprecated<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;in&nbsp;0.4.0.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;No&nbsp;default&nbsp;will&nbsp;be&nbsp;assigned&nbsp;until&nbsp;the&nbsp;API&nbsp;is&nbsp;stabilized.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;custom&nbsp;events&nbsp;will&nbsp;only&nbsp;be&nbsp;surfaced&nbsp;in&nbsp;`v2`.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_names:&nbsp;Only&nbsp;include&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_types:&nbsp;Only&nbsp;include&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_tags:&nbsp;Only&nbsp;include&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;tags.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_names:&nbsp;Exclude&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_types:&nbsp;Exclude&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_tags:&nbsp;Exclude&nbsp;events&nbsp;from&nbsp;runnables&nbsp;with&nbsp;matching&nbsp;tags.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;These&nbsp;will&nbsp;be&nbsp;passed&nbsp;to&nbsp;astream_log&nbsp;as&nbsp;this&nbsp;implementation<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;of&nbsp;astream_events&nbsp;is&nbsp;built&nbsp;on&nbsp;top&nbsp;of&nbsp;astream_log.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;An&nbsp;async&nbsp;stream&nbsp;of&nbsp;StreamEvents.<br>
&nbsp;<br>
Raises:<br>
&nbsp;&nbsp;&nbsp;&nbsp;NotImplementedError:&nbsp;If&nbsp;the&nbsp;version&nbsp;is&nbsp;not&nbsp;`v1`&nbsp;or&nbsp;`v2`.<br>
&nbsp;<br>
Notes<br>
-----<br>
..&nbsp;beta::<br>
&nbsp;&nbsp;&nbsp;This&nbsp;API&nbsp;is&nbsp;in&nbsp;beta&nbsp;and&nbsp;may&nbsp;change&nbsp;in&nbsp;the&nbsp;future.</tt></dd></dl>

<dl><dt>async <a name="ChatGoogleGenerativeAI-astream_log"><strong>astream_log</strong></a>(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -&gt; 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'</dt><dd><tt>Stream&nbsp;all&nbsp;output&nbsp;from&nbsp;a&nbsp;Runnable,&nbsp;as&nbsp;reported&nbsp;to&nbsp;the&nbsp;callback&nbsp;system.<br>
This&nbsp;includes&nbsp;all&nbsp;inner&nbsp;runs&nbsp;of&nbsp;LLMs,&nbsp;Retrievers,&nbsp;Tools,&nbsp;etc.<br>
&nbsp;<br>
Output&nbsp;is&nbsp;streamed&nbsp;as&nbsp;Log&nbsp;objects,&nbsp;which&nbsp;include&nbsp;a&nbsp;list&nbsp;of<br>
Jsonpatch&nbsp;ops&nbsp;that&nbsp;describe&nbsp;how&nbsp;the&nbsp;state&nbsp;of&nbsp;the&nbsp;run&nbsp;has&nbsp;changed&nbsp;in&nbsp;each<br>
step,&nbsp;and&nbsp;the&nbsp;final&nbsp;state&nbsp;of&nbsp;the&nbsp;run.<br>
&nbsp;<br>
The&nbsp;Jsonpatch&nbsp;ops&nbsp;can&nbsp;be&nbsp;applied&nbsp;in&nbsp;order&nbsp;to&nbsp;construct&nbsp;state.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;The&nbsp;input&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;diff:&nbsp;Whether&nbsp;to&nbsp;yield&nbsp;diffs&nbsp;between&nbsp;each&nbsp;step&nbsp;or&nbsp;the&nbsp;current&nbsp;state.<br>
&nbsp;&nbsp;&nbsp;&nbsp;with_streamed_output_list:&nbsp;Whether&nbsp;to&nbsp;yield&nbsp;the&nbsp;streamed_output&nbsp;list.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_names:&nbsp;Only&nbsp;include&nbsp;logs&nbsp;with&nbsp;these&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_types:&nbsp;Only&nbsp;include&nbsp;logs&nbsp;with&nbsp;these&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;include_tags:&nbsp;Only&nbsp;include&nbsp;logs&nbsp;with&nbsp;these&nbsp;tags.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_names:&nbsp;Exclude&nbsp;logs&nbsp;with&nbsp;these&nbsp;names.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_types:&nbsp;Exclude&nbsp;logs&nbsp;with&nbsp;these&nbsp;types.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exclude_tags:&nbsp;Exclude&nbsp;logs&nbsp;with&nbsp;these&nbsp;tags.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;RunLogPatch&nbsp;or&nbsp;RunLog&nbsp;object.</tt></dd></dl>

<dl><dt>async <a name="ChatGoogleGenerativeAI-atransform"><strong>atransform</strong></a>(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -&gt; 'AsyncIterator[Output]'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;atransform,&nbsp;which&nbsp;buffers&nbsp;input&nbsp;and&nbsp;calls&nbsp;astream.<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;start&nbsp;producing&nbsp;output&nbsp;while<br>
input&nbsp;is&nbsp;still&nbsp;being&nbsp;generated.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;An&nbsp;async&nbsp;iterator&nbsp;of&nbsp;inputs&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;Runnable.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;output&nbsp;of&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-batch"><strong>batch</strong></a>(self, inputs: 'List[Input]', config: 'Optional[Union[RunnableConfig, List[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -&gt; 'List[Output]'</dt><dd><tt>Default&nbsp;implementation&nbsp;runs&nbsp;invoke&nbsp;in&nbsp;parallel&nbsp;using&nbsp;a&nbsp;thread&nbsp;pool&nbsp;executor.<br>
&nbsp;<br>
The&nbsp;default&nbsp;implementation&nbsp;of&nbsp;batch&nbsp;works&nbsp;well&nbsp;for&nbsp;IO&nbsp;bound&nbsp;runnables.<br>
&nbsp;<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;batch&nbsp;more&nbsp;efficiently;<br>
e.g.,&nbsp;if&nbsp;the&nbsp;underlying&nbsp;Runnable&nbsp;uses&nbsp;an&nbsp;API&nbsp;which&nbsp;supports&nbsp;a&nbsp;batch&nbsp;mode.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-batch_as_completed"><strong>batch_as_completed</strong></a>(self, inputs: 'Sequence[Input]', config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -&gt; 'Iterator[Tuple[int, Union[Output, Exception]]]'</dt><dd><tt>Run&nbsp;invoke&nbsp;in&nbsp;parallel&nbsp;on&nbsp;a&nbsp;list&nbsp;of&nbsp;inputs,<br>
yielding&nbsp;results&nbsp;as&nbsp;they&nbsp;complete.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-bind"><strong>bind</strong></a>(self, **kwargs: 'Any') -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;arguments&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
Useful&nbsp;when&nbsp;a&nbsp;Runnable&nbsp;in&nbsp;a&nbsp;chain&nbsp;requires&nbsp;an&nbsp;argument&nbsp;that&nbsp;is&nbsp;not<br>
in&nbsp;the&nbsp;output&nbsp;of&nbsp;the&nbsp;previous&nbsp;Runnable&nbsp;or&nbsp;included&nbsp;in&nbsp;the&nbsp;user&nbsp;input.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;The&nbsp;arguments&nbsp;to&nbsp;bind&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;with&nbsp;the&nbsp;arguments&nbsp;bound.<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_community.chat_models&nbsp;import&nbsp;ChatOllama<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.output_parsers&nbsp;import&nbsp;StrOutputParser<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;llm&nbsp;=&nbsp;ChatOllama(model='llama2')<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Without&nbsp;bind.<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain&nbsp;=&nbsp;(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;llm<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;StrOutputParser()<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain.<a href="#ChatGoogleGenerativeAI-invoke">invoke</a>("Repeat&nbsp;quoted&nbsp;words&nbsp;exactly:&nbsp;'One&nbsp;two&nbsp;three&nbsp;four&nbsp;five.'")<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Output&nbsp;is&nbsp;'One&nbsp;two&nbsp;three&nbsp;four&nbsp;five.'<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;With&nbsp;bind.<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain&nbsp;=&nbsp;(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;llm.<a href="#ChatGoogleGenerativeAI-bind">bind</a>(stop=["three"])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;StrOutputParser()<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain.<a href="#ChatGoogleGenerativeAI-invoke">invoke</a>("Repeat&nbsp;quoted&nbsp;words&nbsp;exactly:&nbsp;'One&nbsp;two&nbsp;three&nbsp;four&nbsp;five.'")<br>
&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Output&nbsp;is&nbsp;'One&nbsp;two'</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-config_schema"><strong>config_schema</strong></a>(self, *, include: 'Optional[Sequence[str]]' = None) -&gt; 'Type[BaseModel]'</dt><dd><tt>The&nbsp;type&nbsp;of&nbsp;config&nbsp;this&nbsp;Runnable&nbsp;accepts&nbsp;specified&nbsp;as&nbsp;a&nbsp;pydantic&nbsp;model.<br>
&nbsp;<br>
To&nbsp;mark&nbsp;a&nbsp;field&nbsp;as&nbsp;configurable,&nbsp;see&nbsp;the&nbsp;`configurable_fields`<br>
and&nbsp;`configurable_alternatives`&nbsp;methods.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;include:&nbsp;A&nbsp;list&nbsp;of&nbsp;fields&nbsp;to&nbsp;include&nbsp;in&nbsp;the&nbsp;config&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;config.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-get_graph"><strong>get_graph</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'Graph'</dt><dd><tt>Return&nbsp;a&nbsp;graph&nbsp;representation&nbsp;of&nbsp;this&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-get_input_schema"><strong>get_input_schema</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'Type[BaseModel]'</dt><dd><tt>Get&nbsp;a&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;input&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Runnables&nbsp;that&nbsp;leverage&nbsp;the&nbsp;configurable_fields&nbsp;and&nbsp;configurable_alternatives<br>
methods&nbsp;will&nbsp;have&nbsp;a&nbsp;dynamic&nbsp;input&nbsp;schema&nbsp;that&nbsp;depends&nbsp;on&nbsp;which<br>
configuration&nbsp;the&nbsp;Runnable&nbsp;is&nbsp;invoked&nbsp;with.<br>
&nbsp;<br>
This&nbsp;method&nbsp;allows&nbsp;to&nbsp;get&nbsp;an&nbsp;input&nbsp;schema&nbsp;for&nbsp;a&nbsp;specific&nbsp;configuration.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;generating&nbsp;the&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;input.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-get_name"><strong>get_name</strong></a>(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -&gt; 'str'</dt><dd><tt>Get&nbsp;the&nbsp;name&nbsp;of&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-get_output_schema"><strong>get_output_schema</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'Type[BaseModel]'</dt><dd><tt>Get&nbsp;a&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;output&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Runnables&nbsp;that&nbsp;leverage&nbsp;the&nbsp;configurable_fields&nbsp;and&nbsp;configurable_alternatives<br>
methods&nbsp;will&nbsp;have&nbsp;a&nbsp;dynamic&nbsp;output&nbsp;schema&nbsp;that&nbsp;depends&nbsp;on&nbsp;which<br>
configuration&nbsp;the&nbsp;Runnable&nbsp;is&nbsp;invoked&nbsp;with.<br>
&nbsp;<br>
This&nbsp;method&nbsp;allows&nbsp;to&nbsp;get&nbsp;an&nbsp;output&nbsp;schema&nbsp;for&nbsp;a&nbsp;specific&nbsp;configuration.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;A&nbsp;config&nbsp;to&nbsp;use&nbsp;when&nbsp;generating&nbsp;the&nbsp;schema.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;pydantic&nbsp;model&nbsp;that&nbsp;can&nbsp;be&nbsp;used&nbsp;to&nbsp;validate&nbsp;output.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-get_prompts"><strong>get_prompts</strong></a>(self, config: 'Optional[RunnableConfig]' = None) -&gt; 'List[BasePromptTemplate]'</dt><dd><tt>Return&nbsp;a&nbsp;list&nbsp;of&nbsp;prompts&nbsp;used&nbsp;by&nbsp;this&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-map"><strong>map</strong></a>(self) -&gt; 'Runnable[List[Input], List[Output]]'</dt><dd><tt>Return&nbsp;a&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;maps&nbsp;a&nbsp;list&nbsp;of&nbsp;inputs&nbsp;to&nbsp;a&nbsp;list&nbsp;of&nbsp;outputs,<br>
by&nbsp;calling&nbsp;<a href="#ChatGoogleGenerativeAI-invoke">invoke</a>()&nbsp;with&nbsp;each&nbsp;input.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;maps&nbsp;a&nbsp;list&nbsp;of&nbsp;inputs&nbsp;to&nbsp;a&nbsp;list&nbsp;of&nbsp;outputs.<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;_lambda(x:&nbsp;int)&nbsp;-&gt;&nbsp;int:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;x&nbsp;+&nbsp;1<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(_lambda)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(runnable.<a href="#ChatGoogleGenerativeAI-map">map</a>().<a href="#ChatGoogleGenerativeAI-invoke">invoke</a>([1,&nbsp;2,&nbsp;3]))&nbsp;#&nbsp;[2,&nbsp;3,&nbsp;4]</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-pick"><strong>pick</strong></a>(self, keys: 'Union[str, List[str]]') -&gt; 'RunnableSerializable[Any, Any]'</dt><dd><tt>Pick&nbsp;keys&nbsp;from&nbsp;the&nbsp;dict&nbsp;output&nbsp;of&nbsp;this&nbsp;Runnable.<br>
&nbsp;<br>
Pick&nbsp;single&nbsp;key:<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import&nbsp;json<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda,&nbsp;RunnableMap<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;as_str&nbsp;=&nbsp;RunnableLambda(str)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;as_json&nbsp;=&nbsp;RunnableLambda(json.loads)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chain&nbsp;=&nbsp;RunnableMap(str=as_str,&nbsp;json=as_json)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chain.<a href="#ChatGoogleGenerativeAI-invoke">invoke</a>("[1,&nbsp;2,&nbsp;3]")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;{"str":&nbsp;"[1,&nbsp;2,&nbsp;3]",&nbsp;"json":&nbsp;[1,&nbsp;2,&nbsp;3]}<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;json_only_chain&nbsp;=&nbsp;chain.<a href="#ChatGoogleGenerativeAI-pick">pick</a>("json")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;json_only_chain.<a href="#ChatGoogleGenerativeAI-invoke">invoke</a>("[1,&nbsp;2,&nbsp;3]")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;[1,&nbsp;2,&nbsp;3]<br>
&nbsp;<br>
Pick&nbsp;list&nbsp;of&nbsp;keys:<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;typing&nbsp;import&nbsp;Any<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;import&nbsp;json<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda,&nbsp;RunnableMap<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;as_str&nbsp;=&nbsp;RunnableLambda(str)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;as_json&nbsp;=&nbsp;RunnableLambda(json.loads)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;as_bytes(x:&nbsp;Any)&nbsp;-&gt;&nbsp;bytes:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;bytes(x,&nbsp;"utf-8")<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chain&nbsp;=&nbsp;RunnableMap(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;str=as_str,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;json=as_json,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;bytes=RunnableLambda(as_bytes)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;chain.<a href="#ChatGoogleGenerativeAI-invoke">invoke</a>("[1,&nbsp;2,&nbsp;3]")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;{"str":&nbsp;"[1,&nbsp;2,&nbsp;3]",&nbsp;"json":&nbsp;[1,&nbsp;2,&nbsp;3],&nbsp;"bytes":&nbsp;b"[1,&nbsp;2,&nbsp;3]"}<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;json_and_bytes_chain&nbsp;=&nbsp;chain.<a href="#ChatGoogleGenerativeAI-pick">pick</a>(["json",&nbsp;"bytes"])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;json_and_bytes_chain.<a href="#ChatGoogleGenerativeAI-invoke">invoke</a>("[1,&nbsp;2,&nbsp;3]")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;{"json":&nbsp;[1,&nbsp;2,&nbsp;3],&nbsp;"bytes":&nbsp;b"[1,&nbsp;2,&nbsp;3]"}</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-pipe"><strong>pipe</strong></a>(self, *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]', name: 'Optional[str]' = None) -&gt; 'RunnableSerializable[Input, Other]'</dt><dd><tt>Compose&nbsp;this&nbsp;Runnable&nbsp;with&nbsp;Runnable-like&nbsp;objects&nbsp;to&nbsp;make&nbsp;a&nbsp;RunnableSequence.<br>
&nbsp;<br>
Equivalent&nbsp;to&nbsp;`RunnableSequence(self,&nbsp;*others)`&nbsp;or&nbsp;`self&nbsp;|&nbsp;others[0]&nbsp;|&nbsp;...`<br>
&nbsp;<br>
Example:<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;add_one(x:&nbsp;int)&nbsp;-&gt;&nbsp;int:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;x&nbsp;+&nbsp;1<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;mul_two(x:&nbsp;int)&nbsp;-&gt;&nbsp;int:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;return&nbsp;x&nbsp;*&nbsp;2<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runnable_1&nbsp;=&nbsp;RunnableLambda(add_one)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runnable_2&nbsp;=&nbsp;RunnableLambda(mul_two)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sequence&nbsp;=&nbsp;runnable_1.<a href="#ChatGoogleGenerativeAI-pipe">pipe</a>(runnable_2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;Or&nbsp;equivalently:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;sequence&nbsp;=&nbsp;runnable_1&nbsp;|&nbsp;runnable_2<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;sequence&nbsp;=&nbsp;RunnableSequence(first=runnable_1,&nbsp;last=runnable_2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sequence.<a href="#ChatGoogleGenerativeAI-invoke">invoke</a>(1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;sequence.<a href="#ChatGoogleGenerativeAI-ainvoke">ainvoke</a>(1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;4<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;sequence.<a href="#ChatGoogleGenerativeAI-batch">batch</a>([1,&nbsp;2,&nbsp;3])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;sequence.<a href="#ChatGoogleGenerativeAI-abatch">abatch</a>([1,&nbsp;2,&nbsp;3])<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;#&nbsp;-&gt;&nbsp;[4,&nbsp;6,&nbsp;8]</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-transform"><strong>transform</strong></a>(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -&gt; 'Iterator[Output]'</dt><dd><tt>Default&nbsp;implementation&nbsp;of&nbsp;transform,&nbsp;which&nbsp;buffers&nbsp;input&nbsp;and&nbsp;then&nbsp;calls&nbsp;stream.<br>
Subclasses&nbsp;should&nbsp;override&nbsp;this&nbsp;method&nbsp;if&nbsp;they&nbsp;can&nbsp;start&nbsp;producing&nbsp;output&nbsp;while<br>
input&nbsp;is&nbsp;still&nbsp;being&nbsp;generated.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input:&nbsp;An&nbsp;iterator&nbsp;of&nbsp;inputs&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;use&nbsp;for&nbsp;the&nbsp;Runnable.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;**kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Yields:<br>
&nbsp;&nbsp;&nbsp;&nbsp;The&nbsp;output&nbsp;of&nbsp;the&nbsp;Runnable.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-with_alisteners"><strong>with_alisteners</strong></a>(self, *, on_start: 'Optional[AsyncListener]' = None, on_end: 'Optional[AsyncListener]' = None, on_error: 'Optional[AsyncListener]' = None) -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;asynchronous&nbsp;lifecycle&nbsp;listeners&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
on_start:&nbsp;Asynchronously&nbsp;called&nbsp;before&nbsp;the&nbsp;Runnable&nbsp;starts&nbsp;running.<br>
on_end:&nbsp;Asynchronously&nbsp;called&nbsp;after&nbsp;the&nbsp;Runnable&nbsp;finishes&nbsp;running.<br>
on_error:&nbsp;Asynchronously&nbsp;called&nbsp;if&nbsp;the&nbsp;Runnable&nbsp;throws&nbsp;an&nbsp;error.<br>
&nbsp;<br>
The&nbsp;Run&nbsp;object&nbsp;contains&nbsp;information&nbsp;about&nbsp;the&nbsp;run,&nbsp;including&nbsp;its&nbsp;id,<br>
type,&nbsp;input,&nbsp;output,&nbsp;error,&nbsp;start_time,&nbsp;end_time,&nbsp;and&nbsp;any&nbsp;tags&nbsp;or&nbsp;metadata<br>
added&nbsp;to&nbsp;the&nbsp;run.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;on_start:&nbsp;Asynchronously&nbsp;called&nbsp;before&nbsp;the&nbsp;Runnable&nbsp;starts&nbsp;running.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;on_end:&nbsp;Asynchronously&nbsp;called&nbsp;after&nbsp;the&nbsp;Runnable&nbsp;finishes&nbsp;running.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;on_error:&nbsp;Asynchronously&nbsp;called&nbsp;if&nbsp;the&nbsp;Runnable&nbsp;throws&nbsp;an&nbsp;error.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;with&nbsp;the&nbsp;listeners&nbsp;bound.<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;&nbsp;&nbsp;&nbsp;import&nbsp;time<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;def&nbsp;test_runnable(time_to_sleep&nbsp;:&nbsp;int):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"Runnable[{time_to_sleep}s]:&nbsp;starts&nbsp;at&nbsp;{format_t(time.time())}")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;asyncio.sleep(time_to_sleep)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"Runnable[{time_to_sleep}s]:&nbsp;ends&nbsp;at&nbsp;{format_t(time.time())}")<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;def&nbsp;fn_start(run_obj&nbsp;:&nbsp;Runnable):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"on&nbsp;start&nbsp;callback&nbsp;starts&nbsp;at&nbsp;{format_t(time.time())}<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;asyncio.sleep(3)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"on&nbsp;start&nbsp;callback&nbsp;ends&nbsp;at&nbsp;{format_t(time.time())}")<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;def&nbsp;fn_end(run_obj&nbsp;:&nbsp;Runnable):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"on&nbsp;end&nbsp;callback&nbsp;starts&nbsp;at&nbsp;{format_t(time.time())}<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;asyncio.sleep(2)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(f"on&nbsp;end&nbsp;callback&nbsp;ends&nbsp;at&nbsp;{format_t(time.time())}")<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(test_runnable).<a href="#ChatGoogleGenerativeAI-with_alisteners">with_alisteners</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on_start=fn_start,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on_end=fn_end<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;async&nbsp;def&nbsp;concurrent_runs():<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;await&nbsp;asyncio.gather(runnable.<a href="#ChatGoogleGenerativeAI-ainvoke">ainvoke</a>(2),&nbsp;runnable.<a href="#ChatGoogleGenerativeAI-ainvoke">ainvoke</a>(3))<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;asyncio.run(concurrent_runs())<br>
&nbsp;&nbsp;&nbsp;&nbsp;Result:<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;start&nbsp;callback&nbsp;starts&nbsp;at&nbsp;2024-05-16T14:20:29.637053+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;start&nbsp;callback&nbsp;starts&nbsp;at&nbsp;2024-05-16T14:20:29.637150+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;start&nbsp;callback&nbsp;ends&nbsp;at&nbsp;2024-05-16T14:20:32.638305+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;start&nbsp;callback&nbsp;ends&nbsp;at&nbsp;2024-05-16T14:20:32.638383+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;Runnable[3s]:&nbsp;starts&nbsp;at&nbsp;2024-05-16T14:20:32.638849+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;Runnable[5s]:&nbsp;starts&nbsp;at&nbsp;2024-05-16T14:20:32.638999+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;Runnable[3s]:&nbsp;ends&nbsp;at&nbsp;2024-05-16T14:20:35.640016+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;end&nbsp;callback&nbsp;starts&nbsp;at&nbsp;2024-05-16T14:20:35.640534+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;Runnable[5s]:&nbsp;ends&nbsp;at&nbsp;2024-05-16T14:20:37.640169+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;end&nbsp;callback&nbsp;starts&nbsp;at&nbsp;2024-05-16T14:20:37.640574+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;end&nbsp;callback&nbsp;ends&nbsp;at&nbsp;2024-05-16T14:20:37.640654+00:00<br>
&nbsp;&nbsp;&nbsp;&nbsp;on&nbsp;end&nbsp;callback&nbsp;ends&nbsp;at&nbsp;2024-05-16T14:20:39.641751+00:00</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-with_config"><strong>with_config</strong></a>(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;config&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;config:&nbsp;The&nbsp;config&nbsp;to&nbsp;bind&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;&nbsp;&nbsp;&nbsp;kwargs:&nbsp;Additional&nbsp;keyword&nbsp;arguments&nbsp;to&nbsp;pass&nbsp;to&nbsp;the&nbsp;Runnable.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;with&nbsp;the&nbsp;config&nbsp;bound.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-with_fallbacks"><strong>with_fallbacks</strong></a>(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'Tuple[Type[BaseException], ...]' = (&lt;class 'Exception'&gt;,), exception_key: 'Optional[str]' = None) -&gt; 'RunnableWithFallbacksT[Input, Output]'</dt><dd><tt>Add&nbsp;fallbacks&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
The&nbsp;new&nbsp;Runnable&nbsp;will&nbsp;try&nbsp;the&nbsp;original&nbsp;Runnable,&nbsp;and&nbsp;then&nbsp;each&nbsp;fallback<br>
in&nbsp;order,&nbsp;upon&nbsp;failures.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fallbacks:&nbsp;A&nbsp;sequence&nbsp;of&nbsp;runnables&nbsp;to&nbsp;try&nbsp;if&nbsp;the&nbsp;original&nbsp;Runnable&nbsp;fails.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exceptions_to_handle:&nbsp;A&nbsp;tuple&nbsp;of&nbsp;exception&nbsp;types&nbsp;to&nbsp;handle.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;(Exception,).<br>
&nbsp;&nbsp;&nbsp;&nbsp;exception_key:&nbsp;If&nbsp;string&nbsp;is&nbsp;specified&nbsp;then&nbsp;handled&nbsp;exceptions&nbsp;will&nbsp;be&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;fallbacks&nbsp;as&nbsp;part&nbsp;of&nbsp;the&nbsp;input&nbsp;under&nbsp;the&nbsp;specified&nbsp;key.&nbsp;If&nbsp;None,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;exceptions&nbsp;will&nbsp;not&nbsp;be&nbsp;passed&nbsp;to&nbsp;fallbacks.&nbsp;If&nbsp;used,&nbsp;the&nbsp;base&nbsp;Runnable<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;its&nbsp;fallbacks&nbsp;must&nbsp;accept&nbsp;a&nbsp;dictionary&nbsp;as&nbsp;input.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;will&nbsp;try&nbsp;the&nbsp;original&nbsp;Runnable,&nbsp;and&nbsp;then&nbsp;each<br>
&nbsp;&nbsp;&nbsp;&nbsp;fallback&nbsp;in&nbsp;order,&nbsp;upon&nbsp;failures.<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;typing&nbsp;import&nbsp;Iterator<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableGenerator<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;_generate_immediate_error(input:&nbsp;Iterator)&nbsp;-&gt;&nbsp;Iterator[str]:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise&nbsp;ValueError()<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;yield&nbsp;""<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;_generate(input:&nbsp;Iterator)&nbsp;-&gt;&nbsp;Iterator[str]:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;yield&nbsp;from&nbsp;"foo&nbsp;bar"<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableGenerator(_generate_immediate_error).<a href="#ChatGoogleGenerativeAI-with_fallbacks">with_fallbacks</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;[RunnableGenerator(_generate)]<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print(''.join(runnable.<a href="#ChatGoogleGenerativeAI-stream">stream</a>({})))&nbsp;#foo&nbsp;bar<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;fallbacks:&nbsp;A&nbsp;sequence&nbsp;of&nbsp;runnables&nbsp;to&nbsp;try&nbsp;if&nbsp;the&nbsp;original&nbsp;Runnable&nbsp;fails.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exceptions_to_handle:&nbsp;A&nbsp;tuple&nbsp;of&nbsp;exception&nbsp;types&nbsp;to&nbsp;handle.<br>
&nbsp;&nbsp;&nbsp;&nbsp;exception_key:&nbsp;If&nbsp;string&nbsp;is&nbsp;specified&nbsp;then&nbsp;handled&nbsp;exceptions&nbsp;will&nbsp;be&nbsp;passed<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;to&nbsp;fallbacks&nbsp;as&nbsp;part&nbsp;of&nbsp;the&nbsp;input&nbsp;under&nbsp;the&nbsp;specified&nbsp;key.&nbsp;If&nbsp;None,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;exceptions&nbsp;will&nbsp;not&nbsp;be&nbsp;passed&nbsp;to&nbsp;fallbacks.&nbsp;If&nbsp;used,&nbsp;the&nbsp;base&nbsp;Runnable<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;and&nbsp;its&nbsp;fallbacks&nbsp;must&nbsp;accept&nbsp;a&nbsp;dictionary&nbsp;as&nbsp;input.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;will&nbsp;try&nbsp;the&nbsp;original&nbsp;Runnable,&nbsp;and&nbsp;then&nbsp;each<br>
&nbsp;&nbsp;&nbsp;&nbsp;fallback&nbsp;in&nbsp;order,&nbsp;upon&nbsp;failures.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-with_listeners"><strong>with_listeners</strong></a>(self, *, on_start: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None, on_end: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None, on_error: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None) -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;lifecycle&nbsp;listeners&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
on_start:&nbsp;Called&nbsp;before&nbsp;the&nbsp;Runnable&nbsp;starts&nbsp;running,&nbsp;with&nbsp;the&nbsp;Run&nbsp;object.<br>
on_end:&nbsp;Called&nbsp;after&nbsp;the&nbsp;Runnable&nbsp;finishes&nbsp;running,&nbsp;with&nbsp;the&nbsp;Run&nbsp;object.<br>
on_error:&nbsp;Called&nbsp;if&nbsp;the&nbsp;Runnable&nbsp;throws&nbsp;an&nbsp;error,&nbsp;with&nbsp;the&nbsp;Run&nbsp;object.<br>
&nbsp;<br>
The&nbsp;Run&nbsp;object&nbsp;contains&nbsp;information&nbsp;about&nbsp;the&nbsp;run,&nbsp;including&nbsp;its&nbsp;id,<br>
type,&nbsp;input,&nbsp;output,&nbsp;error,&nbsp;start_time,&nbsp;end_time,&nbsp;and&nbsp;any&nbsp;tags&nbsp;or&nbsp;metadata<br>
added&nbsp;to&nbsp;the&nbsp;run.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;on_start:&nbsp;Called&nbsp;before&nbsp;the&nbsp;Runnable&nbsp;starts&nbsp;running.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;on_end:&nbsp;Called&nbsp;after&nbsp;the&nbsp;Runnable&nbsp;finishes&nbsp;running.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;on_error:&nbsp;Called&nbsp;if&nbsp;the&nbsp;Runnable&nbsp;throws&nbsp;an&nbsp;error.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;with&nbsp;the&nbsp;listeners&nbsp;bound.<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.tracers.schemas&nbsp;import&nbsp;Run<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;import&nbsp;time<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;test_runnable(time_to_sleep&nbsp;:&nbsp;int):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;time.sleep(time_to_sleep)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;fn_start(run_obj:&nbsp;Run):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("start_time:",&nbsp;run_obj.start_time)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;fn_end(run_obj:&nbsp;Run):<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;print("end_time:",&nbsp;run_obj.end_time)<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain&nbsp;=&nbsp;RunnableLambda(test_runnable).<a href="#ChatGoogleGenerativeAI-with_listeners">with_listeners</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on_start=fn_start,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;on_end=fn_end<br>
&nbsp;&nbsp;&nbsp;&nbsp;)<br>
&nbsp;&nbsp;&nbsp;&nbsp;chain.<a href="#ChatGoogleGenerativeAI-invoke">invoke</a>(2)</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-with_retry"><strong>with_retry</strong></a>(self, *, retry_if_exception_type: 'Tuple[Type[BaseException], ...]' = (&lt;class 'Exception'&gt;,), wait_exponential_jitter: 'bool' = True, stop_after_attempt: 'int' = 3) -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Create&nbsp;a&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;retries&nbsp;the&nbsp;original&nbsp;Runnable&nbsp;on&nbsp;exceptions.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;retry_if_exception_type:&nbsp;A&nbsp;tuple&nbsp;of&nbsp;exception&nbsp;types&nbsp;to&nbsp;retry&nbsp;on.<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Defaults&nbsp;to&nbsp;(Exception,).<br>
&nbsp;&nbsp;&nbsp;&nbsp;wait_exponential_jitter:&nbsp;Whether&nbsp;to&nbsp;add&nbsp;jitter&nbsp;to&nbsp;the&nbsp;wait<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;time&nbsp;between&nbsp;retries.&nbsp;Defaults&nbsp;to&nbsp;True.<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop_after_attempt:&nbsp;The&nbsp;maximum&nbsp;number&nbsp;of&nbsp;attempts&nbsp;to&nbsp;make&nbsp;before<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;giving&nbsp;up.&nbsp;Defaults&nbsp;to&nbsp;3.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;retries&nbsp;the&nbsp;original&nbsp;Runnable&nbsp;on&nbsp;exceptions.<br>
&nbsp;<br>
Example:<br>
&nbsp;<br>
..&nbsp;code-block::&nbsp;python<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;from&nbsp;langchain_core.runnables&nbsp;import&nbsp;RunnableLambda<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;count&nbsp;=&nbsp;0<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;def&nbsp;_lambda(x:&nbsp;int)&nbsp;-&gt;&nbsp;None:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;global&nbsp;count<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;count&nbsp;=&nbsp;count&nbsp;+&nbsp;1<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;if&nbsp;x&nbsp;==&nbsp;1:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;raise&nbsp;ValueError("x&nbsp;is&nbsp;1")<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;else:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pass<br>
&nbsp;<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;runnable&nbsp;=&nbsp;RunnableLambda(_lambda)<br>
&nbsp;&nbsp;&nbsp;&nbsp;try:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;runnable.<a href="#ChatGoogleGenerativeAI-with_retry">with_retry</a>(<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;stop_after_attempt=2,<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;retry_if_exception_type=(ValueError,),<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;).<a href="#ChatGoogleGenerativeAI-invoke">invoke</a>(1)<br>
&nbsp;&nbsp;&nbsp;&nbsp;except&nbsp;ValueError:<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;pass<br>
&nbsp;<br>
&nbsp;&nbsp;&nbsp;&nbsp;assert&nbsp;(count&nbsp;==&nbsp;2)<br>
&nbsp;<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;retry_if_exception_type:&nbsp;A&nbsp;tuple&nbsp;of&nbsp;exception&nbsp;types&nbsp;to&nbsp;retry&nbsp;on<br>
&nbsp;&nbsp;&nbsp;&nbsp;wait_exponential_jitter:&nbsp;Whether&nbsp;to&nbsp;add&nbsp;jitter&nbsp;to&nbsp;the&nbsp;wait&nbsp;time<br>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;between&nbsp;retries<br>
&nbsp;&nbsp;&nbsp;&nbsp;stop_after_attempt:&nbsp;The&nbsp;maximum&nbsp;number&nbsp;of&nbsp;attempts&nbsp;to&nbsp;make&nbsp;before&nbsp;giving&nbsp;up<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;that&nbsp;retries&nbsp;the&nbsp;original&nbsp;Runnable&nbsp;on&nbsp;exceptions.</tt></dd></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-with_types"><strong>with_types</strong></a>(self, *, input_type: 'Optional[Type[Input]]' = None, output_type: 'Optional[Type[Output]]' = None) -&gt; 'Runnable[Input, Output]'</dt><dd><tt>Bind&nbsp;input&nbsp;and&nbsp;output&nbsp;types&nbsp;to&nbsp;a&nbsp;Runnable,&nbsp;returning&nbsp;a&nbsp;new&nbsp;Runnable.<br>
&nbsp;<br>
Args:<br>
&nbsp;&nbsp;&nbsp;&nbsp;input_type:&nbsp;The&nbsp;input&nbsp;type&nbsp;to&nbsp;bind&nbsp;to&nbsp;the&nbsp;Runnable.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;&nbsp;&nbsp;&nbsp;output_type:&nbsp;The&nbsp;output&nbsp;type&nbsp;to&nbsp;bind&nbsp;to&nbsp;the&nbsp;Runnable.&nbsp;Defaults&nbsp;to&nbsp;None.<br>
&nbsp;<br>
Returns:<br>
&nbsp;&nbsp;&nbsp;&nbsp;A&nbsp;new&nbsp;Runnable&nbsp;with&nbsp;the&nbsp;types&nbsp;bound.</tt></dd></dl>

<hr>
Readonly properties inherited from <a href="langchain_core.runnables.base.html#Runnable">langchain_core.runnables.base.Runnable</a>:<br>
<dl><dt><strong>config_specs</strong></dt>
<dd><tt>List&nbsp;configurable&nbsp;fields&nbsp;for&nbsp;this&nbsp;Runnable.</tt></dd>
</dl>
<dl><dt><strong>input_schema</strong></dt>
<dd><tt>The&nbsp;type&nbsp;of&nbsp;input&nbsp;this&nbsp;Runnable&nbsp;accepts&nbsp;specified&nbsp;as&nbsp;a&nbsp;pydantic&nbsp;model.</tt></dd>
</dl>
<dl><dt><strong>output_schema</strong></dt>
<dd><tt>The&nbsp;type&nbsp;of&nbsp;output&nbsp;this&nbsp;Runnable&nbsp;produces&nbsp;specified&nbsp;as&nbsp;a&nbsp;pydantic&nbsp;model.</tt></dd>
</dl>
<hr>
Data and other attributes inherited from <a href="langchain_core.runnables.base.html#Runnable">langchain_core.runnables.base.Runnable</a>:<br>
<dl><dt><strong>name</strong> = None</dl>

<hr>
Class methods inherited from <a href="typing.html#Generic">typing.Generic</a>:<br>
<dl><dt><a name="ChatGoogleGenerativeAI-__class_getitem__"><strong>__class_getitem__</strong></a>(params)<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt></dl>

<dl><dt><a name="ChatGoogleGenerativeAI-__init_subclass__"><strong>__init_subclass__</strong></a>(*args, **kwargs)<font color="#909090"><font face="helvetica, arial"> from <a href="pydantic.v1.main.html#ModelMetaclass">pydantic.v1.main.ModelMetaclass</a></font></font></dt><dd><tt>This&nbsp;method&nbsp;is&nbsp;called&nbsp;when&nbsp;a&nbsp;class&nbsp;is&nbsp;subclassed.<br>
&nbsp;<br>
The&nbsp;default&nbsp;implementation&nbsp;does&nbsp;nothing.&nbsp;It&nbsp;may&nbsp;be<br>
overridden&nbsp;to&nbsp;extend&nbsp;subclasses.</tt></dd></dl>

</td></tr></table></td></tr></table><p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#eeaa77">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Functions</strong></big></font></td></tr>
    
<tr><td bgcolor="#eeaa77"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><dl><dt><a name="-init_chat_model"><strong>init_chat_model</strong></a>(proxy_client: gen_ai_hub.proxy.core.base.BaseProxyClient, deployment: gen_ai_hub.proxy.gen_ai_hub_proxy.client.Deployment, temperature: float = 0.0, max_tokens: int = 256, top_k: Optional[int] = None, top_p: float = 1.0)</dt></dl>
</td></tr></table><p>
<table width="100%" cellspacing=0 cellpadding=2 border=0 summary="section">
<tr bgcolor="#55aa55">
<td colspan=3 valign=bottom>&nbsp;<br>
<font color="#ffffff" face="helvetica, arial"><big><strong>Data</strong></big></font></td></tr>
    
<tr><td bgcolor="#55aa55"><tt>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</tt></td><td>&nbsp;</td>
<td width="100%"><strong>Dict</strong> = typing.Dict<br>
<strong>Optional</strong> = typing.Optional<br>
<strong>catalog</strong> = &lt;gen_ai_hub.proxy.langchain.init_models.Catalog object&gt;</td></tr></table>
</body></html>