{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f928a651ed440cb7",
      "metadata": {},
      "source": [
        "# HyperSHAP: Creating an ExplanationTask\n",
        "\n",
        "In this example, we will walk through multiple ways of how to create an explanation task for your specific HPO problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df4744f0bb751909",
      "metadata": {},
      "source": [
        "## Setup Mockup Environment\n",
        "\n",
        "To this end, we first setup some basic environment, which is assumed to be already existing when working with HyperSHAP. We will setup a configuration space and the black-box evaluation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3419e0353198349",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-13T13:25:16.847861Z",
          "start_time": "2025-08-13T13:25:16.843323Z"
        }
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "from ConfigSpace import Configuration, ConfigurationSpace\n",
        "\n",
        "# Configuration space with a float, integer, and categorical hyperparameter\n",
        "cs = ConfigurationSpace(\n",
        "    name=\"myspace\",\n",
        "    space={\n",
        "        \"a\": (0.1, 1.5),  # UniformFloat\n",
        "        \"b\": (2, 10),  # UniformInt\n",
        "        \"c\": [\"X\", \"Y\"],  # Categorical\n",
        "    },\n",
        ")\n",
        "\n",
        "\n",
        "# Some evaluation function that assesses the performance of some configuration\n",
        "def eval_fun(config: Configuration) -> float:\n",
        "    if config[\"c\"] == \"X\":\n",
        "        return math.sin(config[\"a\"]) + config[\"b\"]\n",
        "    elif config[\"c\"] == \"Y\":\n",
        "        return math.cos(config[\"a\"] * config[\"b\"]) + 1.5\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "cs.seed(42)  # set some random seed for reproducibility"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cabf02d8cc409a50",
      "metadata": {},
      "source": [
        "## Create ExplanationTask from Black-Box Function\n",
        "\n",
        "Maybe the easiest way to create an explanation task is to ask HyperSHAP to take care of everything. To this end, we can simply create an explanation task by just providing the configuration space and the black-box function. Additionally, we can specify the number of times the black-box function may be sampled and the type of regressor we would like to use here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "faac84b51713fdcc",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-13T13:25:18.647810Z",
          "start_time": "2025-08-13T13:25:18.106778Z"
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from hypershap.task import ExplanationTask\n",
        "\n",
        "# creating the ExplanationTask\n",
        "et = ExplanationTask.from_function(\n",
        "    config_space=cs,\n",
        "    function=eval_fun,\n",
        "    n_samples=1000,  # optional\n",
        "    base_model=RandomForestRegressor(),  # optional\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4f62902afd0ef13",
      "metadata": {},
      "source": [
        "## Create an ExplanationTask from Evaluation Data\n",
        "\n",
        "Maybe you already have some evaluated configurations that you would like to simply use to serve as a backbone in HyperSHAP for explaining the impact of hyperparameters on performance, then you can also simply provide that data to create an ExplanationTask. For the example, we will first sample a number of random configurations and evaluate those random configurations. Then, we will provide this data to the HyperSHAP to create an ExplanationTask. Optionally, we can choose again the type of base model we would like to build the surrogate model with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "initial_id",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-13T13:25:19.934562Z",
          "start_time": "2025-08-13T13:25:19.823304Z"
        },
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from hypershap.task import ExplanationTask\n",
        "\n",
        "# mockup data\n",
        "configuration_list = cs.sample_configuration(size=1_000)\n",
        "performances = [eval_fun(configuration) for configuration in configuration_list]\n",
        "data = list(zip(configuration_list, performances))\n",
        "\n",
        "# creating the ExplanationTask\n",
        "et = ExplanationTask.from_data(\n",
        "    config_space=cs,\n",
        "    data=data,\n",
        "    base_model=RandomForestRegressor(),  # optional\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b979d4de8285e27c",
      "metadata": {},
      "source": [
        "# Create an ExplanationTask from Existing Model\n",
        "\n",
        "Maybe you already have a model ready to be used to predict the performance of configurations. Please make sure that your model is trained based on the encoding the ConfigSpace package provides via the `get_array()` encoding. This is crucial to ensure proper functioning with the remaining framework of HyperSHAP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9b00650dd46284d6",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-08-13T13:25:21.075964Z",
          "start_time": "2025-08-13T13:25:20.984057Z"
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from hypershap.task import ExplanationTask\n",
        "\n",
        "# mockup surrogate model\n",
        "X = np.array([configuration.get_array() for configuration in configuration_list])\n",
        "y = np.array(performances)\n",
        "model = RandomForestRegressor()\n",
        "model.fit(X, y)\n",
        "\n",
        "# creating the ExplanationTask\n",
        "et = ExplanationTask.from_base_model(config_space=cs, base_model=model)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
