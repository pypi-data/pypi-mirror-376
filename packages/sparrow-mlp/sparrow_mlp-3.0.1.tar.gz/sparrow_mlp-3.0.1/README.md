# Sparrow ğŸ¦: A Dynamic MLP Architecture

[![PyPI version](https://badge.fury.io/py/sparrow-mlp.svg)](https://badge.fury.io/py/sparrow-mlp)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**[English](#english) | [ÙØ§Ø±Ø³ÛŒ](#ÙØ§Ø±Ø³ÛŒ)**

---

## English

Sparrow is a PyTorch library for building **Dynamic Multi-Layer Perceptrons (MLPs)**. This architecture learns its own optimal structure for any given task by combining two powerful concepts:

1.  **Dynamic Depth**: A global router learns to activate or bypass entire hidden layers based on the input, finding the shortest computational path needed.
2.  **Mixture of Experts (MoE)**: Each hidden layer is composed of several smaller "expert" networks. A local router within each layer selects the best expert for the current data, enabling neuron specialization and efficient computation.

This results in a highly efficient and adaptive neural network that prunes itself during training.

### Key Features
-   **Dynamic Depth**: Automatically learns which layers to skip.
-   **Mixture of Experts Layers**: Activates only a subset of neurons in each layer.
-   **Self-Pruning**: Learns to become more computationally efficient as it masters a task.
-   **Simple API**: Build complex dynamic models with just a few lines of code.

### Installation
We recommend creating a new virtual environment.
```bash
pip install sparrow-mlp
```

### Quickstart
Here is a complete example of training a `DynamicMLP` on the Iris dataset.
```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sparrow import DynamicMLP

# 1. Load and prepare data
iris = load_iris()
X, y = iris.data, iris.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
X_train_tensor = torch.FloatTensor(X_train)
y_train_tensor = torch.LongTensor(y_train)
X_test_tensor = torch.FloatTensor(X_test)
y_test_tensor = torch.LongTensor(y_test)

# 2. Define the DynamicMLP model
model = DynamicMLP(
    input_size=4,
    output_size=3,
    hidden_dim=32,
    num_hidden_layers=2,
    num_experts=4,
    expert_hidden_size=16
)

optimizer = optim.Adam(model.parameters(), lr=0.005)
classification_criterion = nn.CrossEntropyLoss()
epochs = 300
LAYER_SPARSITY_LAMBDA = 0.01

# 3. Train the model
for epoch in range(epochs):
    model.train()
    optimizer.zero_grad()
    outputs = model(X_train_tensor)
    classification_loss = classification_criterion(outputs, y_train_tensor)
    # Add sparsity loss to encourage layer skipping
    layer_sparsity_loss = LAYER_SPARSITY_LAMBDA * model.layer_gates_values.sum()
    total_loss = classification_loss + layer_sparsity_loss
    total_loss.backward()
    optimizer.step()

# 4. Evaluate the model
model.eval()
with torch.no_grad():
    test_outputs = model(X_test_tensor)
    _, predicted_labels = torch.max(test_outputs, 1)
    accuracy = accuracy_score(y_test_tensor.numpy(), predicted_labels.numpy())
    print(f'Final Accuracy on Iris Test Data: {accuracy * 100:.2f}%')
```
---

## ÙØ§Ø±Ø³ÛŒ

`Sparrow` ğŸ¦ ÛŒÚ© Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ù¾Ø§ÛŒØªÙˆØ±Ú† Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª **Ù¾Ø±Ø³Ù¾ØªØ±ÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯Ù„Ø§ÛŒÙ‡ Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© (MLP)** Ø§Ø³Øª. Ø§ÛŒÙ† Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø³Ø§Ø®ØªØ§Ø± Ø¨Ù‡ÛŒÙ†Ù‡ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆØ¸ÛŒÙÙ‡ Ø¨Ø§ ØªØ±Ú©ÛŒØ¨ Ø¯Ùˆ Ù…ÙÙ‡ÙˆÙ… Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯:

Û±. **Ø¹Ù…Ù‚ Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ©**: ÛŒÚ© Ù…Ø³ÛŒØ±ÛŒØ§Ø¨ Ø³Ø±Ø§Ø³Ø±ÛŒ ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ú©Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆØ±ÙˆØ¯ÛŒØŒ Ú©Ù„ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù¾Ù†Ù‡Ø§Ù† Ø±Ø§ ÙØ¹Ø§Ù„ ÛŒØ§ Ø§Ø² Ø¢Ù†Ù‡Ø§ Ø¹Ø¨ÙˆØ± Ú©Ù†Ø¯ Ùˆ Ú©ÙˆØªØ§Ù‡â€ŒØªØ±ÛŒÙ† Ù…Ø³ÛŒØ± Ù…Ø­Ø§Ø³Ø¨Ø§ØªÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ù†Ø¯.
Û². **ØªØ±Ú©ÛŒØ¨ÛŒ Ø§Ø² Ù…ØªØ®ØµØµØ§Ù† (MoE)**: Ù‡Ø± Ù„Ø§ÛŒÙ‡ Ù¾Ù†Ù‡Ø§Ù† Ø§Ø² Ú†Ù†Ø¯ÛŒÙ† Ø´Ø¨Ú©Ù‡ "Ù…ØªØ®ØµØµ" Ú©ÙˆÚ†Ú©ØªØ± ØªØ´Ú©ÛŒÙ„ Ø´Ø¯Ù‡ Ø§Ø³Øª. ÛŒÚ© Ù…Ø³ÛŒØ±ÛŒØ§Ø¨ Ù…Ø­Ù„ÛŒ Ø¯Ø± Ù‡Ø± Ù„Ø§ÛŒÙ‡ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…ØªØ®ØµØµ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ ÙØ¹Ù„ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ù…Ù†Ø¬Ø± Ø¨Ù‡ ØªØ®ØµØµÛŒ Ø´Ø¯Ù† Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ Ùˆ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

Ù†ØªÛŒØ¬Ù‡ Ø§ÛŒÙ† ØªØ±Ú©ÛŒØ¨ØŒ ÛŒÚ© Ø´Ø¨Ú©Ù‡ Ø¹ØµØ¨ÛŒ Ø¨Ø³ÛŒØ§Ø± Ú©Ø§Ø±Ø¢Ù…Ø¯ Ùˆ ØªØ·Ø¨ÛŒÙ‚â€ŒÙ¾Ø°ÛŒØ± Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± Ø·ÙˆÙ„ Ø¢Ù…ÙˆØ²Ø´ Ø®ÙˆØ¯ Ø±Ø§ Ù‡Ø±Ø³ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.

### Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ
- **Ø¹Ù…Ù‚ Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ©**: Ø¨Ù‡ Ø·ÙˆØ± Ø®ÙˆØ¯Ú©Ø§Ø± ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ú©Ø¯Ø§Ù… Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ Ø±Ø§ Ø±Ø¯ Ú©Ù†Ø¯.
- **Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ MoE**: ØªÙ†Ù‡Ø§ Ø²ÛŒØ±Ù…Ø¬Ù…ÙˆØ¹Ù‡â€ŒØ§ÛŒ Ø§Ø² Ù†ÙˆØ±ÙˆÙ†â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø± Ù‡Ø± Ù„Ø§ÛŒÙ‡ ÙØ¹Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
- **Ù‡Ø±Ø³ Ø®ÙˆØ¯Ú©Ø§Ø±**: Ø¨Ø§ Ù…Ø³Ù„Ø· Ø´Ø¯Ù† Ø¨Ø± ÛŒÚ© ÙˆØ¸ÛŒÙÙ‡ØŒ ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯ Ú©Ù‡ Ø§Ø² Ù†Ø¸Ø± Ù…Ø­Ø§Ø³Ø¨Ø§ØªÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØªØ± Ø´ÙˆØ¯.
- **API Ø³Ø§Ø¯Ù‡**: Ø³Ø§Ø®Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÙ†Ø§Ù…ÛŒÚ© Ù¾ÛŒÚ†ÛŒØ¯Ù‡ ØªÙ†Ù‡Ø§ Ø¨Ø§ Ú†Ù†Ø¯ Ø®Ø· Ú©Ø¯.

### Ù†ØµØ¨
ØªÙˆØµÛŒÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ ÛŒÚ© Ù…Ø­ÛŒØ· Ù…Ø¬Ø§Ø²ÛŒ Ø¬Ø¯ÛŒØ¯ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†ÛŒØ¯.
```bash
pip install sparrow-mlp
```
### Ø´Ø±ÙˆØ¹ Ø³Ø±ÛŒØ¹
Ø¯Ø± Ø¨Ø§Ù„Ø§ ÛŒÚ© Ù…Ø«Ø§Ù„ Ú©Ø§Ù…Ù„ Ø§Ø² Ø¢Ù…ÙˆØ²Ø´ `DynamicMLP` Ø±ÙˆÛŒ Ø¯ÛŒØªØ§Ø³Øª Ú¯Ù„ Ø²Ù†Ø¨Ù‚ Ø¢Ù…Ø¯Ù‡ Ø§Ø³Øª.