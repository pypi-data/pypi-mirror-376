{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb7f461",
   "metadata": {},
   "source": [
    "# Analyzing machine learning model training results #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d9cd7",
   "metadata": {},
   "source": [
    "## A NOTE BEFORE STARTING ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880421e9",
   "metadata": {},
   "source": [
    "Since the ``emicroml`` git repository tracks this notebook under its original\n",
    "basename ``analyzing_ml_model_training_results.ipynb``, we recommend that you\n",
    "copy the original notebook and rename it to any other basename that is not one\n",
    "of the original basenames that appear in the ``<root>/examples`` directory\n",
    "before executing any of the notebook cells below, where ``<root>`` is the root\n",
    "of the ``emicroml`` repository. For example, you could rename it\n",
    "``analyzing_ml_model_training_results_sandbox.ipynb``. This way you can explore\n",
    "the notebook by executing and modifying cells without changing the original\n",
    "notebook, which is being tracked by git."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b0d90e",
   "metadata": {},
   "source": [
    "## Import necessary modules ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f9787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pattern matching.\n",
    "import re\n",
    "\n",
    "# For listing files and subdirectories in a given directory, and for renaming\n",
    "# directories.\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# For general array handling.\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# For loading objects from HDF5 files.\n",
    "import h5pywrappers\n",
    "\n",
    "# For creating and plotting figures.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "\n",
    "\n",
    "\n",
    "# For loading ML datasets and models for distortion estimation in CBED.\n",
    "import emicroml.modelling.cbed.distortion.estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d0485-cf65-478b-80c4-d5d294879c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e667d633",
   "metadata": {},
   "source": [
    "## Introduction ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f2509",
   "metadata": {},
   "source": [
    "In this notebook, we analyze the output that results from performing the\n",
    "\"actions\" described in the following pages:\n",
    "\n",
    "1. [Generating machine learning datasets for training and validation](https://mrfitzpa.github.io/emicroml/examples/modelling/cbed/distortion/estimation/generate_ml_datasets_for_training_and_validation.html)\n",
    "2. [Combining then splitting machine learning datasets for training and validation](https://mrfitzpa.github.io/emicroml/examples/modelling/cbed/distortion/estimation/combine_ml_datasets_for_training_and_validation_then_split.html)\n",
    "3. [Training machine learning models](https://mrfitzpa.github.io/emicroml/examples/modelling/cbed/distortion/estimation/train_ml_model_set.html)\n",
    "\n",
    "while also demonstrating how one can use a selection of the functions and\n",
    "classes in the module\n",
    "[emicroml.modelling.cbed.distortion.estimation](https://mrfitzpa.github.io/emicroml/_autosummary/emicroml.modelling.cbed.distortion.estimation.html#module-emicroml.modelling.cbed.distortion.estimation).\n",
    "In short, in this notebook we analyze machine learning (ML) model training\n",
    "results for the ML task of estimating distortion in convergent beam electron\n",
    "diffraction (CBED).\n",
    "\n",
    "In order to execute the cells in this notebook as intended, a set of Python\n",
    "libraries need to be installed in the Python environment within which the cells\n",
    "of the notebook are to be executed. See [this\n",
    "page](https://mrfitzpa.github.io/emicroml/examples/prerequisites_for_execution_without_slurm.html)\n",
    "for instructions on how to do so. Additionally, a subset of the output that\n",
    "results from performing the aforementioned actions is required to execute the\n",
    "cells in this notebook as intended. One can obtain this subset of output by\n",
    "executing said actions, however this requires significant computational\n",
    "resources, including significant walltime. Alternatively, one can copy this\n",
    "subset of output from a Federated Research Data Repository dataset by following\n",
    "the instructions given on [this\n",
    "page](https://mrfitzpa.github.io/emicroml/examples/modelling/cbed/distortion/estimation/copying_subset_of_output_from_frdr_dataset.html).\n",
    "\n",
    "You can find the documentation for the ``emicroml`` library\n",
    "[here](https://mrfitzpa.github.io/emicroml/_autosummary/emicroml.html).\n",
    "It is recommended that you consult the documentation of this\n",
    "library as you explore the notebook. Moreover, users should\n",
    "execute the cells in the order that they appear, i.e. from top to\n",
    "bottom, as some cells reference variables that are set in other\n",
    "cells above them. **Users should make sure to navigate the\n",
    "documentation for the version of ``emicroml`` that they are\n",
    "currently using.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9445a5e-76d1-4957-8399-c8638ab57d0c",
   "metadata": {},
   "source": [
    "## Loading and analyzing the ML training dataset ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199654f0-fbb4-4ce9-9ec2-062cd4c7348c",
   "metadata": {},
   "source": [
    "Upon successful completion of the action described in the page [Combining then\n",
    "splitting machine learning datasets for training and\n",
    "validation](https://mrfitzpa.github.io/emicroml/examples/modelling/cbed/distortion/estimation/combine_ml_datasets_for_training_and_validation_then_split.html),\n",
    "ML training and validation datasets are stored in the HDF5 files at the file\n",
    "paths ``../data/ml_datasets/ml_dataset_for_training.h5`` and\n",
    "``../data/ml_datasets/ml_dataset_for_validation.h5`` respectively.\n",
    "\n",
    "Let's look at some of the ML data instances stored in the ML training dataset,\n",
    "namely the first five to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2ce174-3d47-433e-8726-76709c896b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data_dir = \"../data\"\n",
    "path_to_ml_dataset = (path_to_data_dir\n",
    "                      + \"/ml_datasets/ml_dataset_for_training.h5\")\n",
    "\n",
    "module_alias = emicroml.modelling.cbed.distortion.estimation\n",
    "kwargs = {\"path_to_ml_dataset\": path_to_ml_dataset, \n",
    "          \"entire_ml_dataset_is_to_be_cached\": False, \n",
    "          \"ml_data_values_are_to_be_checked\": False}\n",
    "ml_dataset = module_alias.MLDataset(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "single_dim_slice = slice(0, 5)\n",
    "\n",
    "kwargs = {\"single_dim_slice\": single_dim_slice, \n",
    "          \"unnormalize_normalizable_elems\": True}\n",
    "ml_data_instances = ml_dataset.get_ml_data_instances(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9662376b-9b46-4f09-a42e-1083abb01f94",
   "metadata": {},
   "source": [
    "Next, let's print the names of all the features of the ML data instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00460526-83c8-4207-ae8c-053e08f1b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in ml_data_instances:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f091518e-69b0-442d-8e71-a82b7358529a",
   "metadata": {},
   "source": [
    "The features of the ML data instances are described in detail\n",
    "[here](https://mrfitzpa.github.io/emicroml/_autosummary/emicroml.modelling.cbed.distortion.estimation.generate_and_save_ml_dataset.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7192914e-9635-46d6-ac98-18a164401b78",
   "metadata": {},
   "source": [
    "The normalizable features/elements of ML data instances stored in the HDF5 files\n",
    "are expected to be min-max normalized. To determine whether a feature is\n",
    "normalizable, simply check either the normalization weights or biases of the ML\n",
    "dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf55fc0-2fbc-41e6-b4f2-13f3cd818b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_dataset.normalization_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e71c887-2c34-476e-ba19-27dc8390b9c6",
   "metadata": {},
   "source": [
    "If the name of a feature appears in the above dictionary, then that feature is\n",
    "normalizable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67966af-991d-4dca-8de1-f38d0c519cea",
   "metadata": {},
   "source": [
    "In the first code block of this section, we loaded a subset of ML data instances\n",
    "and then subsequently unnormalized the normalizable features of said\n",
    "subset. This was done in a single call to the method\n",
    "[emicroml.modelling.cbed.distortion.estimation.MLDataset.get_ml_data_instances](https://mrfitzpa.github.io/emicroml/_autosummary/emicroml.modelling.cbed.distortion.estimation.MLDataset.html#emicroml.modelling.cbed.distortion.estimation.MLDataset.get_ml_data_instances).\n",
    "\n",
    "Alternatively, we can do this in two steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ffc8e-9f9e-4161-a5b0-efbe49da922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"single_dim_slice\": single_dim_slice, \n",
    "          \"unnormalize_normalizable_elems\": False}\n",
    "ml_data_instances = ml_dataset.get_ml_data_instances(**kwargs)\n",
    "\n",
    "# The following code block modifies ``ml_data_instances`` in place.\n",
    "module_alias = emicroml.modelling.cbed.distortion.estimation\n",
    "kwargs = {\"ml_data_dict\": ml_data_instances,\n",
    "          \"normalization_weights\": ml_dataset.normalization_weights,\n",
    "          \"normalization_biases\": ml_dataset.normalization_biases}\n",
    "module_alias.unnormalize_normalizable_elems_in_ml_data_dict(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f706060b-ecc9-4968-86e2-308a697accf5",
   "metadata": {},
   "source": [
    "For completeness, we show the inverse operation of normalizing the unnormalized\n",
    "normalizable features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce17073-86bc-4360-a561-591bcb71386a",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_alias = emicroml.modelling.cbed.distortion.estimation\n",
    "kwargs = {\"ml_data_dict\": ml_data_instances,\n",
    "          \"normalization_weights\": ml_dataset.normalization_weights,\n",
    "          \"normalization_biases\": ml_dataset.normalization_biases}\n",
    "module_alias.normalize_normalizable_elems_in_ml_data_dict(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b3ad1f-2032-48f4-ae65-b54912ede9c1",
   "metadata": {},
   "source": [
    "Rather than load the subset of ML data instances as a dictionary of arrays, we\n",
    "can load the subset as a sequence `HyperSpy` signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c32282-3c15-4bae-be94-5c716848e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbed_pattern_image = ml_data_instances[\"cbed_pattern_images\"][0]\n",
    "sampling_grid_dims_in_pixels = cbed_pattern_image.shape\n",
    "\n",
    "kwargs = \\\n",
    "    {\"single_dim_slice\": single_dim_slice, \n",
    "     \"sampling_grid_dims_in_pixels\": sampling_grid_dims_in_pixels}\n",
    "ml_data_instances_as_signals = \\\n",
    "    ml_dataset.get_ml_data_instances_as_signals(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9f1e1e-5831-4f4c-934c-abc7cc89f7d7",
   "metadata": {},
   "source": [
    "Let's plot one of the signals, which represents a single ML data instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd3273-6fa5-4561-a8c6-7cd3c0047aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbed_pattern_idx = 0\n",
    "\n",
    "ml_data_instance_as_signal = ml_data_instances_as_signals[cbed_pattern_idx]\n",
    "\n",
    "kwargs = {\"axes_off\": True, \n",
    "          \"scalebar\": False, \n",
    "          \"colorbar\": False, \n",
    "          \"gamma\": 0.2,\n",
    "          \"cmap\": \"plasma\", \n",
    "          \"title\": \"\"}\n",
    "ml_data_instance_as_signal.plot(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ddb166-53ff-42de-874f-41394346313e",
   "metadata": {},
   "source": [
    "And here's the signal's metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c05c77-fdc8-4578-91ad-0a552519b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data_instance_as_signal.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042090c6-95f0-4733-94d6-7d423b787716",
   "metadata": {},
   "source": [
    "See\n",
    "[here](https://mrfitzpa.github.io/fakecbed/_autosummary/fakecbed.discretized.CBEDPattern.html#fakecbed.discretized.CBEDPattern.signal)\n",
    "for a detailed description of the signal data and metadata. See also\n",
    "[here](https://mrfitzpa.github.io/emicroml/_autosummary/emicroml.modelling.cbed.distortion.estimation.MLDataset.html#emicroml.modelling.cbed.distortion.estimation.MLDataset.get_ml_data_instances_as_signals)\n",
    "for additional context. Note that some, if not many, of these simulated CBED\n",
    "patterns do not appear realistic. This is deliberate as we are primarily\n",
    "concerned with generating ML datasets storing simulated CBED patterns that\n",
    "capture the essential geometric features of real CBED patterns, namely perfectly\n",
    "circular CBED disks in the absence of distortion. The placement of the CBED\n",
    "disks are often unusual, but this is also deliberate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6c582d-5dcf-4d64-ada2-37aaa1af1182",
   "metadata": {},
   "source": [
    "An alternative way to load a subset of ML data instances as a sequence of\n",
    "`HyperSpy` signals is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cc94f8-4909-4229-aa68-92ae2455f6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"single_dim_slice\": single_dim_slice, \n",
    "          \"unnormalize_normalizable_elems\": True}\n",
    "ml_data_instances = ml_dataset.get_ml_data_instances(**kwargs)\n",
    "\n",
    "module_alias = emicroml.modelling.cbed.distortion.estimation\n",
    "kwargs = {\"ml_data_dict\": ml_data_instances,\n",
    "          \"sampling_grid_dims_in_pixels\": sampling_grid_dims_in_pixels}\n",
    "ml_data_instances_as_signals = module_alias.ml_data_dict_to_signals(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602148b6-6317-4b5a-962b-37941cac76ad",
   "metadata": {},
   "source": [
    "## Loading and analyzing ML model training summary output data ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ded79-6ab2-4fb9-b81e-b36620bb6010",
   "metadata": {},
   "source": [
    "Upon successful completion of the action described in the page [Training machine\n",
    "learning\n",
    "models](https://mrfitzpa.github.io/emicroml/examples/modelling/cbed/distortion/estimation/train_ml_model_set.html),\n",
    "10 ML models are trained, and a dictionary representation of each ML model is\n",
    "saved to a file. Moreover, the ML model training summary output data for each\n",
    "trained ML model is saved to a HDF5 file. ML model training summary output files\n",
    "are described in detail in the documentation for the method\n",
    "[emicroml.modelling.cbed.distortion.estimation.MLModelTrainer.train_ml_model](https://mrfitzpa.github.io/emicroml/_autosummary/emicroml.modelling.cbed.distortion.estimation.MLModelTrainer.html#emicroml.modelling.cbed.distortion.estimation.MLModelTrainer.train_ml_model).\n",
    "Let's examine the contents of one such HDF5 file.\n",
    "\n",
    "Let's start by looking at the ML model training parameters used to train one of\n",
    "the ML models. Said parameters have been serialized and stored in the HDF5 file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca09896-fb95-4c5e-8086-d9caa9cd4ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_ml_model_training_summary_output_data = \\\n",
    "    (path_to_data_dir\n",
    "     + \"/ml_models/ml_model_1\"\n",
    "     + \"/ml_model_training_summary_output_data.h5\")\n",
    "\n",
    "\n",
    "\n",
    "kwargs = {\"filename\": path_to_ml_model_training_summary_output_data,\n",
    "          \"path_in_file\": \"ml_model_trainer_params\"}\n",
    "json_document_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "serializable_rep_of_ml_model_trainer_params = \\\n",
    "    h5pywrappers.json.document.load(json_document_id)\n",
    "\n",
    "serializable_rep_of_ml_model_trainer_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39de908a-e13b-4317-b34a-b8e7f4d1c016",
   "metadata": {},
   "source": [
    "The above output is a serialized representation of the ML model trainer\n",
    "parameters used to train the ML model. ML model trainers are represented by the\n",
    "[emicroml.modelling.cbed.distortion.estimation.MLModelTrainer](https://mrfitzpa.github.io/emicroml/_autosummary/emicroml.modelling.cbed.distortion.estimation.MLModelTrainer.html)\n",
    "class. Instances of this class are used to train ML models. We can construct an\n",
    "instance of this class using the above serialized representation of the ML model\n",
    "trainer parameters, as long as the parameter values are valid, including those\n",
    "that specify the paths to the ML training and validation datasets (to be)\n",
    "used. It's possible that these paths are invalid in\n",
    "``serializable_rep_of_ml_model_trainer_param`` as they may refer to temporary\n",
    "locations of the ML training and validation datasets during training, e.g. if\n",
    "the training was performed in a SLURM job. For demonstration purposes, let's\n",
    "reset these paths to ensure a successful construction of an instance of the\n",
    "above class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1920b4-8aba-49aa-8a81-55be123062da",
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = (\"training\", \"validation\")\n",
    "\n",
    "for phase in phases:\n",
    "    key_1 = \"ml_dataset_manager\"\n",
    "    key_2 = \"ml_{}_dataset\".format(phase)\n",
    "    key_3 = \"path_to_ml_dataset\"\n",
    "\n",
    "    path_to_ml_dataset = (path_to_data_dir\n",
    "                          + \"/ml_datasets\"\n",
    "                          + \"/ml_dataset_for_{}.h5\".format(phase))\n",
    "\n",
    "    serializable_rep_of_ml_model_trainer_params[key_1][key_2][key_3] = \\\n",
    "        path_to_ml_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7b437b-63f1-4789-9e33-0d013688e188",
   "metadata": {},
   "source": [
    "Now we can construct the ML model trainer using the serialized representation of\n",
    "the ML model trainer parameters (note that this may take some time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1591bba3-4fa7-4155-9f22-9bebd5e24dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_alias = emicroml.modelling.cbed.distortion.estimation\n",
    "kwargs = {\"serializable_rep\": serializable_rep_of_ml_model_trainer_params}\n",
    "ml_model_trainer = module_alias.MLModelTrainer.de_pre_serialize(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1cf856-4cc2-47ef-8638-2fea94505313",
   "metadata": {},
   "source": [
    "Apart from parameters specifying paths, this ML model trainer has the same\n",
    "parameters as those used to train the ML model referenced in the page [Training\n",
    "a machine learning\n",
    "model](https://mrfitzpa.github.io/emicroml/examples/modelling/cbed/distortion/estimation/train_ml_model_set.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf54f716-22bd-468a-bf71-e4ad13ee44bb",
   "metadata": {},
   "source": [
    "From the ML model training summary output data file, we can also extract the\n",
    "number of training and validation mini-batches processed per epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffa386e-977b-4d5a-9e8e-eaf1df5e552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for phase in phases:\n",
    "    hdf5_dataset_path = \"num_{}_mini_batches_per_epoch\".format(phase)\n",
    "    \n",
    "    kwargs = {\"filename\": path_to_ml_model_training_summary_output_data,\n",
    "              \"path_in_file\": hdf5_dataset_path}\n",
    "    hdf5_dataset_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "    kwargs = {\"dataset_id\": hdf5_dataset_id, \"read_only\": True}\n",
    "    num_mini_batches_per_epoch = h5pywrappers.dataset.load(**kwargs)[()]\n",
    "\n",
    "    unformatted_msg = \"# {} mini-batches per epoch: {}\"\n",
    "    msg = unformatted_msg.format(phase, num_mini_batches_per_epoch)\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd337816-a7d0-4690-bf7e-4bcf01603c10",
   "metadata": {},
   "source": [
    "We can also extract the learning rate schedules used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb209756-e0bd-4577-848f-8654417866e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"filename\": path_to_ml_model_training_summary_output_data,\n",
    "          \"path_in_file\": \"/lr_schedules/lr_schedule_0\"}\n",
    "hdf5_dataset_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "kwargs = {\"dataset_id\": hdf5_dataset_id, \"read_only\": True}\n",
    "lr_schedule = h5pywrappers.dataset.load(**kwargs)[()]\n",
    "\n",
    "\n",
    "\n",
    "kwargs = {\"obj_id\": hdf5_dataset_id, \"attr_name\": \"dim_0\"}\n",
    "attr_id = h5pywrappers.attr.ID(**kwargs)\n",
    "\n",
    "kwargs = {\"attr_id\": attr_id}\n",
    "dim_0_of_lr_schedule = h5pywrappers.attr.load(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "y = lr_schedule\n",
    "x = np.arange(y.size).astype(\"float\")\n",
    "if dim_0_of_lr_schedule == \"training mini batch instance idx\":\n",
    "    kwargs = {\"filename\": path_to_ml_model_training_summary_output_data,\n",
    "              \"path_in_file\": \"/num_training_mini_batches_per_epoch\"}\n",
    "    hdf5_dataset_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "    kwargs = {\"dataset_id\": hdf5_dataset_id, \"read_only\": True}\n",
    "    num_mini_batches_per_epoch = h5pywrappers.dataset.load(**kwargs)[()]\n",
    "\n",
    "    x /= num_mini_batches_per_epoch.astype(\"float\")\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x, y)\n",
    "\n",
    "axis_label_font_size = 15\n",
    "ax.set_xlabel(\"epoch\", \n",
    "              fontsize=axis_label_font_size)\n",
    "ax.set_ylabel(\"learning rate (dimensionless)\", \n",
    "              fontsize=axis_label_font_size)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "\n",
    "for spatial_dim in (\"x\", \"y\"):\n",
    "    major_tick_width = 1.5\n",
    "    major_tick_length = 8\n",
    "    minor_tick_width = major_tick_width\n",
    "    minor_tick_length = major_tick_length//2\n",
    "    tick_label_size = 15\n",
    "    linewidth = major_tick_width\n",
    "        \n",
    "    kwargs = {\"axis\": spatial_dim,\n",
    "              \"which\": \"major\",\n",
    "              \"direction\": \"in\",\n",
    "              \"left\": True,\n",
    "              \"right\": True, \n",
    "              \"width\": major_tick_width, \n",
    "              \"length\": major_tick_length, \n",
    "              \"labelsize\": tick_label_size}\n",
    "    ax.tick_params(**kwargs)\n",
    "\n",
    "    kwargs[\"which\"] = \"minor\"\n",
    "    kwargs[\"width\"] = minor_tick_width\n",
    "    kwargs[\"length\"] = minor_tick_length\n",
    "    ax.tick_params(**kwargs)\n",
    "\n",
    "    for side in ['top','bottom','left','right']:\n",
    "        ax.spines[side].set_linewidth(linewidth)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583a529e-3546-43e6-8280-32e5c0944ac6",
   "metadata": {},
   "source": [
    "We can also extract the metrics used to track ML model performance during\n",
    "training. In the cell below, for both the training and validation phases, we\n",
    "plot the first, second, and third quartiles per epoch of the end-point error\n",
    "(EPE) of the predicted \"adjusted\" standard distortion field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61a2797-0377-45b9-8493-4517080c612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "\n",
    "serializable_rep_of_ml_dataset_manager = \\\n",
    "    serializable_rep_of_ml_model_trainer_params[\"ml_dataset_manager\"]\n",
    "mini_batch_size = \\\n",
    "    serializable_rep_of_ml_dataset_manager[\"mini_batch_size\"]\n",
    "\n",
    "\n",
    "\n",
    "phase_to_color_map = {\"training\": \"yellow\", \"validation\": \"green\"}\n",
    "\n",
    "\n",
    "\n",
    "for phase in phase_to_color_map:\n",
    "    hdf5_dataset_path = (\"/ml_data_instance_metrics\"\n",
    "                         \"/{}/epes_of_adjusted_distortion_fields\".format(phase))\n",
    "    \n",
    "    kwargs = {\"filename\": path_to_ml_model_training_summary_output_data,\n",
    "              \"path_in_file\": hdf5_dataset_path}\n",
    "    hdf5_dataset_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "    kwargs = {\"dataset_id\": hdf5_dataset_id, \"read_only\": True}\n",
    "    epes_of_adjust_distortion_fields = h5pywrappers.dataset.load(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "    hdf5_dataset_path = \"num_{}_mini_batches_per_epoch\".format(phase)\n",
    "    \n",
    "    kwargs = {\"filename\": path_to_ml_model_training_summary_output_data,\n",
    "              \"path_in_file\": hdf5_dataset_path}\n",
    "    hdf5_dataset_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "    kwargs = {\"dataset_id\": hdf5_dataset_id, \"read_only\": True}\n",
    "    num_mini_batches_per_epoch = h5pywrappers.dataset.load(**kwargs)[()].item()\n",
    "\n",
    "    \n",
    "\n",
    "    num_ml_data_instances_in_ml_dataset = \\\n",
    "        (num_mini_batches_per_epoch * mini_batch_size)\n",
    "    num_ml_data_instances_in_ml_dataset = \\\n",
    "        min(num_ml_data_instances_in_ml_dataset,\n",
    "            epes_of_adjust_distortion_fields.size)\n",
    "\n",
    "\n",
    "\n",
    "    start = 0\n",
    "    stop = (epes_of_adjust_distortion_fields.size\n",
    "            - (epes_of_adjust_distortion_fields.size\n",
    "               % num_ml_data_instances_in_ml_dataset))\n",
    "    single_dim_slice = slice(start, stop)\n",
    "\n",
    "    epes_to_analyze = \\\n",
    "        epes_of_adjust_distortion_fields[single_dim_slice]\n",
    "    epes_to_analyze_grouped_by_epoch = \\\n",
    "        epes_to_analyze.reshape(-1, num_ml_data_instances_in_ml_dataset)\n",
    "    \n",
    "    quantiles = np.quantile(epes_to_analyze_grouped_by_epoch, \n",
    "                            q=(0.25, 0.5, 0.75), \n",
    "                            axis=1)\n",
    "    y = quantiles[1]\n",
    "    y_err = quantiles[[0, 2], :]\n",
    "    y_err[0] = y-y_err[0]\n",
    "    y_err[1] = y_err[1]-y\n",
    "    x = np.arange(y.size)\n",
    "\n",
    "\n",
    "\n",
    "    kwargs = {\"x\": x,\n",
    "              \"y\": y,\n",
    "              \"yerr\": y_err,\n",
    "              \"label\": phase,\n",
    "              \"marker\": \"o\",\n",
    "              \"markersize\": 3,\n",
    "              \"color\": phase_to_color_map[phase],\n",
    "              \"mfc\": phase_to_color_map[phase],\n",
    "              \"ecolor\": (phase_to_color_map[phase], 0.3)}\n",
    "    ax.errorbar(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "legend_label_font_size = axis_label_font_size\n",
    "ax.legend(loc=\"upper right\", \n",
    "          fontsize=legend_label_font_size)\n",
    "\n",
    "ax.set_xlabel(\"epoch\", \n",
    "              fontsize=axis_label_font_size)\n",
    "ax.set_ylabel(\"EPE of adjusted distortion\\nfield (image width)\", \n",
    "              fontsize=axis_label_font_size)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "\n",
    "for spatial_dim in (\"x\", \"y\"):        \n",
    "    kwargs = {\"axis\": spatial_dim,\n",
    "              \"which\": \"major\",\n",
    "              \"direction\": \"in\",\n",
    "              \"left\": True,\n",
    "              \"right\": True, \n",
    "              \"width\": major_tick_width, \n",
    "              \"length\": major_tick_length, \n",
    "              \"labelsize\": tick_label_size}\n",
    "    ax.tick_params(**kwargs)\n",
    "\n",
    "    kwargs[\"which\"] = \"minor\"\n",
    "    kwargs[\"width\"] = minor_tick_width\n",
    "    kwargs[\"length\"] = minor_tick_length\n",
    "    ax.tick_params(**kwargs)\n",
    "\n",
    "    for side in ['top','bottom','left','right']:\n",
    "        ax.spines[side].set_linewidth(linewidth)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af3aa05-e83d-4bff-9853-76a3a37894dc",
   "metadata": {},
   "source": [
    "In the plot directly above, for each phase, for each epoch, the vertical line\n",
    "indicates the interquartile range, and the circle/dot indicates the median. The\n",
    "metric plotted directly above is described in detail in the summary\n",
    "documentation of the class\n",
    "[emicroml.modelling.cbed.distortion.estimation.MLModelTrainer](https://mrfitzpa.github.io/emicroml/_autosummary/emicroml.modelling.cbed.distortion.estimation.MLModelTrainer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b558c2af-7ac7-4e45-840b-4f23ec540322",
   "metadata": {},
   "source": [
    "Similarly to performance metrics, we can also extract the time-series of the\n",
    "total mini-batch loss, which we plot below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca802727-5017-4398-a031-f80561174752",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "\n",
    "for phase in phase_to_color_map:\n",
    "    hdf5_dataset_path = \"num_{}_mini_batches_per_epoch\".format(phase)\n",
    "    \n",
    "    kwargs = {\"filename\": path_to_ml_model_training_summary_output_data,\n",
    "              \"path_in_file\": hdf5_dataset_path}\n",
    "    hdf5_dataset_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "    kwargs = {\"dataset_id\": hdf5_dataset_id, \"read_only\": True}\n",
    "    num_mini_batches_per_epoch = h5pywrappers.dataset.load(**kwargs)[()]\n",
    "\n",
    "\n",
    "\n",
    "    hdf5_dataset_path = (\"/mini_batch_losses/{}/total\".format(phase))\n",
    "    \n",
    "    kwargs = {\"filename\": path_to_ml_model_training_summary_output_data,\n",
    "              \"path_in_file\": hdf5_dataset_path}\n",
    "    hdf5_dataset_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "    kwargs = {\"dataset_id\": hdf5_dataset_id, \"read_only\": True}\n",
    "    total_mini_batch_losses = h5pywrappers.dataset.load(**kwargs)[()]\n",
    "\n",
    "\n",
    "    \n",
    "    start = 0\n",
    "    stop = (total_mini_batch_losses.size\n",
    "            - (total_mini_batch_losses.size % num_mini_batches_per_epoch))\n",
    "    single_dim_slice = slice(start, stop)\n",
    "\n",
    "    losses_to_analyze = \\\n",
    "        total_mini_batch_losses[single_dim_slice]\n",
    "    losses_to_analyze_grouped_by_epoch = \\\n",
    "        losses_to_analyze.reshape(-1, num_mini_batches_per_epoch)\n",
    "    \n",
    "    quantiles = np.quantile(losses_to_analyze_grouped_by_epoch, \n",
    "                            q=(0.25, 0.5, 0.75), \n",
    "                            axis=1)\n",
    "    y = quantiles[1]\n",
    "    y_err = quantiles[[0, 2], :]\n",
    "    y_err[0] = y-y_err[0]\n",
    "    y_err[1] = y_err[1]-y\n",
    "    x = np.arange(y.size)\n",
    "\n",
    "\n",
    "\n",
    "    kwargs = {\"x\": x,\n",
    "              \"y\": y,\n",
    "              \"yerr\": y_err,\n",
    "              \"label\": phase,\n",
    "              \"marker\": \"o\",\n",
    "              \"markersize\": 3,\n",
    "              \"color\": phase_to_color_map[phase],\n",
    "              \"mfc\": phase_to_color_map[phase],\n",
    "              \"ecolor\": (phase_to_color_map[phase], 0.3)}\n",
    "    ax.errorbar(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "legend_label_font_size = axis_label_font_size\n",
    "ax.legend(loc=\"upper right\", \n",
    "          fontsize=legend_label_font_size)\n",
    "\n",
    "ax.set_xlabel(\"epoch\", \n",
    "              fontsize=axis_label_font_size)\n",
    "ax.set_ylabel(\"total mini-batch loss (image width)\", \n",
    "              fontsize=axis_label_font_size)\n",
    "\n",
    "ax.set_yscale('log')\n",
    "\n",
    "for spatial_dim in (\"x\", \"y\"):        \n",
    "    kwargs = {\"axis\": spatial_dim,\n",
    "              \"which\": \"major\",\n",
    "              \"direction\": \"in\",\n",
    "              \"left\": True,\n",
    "              \"right\": True, \n",
    "              \"width\": major_tick_width, \n",
    "              \"length\": major_tick_length, \n",
    "              \"labelsize\": tick_label_size}\n",
    "    ax.tick_params(**kwargs)\n",
    "\n",
    "    kwargs[\"which\"] = \"minor\"\n",
    "    kwargs[\"width\"] = minor_tick_width\n",
    "    kwargs[\"length\"] = minor_tick_length\n",
    "    ax.tick_params(**kwargs)\n",
    "\n",
    "    for side in ['top','bottom','left','right']:\n",
    "        ax.spines[side].set_linewidth(linewidth)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93c91d-69ff-4fae-ac95-fb97e32c46a6",
   "metadata": {},
   "source": [
    "The total mini-batch loss plotted directly above is also described in detail in\n",
    "the summary documentation of the class\n",
    "[emicroml.modelling.cbed.distortion.estimation.MLModelTrainer](https://mrfitzpa.github.io/emicroml/_autosummary/emicroml.modelling.cbed.distortion.estimation.MLModelTrainer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5994d3-f92e-41bf-832e-bcb8f74afa81",
   "metadata": {},
   "source": [
    "Let's circle back to the EPE of the predicted \"adjusted\" standard distortion\n",
    "field, where this time we plot its cumulative distribution function (CDF) at the\n",
    "end of training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be4561c-5d81-4b1d-9795-fd0622e8ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "\n",
    "phases = (\"validation\", \"training\")\n",
    "\n",
    "\n",
    "\n",
    "for phase in phases:\n",
    "    hdf5_dataset_path = (\"/ml_data_instance_metrics\"\n",
    "                         \"/{}/epes_of_adjusted_distortion_fields\".format(phase))\n",
    "    \n",
    "    kwargs = {\"filename\": path_to_ml_model_training_summary_output_data,\n",
    "              \"path_in_file\": hdf5_dataset_path}\n",
    "    hdf5_dataset_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "    kwargs = {\"dataset_id\": hdf5_dataset_id, \"read_only\": True}\n",
    "    epes_of_adjust_distortion_fields = h5pywrappers.dataset.load(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "    hdf5_dataset_path = \"num_{}_mini_batches_per_epoch\".format(phase)\n",
    "    \n",
    "    kwargs = {\"filename\": path_to_ml_model_training_summary_output_data,\n",
    "              \"path_in_file\": hdf5_dataset_path}\n",
    "    hdf5_dataset_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "    kwargs = {\"dataset_id\": hdf5_dataset_id, \"read_only\": True}\n",
    "    num_mini_batches_per_epoch = h5pywrappers.dataset.load(**kwargs)[()].item()\n",
    "\n",
    "    \n",
    "\n",
    "    num_ml_data_instances_in_ml_dataset = \\\n",
    "        (num_mini_batches_per_epoch * mini_batch_size)\n",
    "    num_ml_data_instances_in_ml_dataset = \\\n",
    "        min(num_ml_data_instances_in_ml_dataset,\n",
    "            epes_of_adjust_distortion_fields.size)\n",
    "\n",
    "\n",
    "\n",
    "    start = -num_ml_data_instances_in_ml_dataset\n",
    "    stop = None\n",
    "    single_dim_slice = slice(start, stop)\n",
    "    multi_dim_slice = (single_dim_slice,)\n",
    "\n",
    "\n",
    "\n",
    "    hdf5_dataset_path = (\"/ml_data_instance_metrics\"\n",
    "                         \"/{}/epes_of_adjusted_distortion_fields\".format(phase))\n",
    "    \n",
    "    kwargs = {\"filename\": path_to_ml_model_training_summary_output_data,\n",
    "              \"path_in_file\": hdf5_dataset_path}\n",
    "    hdf5_dataset_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "    kwargs = {\"dataset_id\": hdf5_dataset_id,\n",
    "              \"multi_dim_slice\": multi_dim_slice}\n",
    "    hdf5_datasubset_id = h5pywrappers.datasubset.ID(**kwargs)\n",
    "\n",
    "    kwargs = {\"datasubset_id\": hdf5_datasubset_id}\n",
    "    x = h5pywrappers.datasubset.load(**kwargs)\n",
    "\n",
    "\n",
    "    \n",
    "    kwargs = {\"x\": x,\n",
    "              \"bins\": np.linspace(0, 0.04, 100),\n",
    "              \"histtype\": \"bar\", \n",
    "              \"ec\": \"black\", \n",
    "              \"cumulative\": True, \n",
    "              \"density\": True, \n",
    "              \"log\": False,\n",
    "              \"alpha\": 1,\n",
    "              \"color\": phase_to_color_map[phase],\n",
    "              \"label\": phase}\n",
    "    ax.hist(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "ax.legend(loc=\"lower right\", \n",
    "          fontsize=legend_label_font_size)\n",
    "\n",
    "ax.set_xlabel(\"EPE of adjusted distortion field (image width)\", \n",
    "              fontsize=axis_label_font_size)\n",
    "ax.set_ylabel(\"portion of images\", \n",
    "              fontsize=axis_label_font_size)\n",
    "\n",
    "ax.yaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(1))\n",
    "\n",
    "for spatial_dim in (\"x\", \"y\"):        \n",
    "    kwargs = {\"axis\": spatial_dim,\n",
    "              \"which\": \"major\",\n",
    "              \"direction\": \"in\",\n",
    "              \"left\": True,\n",
    "              \"right\": True, \n",
    "              \"width\": major_tick_width, \n",
    "              \"length\": major_tick_length, \n",
    "              \"labelsize\": tick_label_size}\n",
    "    ax.tick_params(**kwargs)\n",
    "\n",
    "    kwargs[\"which\"] = \"minor\"\n",
    "    kwargs[\"width\"] = minor_tick_width\n",
    "    kwargs[\"length\"] = minor_tick_length\n",
    "    ax.tick_params(**kwargs)\n",
    "\n",
    "    for side in ['top','bottom','left','right']:\n",
    "        ax.spines[side].set_linewidth(linewidth)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d8ae4b-096c-475a-a0a4-cb5620ba997b",
   "metadata": {},
   "source": [
    "## Loading and using a trained ML model ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35687b76-6a75-46ab-9a91-1a7fe81a5271",
   "metadata": {},
   "source": [
    "Let's load a trained ML model (this may take some time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7948c21-138b-4ee5-9bee-985a1469f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_ml_model_state_dicts = path_to_data_dir + \"/ml_models/ml_model_1\"\n",
    "pattern = \"ml_model_at_lr_step_[0-9]*\\.pth\"\n",
    "largest_lr_step_idx = max([name.split(\"_\")[-1].split(\".\")[0]\n",
    "                           for name in os.listdir(path_to_ml_model_state_dicts)\n",
    "                           if re.fullmatch(pattern, name)])\n",
    "\n",
    "ml_model_state_dict_filename = \\\n",
    "    (path_to_ml_model_state_dicts\n",
    "     + \"/ml_model_at_lr_step_{}.pth\".format(largest_lr_step_idx))\n",
    "\n",
    "\n",
    "\n",
    "module_alias = emicroml.modelling.cbed.distortion.estimation\n",
    "kwargs = {\"ml_model_state_dict_filename\": ml_model_state_dict_filename,\n",
    "          \"device_name\": None}  # Default to CUDA device if available.\n",
    "ml_model = module_alias.load_ml_model_from_file(**kwargs)\n",
    "\n",
    "_ = ml_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d918e839-24f4-48e1-8312-0939a2960ed3",
   "metadata": {},
   "source": [
    "Alternatively, we can load the trained ML model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9e25f7-a9c2-408e-a2f1-e72305e3b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"f\": ml_model_state_dict_filename,\n",
    "          \"map_location\": torch.device('cpu'),\n",
    "          \"weights_only\": True}\n",
    "ml_model_state_dict = torch.load(**kwargs)\n",
    "\n",
    "\n",
    "module_alias = emicroml.modelling.cbed.distortion.estimation\n",
    "kwargs = {\"ml_model_state_dict\": ml_model_state_dict,\n",
    "          \"device_name\": None}\n",
    "ml_model = module_alias.load_ml_model_from_state_dict(**kwargs)\n",
    "\n",
    "_ = ml_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dcc357-42c7-4f4a-80a2-b1ac388e6c61",
   "metadata": {},
   "source": [
    "With the ML model loaded, let's try estimating the distortion field in a\n",
    "simulated CBED pattern. First, we need to load a simulated CBED pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4095f2d-5c71-4b09-80d5-571c890a0a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_ml_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db0cf0-df57-4113-ad02-caf6fdb24bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_ml_dataset = (path_to_data_dir\n",
    "                      + \"/ml_datasets/ml_dataset_for_validation.h5\")\n",
    "\n",
    "module_alias = emicroml.modelling.cbed.distortion.estimation\n",
    "kwargs = {\"path_to_ml_dataset\": path_to_ml_dataset, \n",
    "          \"entire_ml_dataset_is_to_be_cached\": False, \n",
    "          \"ml_data_values_are_to_be_checked\": False}\n",
    "ml_dataset = module_alias.MLDataset(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "cbed_pattern_idx = 0\n",
    "\n",
    "kwargs = \\\n",
    "    {\"single_dim_slice\": slice(cbed_pattern_idx, cbed_pattern_idx+1), \n",
    "     \"sampling_grid_dims_in_pixels\": sampling_grid_dims_in_pixels}\n",
    "ml_data_instances_as_signals = \\\n",
    "    ml_dataset.get_ml_data_instances_as_signals(**kwargs)\n",
    "\n",
    "distorted_cbed_pattern_signal = ml_data_instances_as_signals[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d300465-1607-480d-9bc5-f28312f940cd",
   "metadata": {},
   "source": [
    "Let's plot the distorted CBED pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b5e75c-290d-449d-91fa-df421a544ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"axes_off\": True, \n",
    "          \"scalebar\": False, \n",
    "          \"colorbar\": False, \n",
    "          \"gamma\": 0.2,\n",
    "          \"cmap\": \"plasma\", \n",
    "          \"title\": \"\"}\n",
    "distorted_cbed_pattern_signal.plot(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adf1de8-1c09-41c6-b84d-68ea0fc40b1b",
   "metadata": {},
   "source": [
    "There are essentially two ways to estimate the distortion field. The first way\n",
    "is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9b6c31-f5b5-4e7b-95f6-3887cfc0dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "distorted_cbed_pattern_image = distorted_cbed_pattern_signal.data[0]\n",
    "distorted_cbed_pattern_images = distorted_cbed_pattern_image[None, :, :]\n",
    "ml_inputs = {\"cbed_pattern_images\": distorted_cbed_pattern_images}\n",
    "\n",
    "kwargs = {\"ml_inputs\": ml_inputs,\n",
    "          \"unnormalize_normalizable_elems_of_ml_predictions\": True}\n",
    "ml_predictions = ml_model.make_predictions(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "module_alias = emicroml.modelling.cbed.distortion.estimation\n",
    "kwargs = {\"ml_data_dict\": ml_predictions,\n",
    "          \"sampling_grid_dims_in_pixels\": sampling_grid_dims_in_pixels}\n",
    "distortion_models = module_alias.ml_data_dict_to_distortion_models(**kwargs)\n",
    "\n",
    "distortion_model = distortion_models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478d5b3b-3666-4d2d-94c2-09e5beadcec7",
   "metadata": {},
   "source": [
    "And the second way, which is more direct, is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1033076-3993-44e8-bd30-84b1cccb2a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"cbed_pattern_images\": distorted_cbed_pattern_images,\n",
    "          \"sampling_grid_dims_in_pixels\": sampling_grid_dims_in_pixels}\n",
    "distortion_models = ml_model.predict_distortion_models(**kwargs)\n",
    "\n",
    "distortion_model = distortion_models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d11ab1-cd69-44f5-9850-40d5d73cd5dd",
   "metadata": {},
   "source": [
    "Note that each input distorted CBED pattern must have image dimensions, in units\n",
    "of pixels, equal to\n",
    "``2*(ml_model.core_attrs[\"num_pixels_across_each_cbed_pattern\"],)``. This is\n",
    "because a given ML model is trained for images of fixed dimensions, in units of\n",
    "pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c90aaec-88d0-442d-9e6e-5f89d4935702",
   "metadata": {},
   "source": [
    "``distortion_model`` is an instance of the class\n",
    "[distoptica.DistortionModel](https://mrfitzpa.github.io/distoptica/_autosummary/distoptica.DistortionModel.html). This\n",
    "object stores the predicted distortion field, and can be used to perform\n",
    "distortion correction. Let's visualize the predicted distortion field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317d9f44-b86e-4761-8bb5-1d61ba8e3268",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_step = 16\n",
    "\n",
    "\n",
    "\n",
    "quiver_kwargs = {\"angles\": \"uv\",\n",
    "                 \"pivot\": \"middle\",\n",
    "                 \"scale_units\": \"width\"}\n",
    "\n",
    "\n",
    "\n",
    "attr_name = \"sampling_grid\"\n",
    "sampling_grid = getattr(distortion_model, attr_name)\n",
    "sampling_grid = (sampling_grid[0].numpy(), sampling_grid[1].numpy())\n",
    "\n",
    "X = sampling_grid[0][::slice_step, ::slice_step]\n",
    "Y = sampling_grid[1][::slice_step, ::slice_step]\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "attr_name = \"flow_field_of_coord_transform\"\n",
    "flow_field = getattr(distortion_model, attr_name)\n",
    "flow_field = (flow_field[0].numpy(), flow_field[1].numpy())\n",
    "\n",
    "U = flow_field[0][::slice_step, ::slice_step]\n",
    "V = flow_field[1][::slice_step, ::slice_step]\n",
    "\n",
    "kwargs = quiver_kwargs\n",
    "ax.quiver(X, Y, U, V, **kwargs)\n",
    "\n",
    "title_font_size = axis_label_font_size\n",
    "\n",
    "ax.set_title(\"Flow Field Of Coordinate Transformation\", \n",
    "             fontsize=title_font_size)\n",
    "ax.set_xlabel(\"fractional horizontal coordinate\", \n",
    "              fontsize=axis_label_font_size)\n",
    "ax.set_ylabel(\"fractional vertical coordinate\", \n",
    "              fontsize=axis_label_font_size)\n",
    "\n",
    "for spatial_dim in (\"x\", \"y\"):\n",
    "    kwargs = {\"axis\": spatial_dim,\n",
    "              \"which\": \"major\",\n",
    "              \"direction\": \"out\",\n",
    "              \"left\": True,\n",
    "              \"right\": True, \n",
    "              \"width\": major_tick_width, \n",
    "              \"length\": major_tick_length, \n",
    "              \"labelsize\": tick_label_size}\n",
    "    ax.tick_params(**kwargs)\n",
    "\n",
    "    kwargs[\"which\"] = \"minor\"\n",
    "    kwargs[\"width\"] = minor_tick_width\n",
    "    kwargs[\"length\"] = minor_tick_length\n",
    "    ax.tick_params(**kwargs)\n",
    "\n",
    "    ax.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "    ax.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\n",
    "\n",
    "    for side in ['top','bottom','left','right']:\n",
    "        ax.spines[side].set_linewidth(major_tick_width)\n",
    "\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e346b2c1-282e-4fac-8267-a2248820447c",
   "metadata": {},
   "source": [
    "Let's now use ``distortion_model`` to correct the distortion in the original\n",
    "distorted CBED pattern, according to the predicted distortion field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfcc6dc-ff2a-4537-81c4-a89d7fec5ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = \\\n",
    "    {\"distorted_images\": distorted_cbed_pattern_image[None, None, :, :]}\n",
    "undistorted_then_resampled_images = \\\n",
    "    distortion_model.undistort_then_resample_images(**kwargs)\n",
    "\n",
    "undistorted_cbed_pattern_image = undistorted_then_resampled_images[0, 0]\n",
    "undistorted_cbed_pattern_images = undistorted_cbed_pattern_image[None, :, :]\n",
    "\n",
    "\n",
    "\n",
    "ml_data_dict = {\"cbed_pattern_images\": undistorted_cbed_pattern_images}\n",
    "\n",
    "module_alias = \\\n",
    "    emicroml.modelling.cbed.distortion.estimation\n",
    "kwargs = \\\n",
    "    {\"ml_data_dict\": ml_data_dict,\n",
    "     \"sampling_grid_dims_in_pixels\": sampling_grid_dims_in_pixels}\n",
    "undistorted_cbed_pattern_signals = \\\n",
    "    module_alias.ml_data_dict_to_signals(**kwargs)\n",
    "\n",
    "undistorted_cbed_pattern_signal = undistorted_cbed_pattern_signals[0]\n",
    "\n",
    "kwargs = {\"axes_off\": True, \n",
    "          \"scalebar\": False, \n",
    "          \"colorbar\": False, \n",
    "          \"gamma\": 0.2,\n",
    "          \"cmap\": \"plasma\", \n",
    "          \"title\": \"\"}\n",
    "undistorted_cbed_pattern_signal.inav[0].plot(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cecb66-eeae-410c-bde0-2643ee2b2ec9",
   "metadata": {},
   "source": [
    "Note how the CBED disks are now much more circular, which indicates\n",
    "qualitatively that the ML model did a reasonable job predicting the distortion\n",
    "field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33a9ea9-f48d-41af-aa19-1e00c1938ed1",
   "metadata": {},
   "source": [
    "Another useful instance attribute in this context is\n",
    "[distoptica.DistortionModel.out_of_bounds_map_of_undistorted_then_resampled_images](https://mrfitzpa.github.io/distoptica/_autosummary/distoptica.DistortionModel.html#distoptica.DistortionModel.out_of_bounds_map_of_undistorted_then_resampled_images):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8699dd9b-0af2-41cc-afd9-05f2f6ee545b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_of_bounds_map_of_undistorted_then_resampled_images = \\\n",
    "    distortion_model.out_of_bounds_map_of_undistorted_then_resampled_images\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.imshow(out_of_bounds_map_of_undistorted_then_resampled_images)\n",
    "\n",
    "ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f50900-0f9b-42ec-a7a5-fbe3a95babb6",
   "metadata": {},
   "source": [
    "## A note on ``fancytypes`` ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5981b820-96fe-4771-a702-cd522c2b03d4",
   "metadata": {},
   "source": [
    "The variables ``ml_dataset``, ``ml_model_trainer``, and ``distortion_model``,\n",
    "used throughout the notebook, reference instances of subclasses of the\n",
    "[fancytypes.PreSerializableAndUpdatable](https://mrfitzpa.github.io/fancytypes/_autosummary/fancytypes.PreSerializableAndUpdatable.html)\n",
    "class.  One of the nice features of this class is that an instance of a subclass\n",
    "thereof can be serialize straightforwardly and later reconstructed from the said\n",
    "serialized representation. For example, let's serialize ``ml_dataset``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d8383e-3e47-42f8-8bbc-c85a2e6765a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "serialized_rep_of_ml_dataset = ml_dataset.dumps()\n",
    "serialized_rep_of_ml_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b35edcd-38d0-4ae5-9458-281f5332dc05",
   "metadata": {},
   "source": [
    "Now let's reconstruct ``ml_dataset`` from its serialized representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b315c8-545b-42af-a743-e30774d653b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_alias = emicroml.modelling.cbed.distortion.estimation\n",
    "kwargs = {\"serialized_rep\": serialized_rep_of_ml_dataset}\n",
    "ml_dataset = module_alias.MLDataset.loads(**kwargs)\n",
    "ml_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d344f8-1e81-42fb-a8e7-d1c6fe70451b",
   "metadata": {},
   "source": [
    "The\n",
    "[fancytypes.PreSerializableAndUpdatable](https://mrfitzpa.github.io/fancytypes/_autosummary/fancytypes.PreSerializableAndUpdatable.html)\n",
    "has other nice features, which you can read about in more detail in the\n",
    "documentation thereof."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
