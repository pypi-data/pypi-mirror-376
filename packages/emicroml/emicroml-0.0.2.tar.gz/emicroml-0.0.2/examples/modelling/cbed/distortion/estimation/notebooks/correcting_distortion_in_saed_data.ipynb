{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb7f461",
   "metadata": {},
   "source": [
    "# Correcting distortion in SAED data #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d9cd7",
   "metadata": {},
   "source": [
    "## A NOTE BEFORE STARTING ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880421e9",
   "metadata": {},
   "source": [
    "Since the ``emicroml`` git repository tracks this notebook under its original\n",
    "basename ``correcting_distortion_in_saed_data.ipynb``, we recommend that you\n",
    "copy the original notebook and rename it to any other basename that is not one\n",
    "of the original basenames that appear in the ``<root>/examples`` directory\n",
    "before executing any of the notebook cells below, where ``<root>`` is the root\n",
    "of the ``emicroml`` repository. For example, you could rename it\n",
    "``correcting_distortion_in_saed_data.ipynb``. This way you can explore the\n",
    "notebook by executing and modifying cells without changing the original\n",
    "notebook, which is being tracked by git."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b0d90e",
   "metadata": {},
   "source": [
    "## Import necessary modules ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f9787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pattern matching.\n",
    "import re\n",
    "\n",
    "# For listing files and subdirectories in a given directory, and for renaming\n",
    "# directories.\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# For general array handling.\n",
    "import numpy as np\n",
    "\n",
    "# For creating and plotting figures.\n",
    "import hyperspy.api as hs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For minimizing objective functions.\n",
    "import scipy.optimize\n",
    "\n",
    "\n",
    "\n",
    "# For loading ML models for distortion estimation in CBED.\n",
    "import emicroml.modelling.cbed.distortion.estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d0485-cf65-478b-80c4-d5d294879c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib ipympl\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e667d633",
   "metadata": {},
   "source": [
    "## Introduction ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f2509",
   "metadata": {},
   "source": [
    "In this notebook, we show how one can use one of the machine learning (ML)\n",
    "models that are trained as a result of executing the \"action\" described in the\n",
    "page [Training machine learning\n",
    "models](https://mrfitzpa.github.io/emicroml/examples/modelling/cbed/distortion/estimation/train_ml_model_set.html)\n",
    "to correct distortion in selected area electron diffraction (SAED)\n",
    "data. Strictly speaking, each ML model is trained to estimate distortion in\n",
    "convergent beam electron diffraction (CBED) patterns. However, by exploiting the\n",
    "fact that distortions predominantly come from post-specimen lenses,\n",
    "e.g. projection lenses, we can estimate and correction distortion in SAED data\n",
    "as follows:\n",
    "\n",
    "1. Collect the target experimental SAED data;\n",
    "2. Modify only pre-specimen lenses to produce CBED data;\n",
    "3. Use a ML model to estimate distortion field in CBED data;\n",
    "4. Correct distortion in SAED data using distortion field from step 3.\n",
    "\n",
    "We demonstrate steps 3 and 4 using pre-collected experimental SAED and CBED\n",
    "patterns of a calibration sample of single-crystal Au oriented in the \\[100\\]\n",
    "direction. This experimental data was collected on a modified Hitachi SU9000\n",
    "scanning electron microscope operated at 20 keV.\n",
    "\n",
    "In order to execute the cells in this notebook as intended, a set of Python\n",
    "libraries need to be installed in the Python environment within which the cells\n",
    "of the notebook are to be executed. See [this\n",
    "page](https://mrfitzpa.github.io/emicroml/examples/prerequisites_for_execution_without_slurm.html)\n",
    "for instructions on how to do so. Additionally, a subset of the output that\n",
    "results from performing the aforementioned actions is required to execute the\n",
    "cells in this notebook as intended. One can obtain this subset of output by\n",
    "executing said actions, however this requires significant computational\n",
    "resources, including significant walltime. Alternatively, one can copy this\n",
    "subset of output from a Federated Research Data Repository dataset by following\n",
    "the instructions given on [this\n",
    "page](https://mrfitzpa.github.io/emicroml/examples/modelling/cbed/distortion/estimation/copying_subset_of_output_from_frdr_dataset.html).\n",
    "\n",
    "You can find the documentation for the ``emicroml`` library\n",
    "[here](https://mrfitzpa.github.io/emicroml/_autosummary/emicroml.html).\n",
    "It is recommended that you consult the documentation of this\n",
    "library as you explore the notebook. Moreover, users should\n",
    "execute the cells in the order that they appear, i.e. from top to\n",
    "bottom, as some cells reference variables that are set in other\n",
    "cells above them. **Users should make sure to navigate the\n",
    "documentation for the version of ``emicroml`` that they are\n",
    "currently using.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9445a5e-76d1-4957-8399-c8638ab57d0c",
   "metadata": {},
   "source": [
    "## Loading and visualizing the SAED and CBED patterns ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e3c338-e3cb-424c-84c7-b5fa6527dd24",
   "metadata": {},
   "source": [
    "Let's load and visualize the target SAED pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8991614a-3d4b-42f8-9acb-5e6f190959e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_data_dir = \"../data\"\n",
    "filename = (path_to_data_dir \n",
    "            + \"/for_demo_of_distortion_correction_in_saed_data\"\n",
    "            + \"/distorted_saed_pattern.npy\")\n",
    "\n",
    "kwargs = {\"file\": filename}\n",
    "distorted_saed_pattern_image = np.load(**kwargs)\n",
    "\n",
    "kwargs = {\"data\": distorted_saed_pattern_image}\n",
    "distorted_saed_pattern_signal = hs.signals.Signal2D(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "kwargs = {\"axes_off\": True, \n",
    "          \"scalebar\": False, \n",
    "          \"colorbar\": False, \n",
    "          \"gamma\": 0.2,\n",
    "          \"cmap\": \"plasma\", \n",
    "          \"title\": \"\"}\n",
    "distorted_saed_pattern_signal.plot(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682f66fa-5610-40f9-b17f-99bbadf281cb",
   "metadata": {},
   "source": [
    "This SAED pattern is subject to optical distortion which we want to correct. To\n",
    "do this, keeping the sample inside, we modify only the pre-specimen lenses to\n",
    "produce a CBED pattern which should be subject approximately to the same\n",
    "distortion.\n",
    "\n",
    "Let's load and visualize the target CBED pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b187307-faa4-4853-9999-e7e4bf1dab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = (path_to_data_dir \n",
    "            + \"/for_demo_of_distortion_correction_in_saed_data\"\n",
    "            + \"/distorted_cbed_pattern.npy\")\n",
    "\n",
    "kwargs = {\"file\": filename}\n",
    "distorted_cbed_pattern_image = np.load(**kwargs)\n",
    "\n",
    "kwargs = {\"data\": distorted_cbed_pattern_image}\n",
    "distorted_cbed_pattern_signal = hs.signals.Signal2D(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "kwargs = {\"axes_off\": True, \n",
    "          \"scalebar\": False, \n",
    "          \"colorbar\": False, \n",
    "          \"gamma\": 0.2,\n",
    "          \"cmap\": \"plasma\", \n",
    "          \"title\": \"\"}\n",
    "distorted_cbed_pattern_signal.plot(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510f1bbb-5d5b-48aa-8efa-45ded8adab30",
   "metadata": {},
   "source": [
    "Next, we apply a mask to block all but most of the zero-order Laue zone (ZOLZ)\n",
    "reflections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1fe5f1-efdf-492f-9516-b63ca82c7b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_x, N_y = distorted_cbed_pattern_signal.data.shape\n",
    "\n",
    "L = 70\n",
    "R = N_x-420\n",
    "B = N_y-512\n",
    "T = 110\n",
    "\n",
    "distorted_cbed_pattern_signal.data[:, :L] = 0\n",
    "distorted_cbed_pattern_signal.data[:, N_x-R:] = 0\n",
    "distorted_cbed_pattern_signal.data[:T, :] = 0\n",
    "distorted_cbed_pattern_signal.data[N_y-B:, :] = 0\n",
    "\n",
    "\n",
    "\n",
    "kwargs = {\"axes_off\": True, \n",
    "          \"scalebar\": False, \n",
    "          \"colorbar\": False, \n",
    "          \"gamma\": 0.2,\n",
    "          \"cmap\": \"plasma\", \n",
    "          \"title\": \"\"}\n",
    "distorted_cbed_pattern_signal.plot(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae091c6-4798-4591-bcf5-f4f4c392d0e6",
   "metadata": {},
   "source": [
    "We found that masking can improve the performance of our DL model. One possible\n",
    "explanation is that at low beam energies and small CBED disk sizes, the Ewald\n",
    "sphere curvature can be quite pronounced and the small-angle approximation may\n",
    "not hold across the entire angular field of view of a given CBED pattern, both\n",
    "of which may affect the validity of our assumption that the CBED pattern should\n",
    "depict only near-perfect circular CBED disks of the same common radius, in the\n",
    "absence of distortion. This should only be a concern at larger scattering\n",
    "angles, which is why we did not mask most of the ZOLZ reflections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602148b6-6317-4b5a-962b-37941cac76ad",
   "metadata": {},
   "source": [
    "## Estimating the distortion in the CBED pattern ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf6f43e-07af-40f7-a220-8a3d8254849b",
   "metadata": {},
   "source": [
    "Now let's load a ML model so that we can estimate the distortion in the CBED\n",
    "pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45db07e-a183-427c-b4dc-a71260cc49fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_ml_model_state_dicts = path_to_data_dir + \"/ml_models/ml_model_1\"\n",
    "pattern = \"ml_model_at_lr_step_[0-9]*\\.pth\"\n",
    "largest_lr_step_idx = max([name.split(\"_\")[-1].split(\".\")[0]\n",
    "                           for name in os.listdir(path_to_ml_model_state_dicts)\n",
    "                           if re.fullmatch(pattern, name)])\n",
    "\n",
    "ml_model_state_dict_filename = \\\n",
    "    (path_to_ml_model_state_dicts\n",
    "     + \"/ml_model_at_lr_step_{}.pth\".format(largest_lr_step_idx))\n",
    "\n",
    "\n",
    "\n",
    "module_alias = emicroml.modelling.cbed.distortion.estimation\n",
    "kwargs = {\"ml_model_state_dict_filename\": ml_model_state_dict_filename,\n",
    "          \"device_name\": None}  # Default to CUDA device if available.\n",
    "ml_model = module_alias.load_ml_model_from_file(**kwargs)\n",
    "\n",
    "_ = ml_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35f944d-1f00-437a-8896-645499067a14",
   "metadata": {},
   "source": [
    "With the ML model loaded, let's estimate the distortion in the CBED pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57f4838-2bf4-4df7-bb38-b515894ccffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_grid_dims_in_pixels = distorted_cbed_pattern_image.shape\n",
    "distorted_cbed_pattern_images = distorted_cbed_pattern_image[None, :, :]\n",
    "\n",
    "kwargs = {\"cbed_pattern_images\": distorted_cbed_pattern_images,\n",
    "          \"sampling_grid_dims_in_pixels\": sampling_grid_dims_in_pixels}\n",
    "distortion_models = ml_model.predict_distortion_models(**kwargs)\n",
    "\n",
    "distortion_model = distortion_models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdd0855-b333-4d80-86b0-5873e3adc8c4",
   "metadata": {},
   "source": [
    "Note that any input distorted CBED pattern must have image dimensions, in units\n",
    "of pixels, equal to\n",
    "``2*(ml_model.core_attrs[\"num_pixels_across_each_cbed_pattern\"],)``. This is\n",
    "because a given ML model is trained for images of fixed dimensions, in units of\n",
    "pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d0ecd5-cb5c-4576-8dad-0117b52f8c60",
   "metadata": {},
   "source": [
    "Let's visualize the predicted distortion field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d643920-26da-4d75-899f-a11c8cf3afe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_step = 16\n",
    "\n",
    "\n",
    "\n",
    "quiver_kwargs = {\"angles\": \"uv\",\n",
    "                 \"pivot\": \"middle\",\n",
    "                 \"scale_units\": \"width\"}\n",
    "\n",
    "\n",
    "\n",
    "attr_name = \"sampling_grid\"\n",
    "sampling_grid = getattr(distortion_model, attr_name)\n",
    "sampling_grid = (sampling_grid[0].numpy(), sampling_grid[1].numpy())\n",
    "\n",
    "X = sampling_grid[0][::slice_step, ::slice_step]\n",
    "Y = sampling_grid[1][::slice_step, ::slice_step]\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "attr_name = \"flow_field_of_coord_transform\"\n",
    "flow_field = getattr(distortion_model, attr_name)\n",
    "flow_field = (flow_field[0].numpy(), flow_field[1].numpy())\n",
    "\n",
    "U = flow_field[0][::slice_step, ::slice_step]\n",
    "V = flow_field[1][::slice_step, ::slice_step]\n",
    "\n",
    "kwargs = quiver_kwargs\n",
    "ax.quiver(X, Y, U, V, **kwargs)\n",
    "\n",
    "title_font_size = 15\n",
    "\n",
    "ax.set_title(\"Flow Field Of Coordinate Transformation\", \n",
    "             fontsize=title_font_size)\n",
    "\n",
    "axis_label_font_size = title_font_size\n",
    "ax.set_xlabel(\"fractional horizontal coordinate\", \n",
    "              fontsize=axis_label_font_size)\n",
    "ax.set_ylabel(\"fractional vertical coordinate\", \n",
    "              fontsize=axis_label_font_size)\n",
    "\n",
    "for spatial_dim in (\"x\", \"y\"):\n",
    "    major_tick_width = 1.5\n",
    "    major_tick_length = 8\n",
    "    minor_tick_width = major_tick_width\n",
    "    minor_tick_length = major_tick_length//2\n",
    "    tick_label_size = 15\n",
    "    \n",
    "    kwargs = {\"axis\": spatial_dim,\n",
    "              \"which\": \"major\",\n",
    "              \"direction\": \"out\",\n",
    "              \"left\": True,\n",
    "              \"right\": True, \n",
    "              \"width\": major_tick_width, \n",
    "              \"length\": major_tick_length, \n",
    "              \"labelsize\": tick_label_size}\n",
    "    ax.tick_params(**kwargs)\n",
    "\n",
    "    kwargs[\"which\"] = \"minor\"\n",
    "    kwargs[\"width\"] = minor_tick_width\n",
    "    kwargs[\"length\"] = minor_tick_length\n",
    "    ax.tick_params(**kwargs)\n",
    "\n",
    "    ax.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "    ax.tick_params(axis='y', which='both', left=False, right=False, labelleft=False)\n",
    "\n",
    "    for side in ['top','bottom','left','right']:\n",
    "        ax.spines[side].set_linewidth(major_tick_width)\n",
    "\n",
    "plt.gca().set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65364360-a6cd-4a0f-ad79-23e68905f0ba",
   "metadata": {},
   "source": [
    "## Correcting the distortion in the SAED pattern ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca339ea-5436-40bf-80c1-adf166c14f55",
   "metadata": {},
   "source": [
    "Let's use the predicted distortion model to correct the distortion in the SAED\n",
    "pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00de7eb8-4f64-4f7a-8e26-bbfeaccbf3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = \\\n",
    "    {\"distorted_images\": distorted_saed_pattern_image[None, None, :, :]}\n",
    "undistorted_then_resampled_images = \\\n",
    "    distortion_model.undistort_then_resample_images(**kwargs)\n",
    "\n",
    "undistorted_saed_pattern_image = \\\n",
    "    undistorted_then_resampled_images[0, 0].numpy(force=True)\n",
    "\n",
    "\n",
    "\n",
    "kwargs = {\"data\": undistorted_saed_pattern_image}\n",
    "undistorted_saed_pattern_signal = hs.signals.Signal2D(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "kwargs = {\"axes_off\": True, \n",
    "          \"scalebar\": False, \n",
    "          \"colorbar\": False, \n",
    "          \"gamma\": 0.2,\n",
    "          \"cmap\": \"plasma\", \n",
    "          \"title\": \"\"}\n",
    "undistorted_saed_pattern_signal.plot(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c728eeaa-64c7-4827-bd2a-e163eb03bf31",
   "metadata": {},
   "source": [
    "## Assessing the accuracy of the distortion correction ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d45ab6-c8fc-4e23-8d4a-217aff0361f9",
   "metadata": {},
   "source": [
    "We know that the sample is single-crystal Au oriented in the \\[100\\] direction,\n",
    "used for calibration. As such, in the absence of distortions, the zero-order\n",
    "Laue zone (ZOLZ) reflections should lie approximately on a square lattice. We\n",
    "say approximately because strictly speaking, the ZOLZ reflections furthest from\n",
    "the direct beam should deviate from a square lattice by approximately 2 pixels,\n",
    "due to the curvature of the Ewald sphere for a beam energy of 20\n",
    "keV. Nevertheless, fitting square lattices to the most visible ZOLZ reflections\n",
    "in both the distorted SAED pattern and the undistorted SAED pattern, and\n",
    "comparing the errors of the fits, should be a reasonable way to assess the\n",
    "accuracy of the distortion correction.\n",
    "\n",
    "The first step is to locate the ZOLZ reflections that are sufficiently visible\n",
    "in the SAED patterns. We can do this by applying masks and peak-finding\n",
    "algorithms. Let's define a function that locates the visible ZOLZ reflections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca4197-437a-4b2f-a441-3a14d74b9bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_visible_zolz_reflections(saed_pattern_signal):\n",
    "    # Specify a mask that only reveals the visible ZOLZ reflections.\n",
    "    N_x, N_y = saed_pattern_signal.data.shape[::-1]\n",
    "\n",
    "    L = 30\n",
    "    R = N_x-345\n",
    "    B = N_y-455\n",
    "    T = 110\n",
    "\n",
    "    rectangular_mask_image = np.zeros((N_y, N_x), dtype=bool)\n",
    "    rectangular_mask_image[T:N_y-B, L:N_x-R] = True\n",
    "\n",
    "    rectangular_mask_signal = hs.signals.Signal2D(data=rectangular_mask_image)\n",
    "\n",
    "\n",
    "\n",
    "    # Apply the mask to the SAED pattern and then apply the Difference of \n",
    "    # Gaussian peak-finding method to find candidate peaks that may be located \n",
    "    # at visible ZOLZ reflection locations.\n",
    "    masked_saed_pattern_signal = saed_pattern_signal*rectangular_mask_signal\n",
    "\n",
    "    kwargs = {\"method\": \"difference_of_gaussian\", \n",
    "              \"overlap\": 0, \n",
    "              \"threshold\": 0.0025, \n",
    "              \"min_sigma\": 1,\n",
    "              \"max_sigma\": 2,\n",
    "              \"interactive\": False, \n",
    "              \"show_progressbar\": False}\n",
    "    find_peaks_result = masked_saed_pattern_signal.find_peaks(**kwargs)\n",
    "    candidate_peak_locations = find_peaks_result.data[0][:, ::-1]\n",
    "\n",
    "\n",
    "\n",
    "    # Some ZOLZ reflections have satelite peaks that get picked up by the above\n",
    "    # peak-finding algorithm, hence we need to remove them. \n",
    "    selection = tuple()\n",
    "    num_candidate_peaks = len(candidate_peak_locations)\n",
    "    ref_distance = None\n",
    "    num_iterations = 2\n",
    "\n",
    "    for iteration_idx in range(num_iterations):\n",
    "        nearest_neighbour_distances = tuple()\n",
    "        \n",
    "        for candidate_peak_idx in range(num_candidate_peaks):\n",
    "            candidate_peak_location = \\\n",
    "                candidate_peak_locations[candidate_peak_idx]\n",
    "\n",
    "            displacements = candidate_peak_locations-candidate_peak_location\n",
    "            distances = np.sort(np.linalg.norm(displacements, axis=(1,)))[1:]\n",
    "\n",
    "            nearest_neighbour_distance = distances[0]\n",
    "            nearest_neighbour_distances += (nearest_neighbour_distance,)\n",
    "\n",
    "            if ref_distance is not None:\n",
    "                if 2*nearest_neighbour_distance >= ref_distance:\n",
    "                    selection += \\\n",
    "                        (candidate_peak_idx,)\n",
    "                else:\n",
    "                    lattice_spacing_estimate = \\\n",
    "                        distances[2*distances > ref_distance][0]\n",
    "\n",
    "                    abs_diff = np.abs(lattice_spacing_estimate-ref_distance)\n",
    "                    rel_diff = abs_diff / ref_distance\n",
    "                    tol = 0.06\n",
    "                \n",
    "                    if rel_diff < tol:\n",
    "                        selection += (candidate_peak_idx,)\n",
    "                \n",
    "\n",
    "        if ref_distance is None:\n",
    "            nn_distances = nearest_neighbour_distances\n",
    "            nn_distances = np.array(nearest_neighbour_distances)\n",
    "\n",
    "            outlier_threshold = 2\n",
    "            outlier_registry = (np.abs(nn_distances - nn_distances.mean())\n",
    "                                > outlier_threshold*nn_distances.std())\n",
    "\n",
    "            ref_distance = nn_distances[~outlier_registry].mean()\n",
    "\n",
    "    visible_zolz_reflections = \\\n",
    "        tuple(candidate_peak_locations[(selection,)].tolist())\n",
    "\n",
    "\n",
    "\n",
    "    # Plot the SAED pattern with markers at the locations of the visible ZOLZ \n",
    "    # reflections.\n",
    "    kwargs = {\"axes_off\": True, \n",
    "              \"scalebar\": False, \n",
    "              \"colorbar\": False, \n",
    "              \"gamma\": 0.2,\n",
    "              \"cmap\": \"plasma\", \n",
    "              \"title\": \"\"}\n",
    "    saed_pattern_signal.plot(**kwargs)\n",
    "\n",
    "    for zolz_reflection in visible_zolz_reflections:\n",
    "        kwargs = {\"color\": \"black\", \n",
    "                  \"sizes\": 3, \n",
    "                  \"offsets\": zolz_reflection}\n",
    "        marker = hs.plot.markers.Points(**kwargs)\n",
    "        saed_pattern_signal.add_marker(marker, permanent=False)\n",
    "\n",
    "    return visible_zolz_reflections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe3e56e-2a88-405c-894b-2e216ff867e3",
   "metadata": {},
   "source": [
    "Now let's locate the sufficiently visble ZOLZ reflections of the distorted SAED\n",
    "pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc06ba-1411-4d67-8056-8883056de9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = \\\n",
    "    {\"saed_pattern_signal\": distorted_saed_pattern_signal}\n",
    "zolz_reflection_selection_of_distorted_saed_pattern = \\\n",
    "    find_visible_zolz_reflections(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22153e1d-bd16-4525-9d37-bc32e87f200e",
   "metadata": {},
   "source": [
    "Now let's do the same for the undistorted SAED pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6aa58f-4185-4ad4-b738-a5af7118c086",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = \\\n",
    "    {\"saed_pattern_signal\": undistorted_saed_pattern_signal}\n",
    "zolz_reflection_selection_of_undistorted_saed_pattern = \\\n",
    "    find_visible_zolz_reflections(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142b2fe3-06f8-4ee4-99d2-bf69874af371",
   "metadata": {},
   "source": [
    "Now we need to perform the fits. The objective function that we will minimize is\n",
    "the square root of the mean of the Euclidean distances squared between the ZOLZ\n",
    "reflections and their corresponding points on the square lattice fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f681443a-3f6d-4fac-8a0f-d6e6e2364461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(x, visible_zolz_reflections, N_x):\n",
    "    u_O_x, u_O_y, b, theta = x\n",
    "\n",
    "    # u_0_x: fractional horizontal coordinate of origin of square lattice fit.\n",
    "    # u_0_y: fractional vertical coordinate of origin of square lattice fit.\n",
    "    # b: length of primitive lattice vector.\n",
    "    # theta: rotation applied to lattice.\n",
    "    # N_x: Number of pixels across SAED pattern.\n",
    "\n",
    "    N = N_x\n",
    "\n",
    "    result = 0.0\n",
    "\n",
    "    for (k_x, k_y) in visible_zolz_reflections:\n",
    "        to_round = ((k_x-u_O_x)*np.cos(theta) + (k_y-u_O_y)*np.sin(theta)) / b\n",
    "        rounded = np.round(to_round)\n",
    "        result += (to_round-rounded)**2\n",
    "\n",
    "        to_round = (-(k_x-u_O_x)*np.sin(theta) + (k_y-u_O_y)*np.cos(theta)) / b\n",
    "        rounded = np.round(to_round)\n",
    "        result += (to_round-rounded)**2\n",
    "\n",
    "    result *= ((b/N)**2) / len(visible_zolz_reflections)\n",
    "    result = np.sqrt(result)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8df45f-98d5-4843-82b6-016791841575",
   "metadata": {},
   "source": [
    "We define the fitting error to be the final value of the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e3e8ae-bea3-4fb1-acfd-de4654dc9a25",
   "metadata": {},
   "source": [
    "Next, let's define a function that performs the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94907b26-70bb-45b0-b63f-44b7972e2586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_visible_zolz_reflections_to_square_lattice(visible_zolz_reflections,\n",
    "                                                   saed_pattern_signal):\n",
    "    visible_zolz_reflections = np.array(visible_zolz_reflections)\n",
    "    saed_pattern_image = saed_pattern_signal.data\n",
    "    N_x, N_y = saed_pattern_signal.data.shape[::-1]\n",
    "    \n",
    "    ref_point = np.array((236, 242))\n",
    "    u_O_guess = None\n",
    "\n",
    "    for zolz_reflection in visible_zolz_reflections:\n",
    "        if u_O_guess is None:\n",
    "            u_O_guess = zolz_reflection\n",
    "        else:\n",
    "            distance_1 = np.linalg.norm(ref_point-u_O_guess)\n",
    "            distance_2 = np.linalg.norm(ref_point-zolz_reflection)\n",
    "            if distance_2 < distance_1:\n",
    "                u_O_guess = zolz_reflection\n",
    "\n",
    "    u_O_x_guess, u_O_y_guess = u_O_guess\n",
    "\n",
    "                \n",
    "\n",
    "    displacements = visible_zolz_reflections-u_O_guess\n",
    "    b_guess = np.sort(np.linalg.norm(displacements, axis=(1,)))[1]\n",
    "\n",
    "    \n",
    "\n",
    "    for zolz_reflection in visible_zolz_reflections:\n",
    "        displacement = zolz_reflection-u_O_guess\n",
    "        distance = np.linalg.norm(displacement)\n",
    "        if 1.1*b_guess > distance > 0:\n",
    "            if displacement[1] > displacement[0] > 0:\n",
    "                theta_guess = np.arctan2(displacement[1], \n",
    "                                         displacement[0])\n",
    "\n",
    "\n",
    "\n",
    "    initial_guesses = (u_O_x_guess,\n",
    "                       u_O_y_guess,\n",
    "                       b_guess,\n",
    "                       theta_guess)\n",
    "\n",
    "\n",
    "    \n",
    "    u_O_x_bounds = (0, N_x)\n",
    "    u_O_y_bounds = (0, N_y)\n",
    "    b_bounds = (0.5*b_guess, 1.5*b_guess)\n",
    "    theta_bounds = (0.75*theta_guess, 1.25*theta_guess)\n",
    "\n",
    "    bounds = (u_O_x_bounds,\n",
    "              u_O_y_bounds,\n",
    "              b_bounds,\n",
    "              theta_bounds)\n",
    "\n",
    "    \n",
    "\n",
    "    kwargs = {\"fun\": objective,\n",
    "              \"args\": (visible_zolz_reflections, N_x),\n",
    "              \"x0\": initial_guesses,\n",
    "              \"bounds\": bounds}\n",
    "    minimization_result = scipy.optimize.minimize(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "    u_O_x, u_O_y, b, theta = minimization_result.x\n",
    "\n",
    "    u_O = np.array((u_O_x, u_O_y))\n",
    "    b_1 = b*np.array((np.cos(theta), np.sin(theta)))\n",
    "    b_2 = b*np.array((-np.sin(theta), np.cos(theta)))\n",
    "\n",
    "\n",
    "\n",
    "    kwargs = {\"axes_off\": True, \n",
    "              \"scalebar\": False, \n",
    "              \"colorbar\": False, \n",
    "              \"gamma\": 0.2,\n",
    "              \"cmap\": \"plasma\", \n",
    "              \"title\": \"\"}\n",
    "    saed_pattern_signal.plot(**kwargs)\n",
    "\n",
    "\n",
    "\n",
    "    M = 10\n",
    "\n",
    "    for m_1 in range(-M, M+1):\n",
    "        for m_2 in range(-M, M+1):\n",
    "            lattice_position = (u_O + m_1*b_1 + m_2*b_2)\n",
    "        \n",
    "            displacements = visible_zolz_reflections-lattice_position\n",
    "            distance = np.sort(np.linalg.norm(displacements, axis=(1,)))[0]            \n",
    "\n",
    "            if 2*distance < b:\n",
    "                kwargs = {\"color\": \"black\", \n",
    "                          \"sizes\": 3, \n",
    "                          \"offsets\": lattice_position.tolist()}\n",
    "                marker = hs.plot.markers.Points(**kwargs)\n",
    "                saed_pattern_signal.add_marker(marker, permanent=False)\n",
    "\n",
    "\n",
    "    \n",
    "    fitting_error = minimization_result.fun\n",
    "    unformatted_msg = (\"The error of the fit is: {}, \"\n",
    "                       \"in units of the image width.\")\n",
    "    msg = unformatted_msg.format(fitting_error)\n",
    "    print(msg)\n",
    "\n",
    "    return fitting_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5676b1-e03b-4508-88e5-dd68214b4e31",
   "metadata": {},
   "source": [
    "Let's perform the fit for the distorted SAED pattern, using the above function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b819f-e09a-4b82-ab2f-c9ba1a08b7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"visible_zolz_reflections\": \\\n",
    "          zolz_reflection_selection_of_distorted_saed_pattern, \n",
    "          \"saed_pattern_signal\": \\\n",
    "          distorted_saed_pattern_signal}\n",
    "_ = fit_visible_zolz_reflections_to_square_lattice(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9a4564-aca0-4fe7-ab51-987584ce0af0",
   "metadata": {},
   "source": [
    "The black dots in the figure directly above form the best square lattice fit to\n",
    "the sufficiently visible ZOLZ reflections.\n",
    "\n",
    "Now let's do the same fitting procedure for the undistorted SAED pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69049072-2891-41e6-a759-e5320038e102",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"visible_zolz_reflections\": \\\n",
    "          zolz_reflection_selection_of_undistorted_saed_pattern, \n",
    "          \"saed_pattern_signal\": \\\n",
    "          undistorted_saed_pattern_signal}\n",
    "_ = fit_visible_zolz_reflections_to_square_lattice(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23fc829-cda2-4b2a-843c-7e58b671ec40",
   "metadata": {},
   "source": [
    "As we can see both visually and from the lattice fit errors, our ML approach\n",
    "corrects an appreciable amount of the distortion in the SAED pattern."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
