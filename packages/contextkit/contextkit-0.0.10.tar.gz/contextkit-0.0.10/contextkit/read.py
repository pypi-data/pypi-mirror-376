"""Read X for llm context"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_read.ipynb.

# %% auto 0
__all__ = ['read_url', 'read_text', 'read_link', 'read_gist', 'read_gh_file', 'read_file', 'read_dir', 'read_pdf',
           'read_google_sheet', 'read_gdoc', 'read_arxiv', 'read_gh_repo']

# %% ../nbs/00_read.ipynb 5
import httpx 
import html2text
from fastcore.all import delegates, ifnone

import re, os, glob, string, warnings, functools
import requests
import fnmatch, mimetypes

from pypdf import PdfReader
from toolslm.download import html2md, read_html

import tempfile, subprocess, os, re, shutil
from pathlib import Path

from typing import Optional, List, Dict, Union

# %% ../nbs/00_read.ipynb 8
def read_text(url, # URL to read
             ): # Text from page
    "Get text from `url`"
    return httpx.get(url, follow_redirects=True).text

# %% ../nbs/00_read.ipynb 10
def read_link(url: str,   # URL to read
             heavy: bool = False,   # Use headless browser (requires extra setup steps before use)
             sel: Optional[str] = None,  # Css selector to pull content from
             useJina: bool = False, # Use Jina for the markdown conversion
             ignore_links: bool = False, # Whether to keep links or not
             ): 
    "Reads a url and converts to markdown"
    if not heavy and not useJina: return read_html(url,sel=sel, ignore_links=ignore_links)
    elif not heavy and useJina:   return httpx.get(f"https://r.jina.ai/{url}").text
    elif heavy and not useJina: 
        import playwrightnb
        return playwrightnb.url2md(url,sel=ifnone(sel,'body'))
    elif heavy and useJina: raise NotImplementedError("Unsupported. No benefit to using Jina with playwrightnb")

# %% ../nbs/00_read.ipynb 14
def read_url(*args,**kwargs):
    warnings.warn("read_url() is deprecated, use read_link() instead. It is behaviorally identical.", 
                  DeprecationWarning, stacklevel=2)
    return read_link(*args,**kwargs)

read_url = functools.wraps(read_link)(read_url)

# %% ../nbs/00_read.ipynb 18
def read_gist(url:str  # gist URL, of gist to read
             ):
    "Returns raw gist content, or None"
    pattern = r'https://gist\.github\.com/([^/]+)/([^/]+)'
    match = re.match(pattern, url)
    if match:
        user, gist_id = match.groups()
        raw_url = f'https://gist.githubusercontent.com/{user}/{gist_id}/raw'
        return httpx.get(raw_url).text
    else:
        return None

# %% ../nbs/00_read.ipynb 22
def read_gh_file(url:str # GitHub URL of the file to read
                ):
    "Reads the contents of a file from its GitHub URL"
    pattern = r'https://github\.com/([^/]+)/([^/]+)/blob/([^/]+)/(.+)'
    replacement = r'https://raw.githubusercontent.com/\1/\2/refs/heads/\3/\4'
    raw_url = re.sub(pattern, replacement, url)
    return httpx.get(raw_url).text

# %% ../nbs/00_read.ipynb 26
def read_file(path:str):
    "returns file contents"
    with open(path,'r') as f: return f.read()

# %% ../nbs/00_read.ipynb 27
def _is_unicode(filepath:str, sample_size:int=1024):
    try:
        with open(filepath, 'r') as file: sample = file.read(sample_size)
        return True
    except UnicodeDecodeError: return False

# %% ../nbs/00_read.ipynb 30
def read_dir(path: str,                          # path to read
             unicode_only: bool = True,             # ignore non-unicode files
             included_patterns: List[str] = ["*"],       # glob pattern of files to include
             excluded_patterns: List[str] = [".git/**"], # glob pattern of files to exclude
             verbose: bool = False,                # log paths of files being read
             as_dict: bool = False                  # returns dict of {path,content}
            ) -> Union[str, Dict[str, str]]:            # returns string with contents of files read
    """Reads files in path, returning a dict with the filenames and contents if as_dict=True, otherwise concatenating file contents into a single string. Takes optional glob patterns for files to include or exclude."""
    pattern = '**/*'
    result = {}
    for file_path in glob.glob(os.path.join(path, pattern), recursive=True):
        if any(fnmatch.fnmatch(file_path, pat) for pat in excluded_patterns):
            continue
        if not any(fnmatch.fnmatch(file_path, pat) for pat in included_patterns):
            continue
        if os.path.isfile(file_path):
            if unicode_only and not _is_unicode(file_path):
                continue
            if verbose:
                print(f"Including {file_path}")
            with open(file_path, 'r', errors='ignore') as f:
                result[file_path] = f.read()
    if not as_dict:
        return '\n'.join([f"--- File: {file_path} ---\n{v}\n--- End of {file_path} ---" for file_path,v in result.items()])
    else:
        return result

# %% ../nbs/00_read.ipynb 33
def read_pdf(file_path: str # path of PDF file to read
            ) -> str:
    "Reads the text of a PDF with PdfReader"
    with open(file_path, 'rb') as file:
        reader = PdfReader(file)
        return ' '.join(page.extract_text() for page in reader.pages)

# %% ../nbs/00_read.ipynb 38
def read_google_sheet(url: str # URL of a Google Sheet to read
                     ):
    "Reads the contents of a Google Sheet into text"
    sheet_id = url.split('/d/')[1].split('/')[0]
    csv_url = f'https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv&id={sheet_id}&gid=0'
    res = requests.get(url=csv_url)
    return res.content

# %% ../nbs/00_read.ipynb 43
def read_gdoc(url: str  # URL of Google Doc to read
             ):
    "Gets the text content of a Google Doc using html2text"
    import html2text
    doc_url = url
    doc_id = doc_url.split('/d/')[1].split('/')[0]
    export_url = f'https://docs.google.com/document/d/{doc_id}/export?format=html'
    html_doc_content = requests.get(export_url).text
    doc_content = html2text.html2text(html_doc_content)
    return doc_content

# %% ../nbs/00_read.ipynb 46
def read_arxiv(url:str, # arxiv PDF URL, or arxiv abstract URL, or arxiv ID
               save_pdf:bool=False, # True, will save the downloaded PDF
               save_dir:str='.' # directory in which to save the PDF
              ):
    "Get paper information from arxiv URL or ID, optionally saving PDF to disk"
    import re, httpx, tarfile, io, os
    import xml.etree.ElementTree as ET
    
    if save_pdf: os.makedirs(save_dir, exist_ok=True)
    arxiv_id = url.split('/')[-1] if '/' in url else url
    
    # Remove version number if present but save it for downloads
    version = re.search(r'v(\d+)$', arxiv_id)
    version_num = version.group(1) if version else None
    arxiv_id = re.sub(r'v\d+$', '', arxiv_id)
    
    api_url = f'https://export.arxiv.org/api/query?id_list={arxiv_id}'
    
    response = httpx.get(api_url)
    
    if response.status_code != 200: raise Exception(f"Failed to fetch arxiv data: {response.status_code}")
    
    root = ET.fromstring(response.text)
    ns = {'arxiv': 'http://www.w3.org/2005/Atom'}
    entry = root.find('arxiv:entry', ns)
    if entry is None: raise Exception("No paper found")
    
    links = entry.findall('arxiv:link', ns)
    pdf_url = next((l.get('href') for l in links if l.get('title') == 'pdf'), None)
    
    result = {
        'title': entry.find('arxiv:title', ns).text.strip(),
        'authors': [author.find('arxiv:name', ns).text for author in entry.findall('arxiv:author', ns)],
        'summary': entry.find('arxiv:summary', ns).text.strip(),
        'published': entry.find('arxiv:published', ns).text,
        'link': entry.find('arxiv:id', ns).text,
        'pdf_url': pdf_url
    }
    
    if save_pdf and pdf_url:
        pdf_response = httpx.get(pdf_url)
        if pdf_response.status_code == 200:
            pdf_filename = f"{arxiv_id}{'v'+version_num if version_num else ''}.pdf"
            pdf_path = os.path.join(save_dir, pdf_filename)
            with open(pdf_path, 'wb') as f:
                f.write(pdf_response.content)
            result['pdf_path'] = pdf_path
    
    source_url = f'https://arxiv.org/e-print/{arxiv_id}{"v"+version_num if version_num else ""}'
    try:
        source_response = httpx.get(source_url)
        if source_response.status_code == 200:
            # Try to extract main tex file from tar archive
            tar_content = io.BytesIO(source_response.content)
            with tarfile.open(fileobj=tar_content, mode='r:*') as tar:
                # Look for main tex file
                tex_files = [f for f in tar.getnames() if f.endswith('.tex')]
                if tex_files:
                    main_tex = tar.extractfile(tex_files[0])
                    result['source'] = main_tex.read().decode('utf-8', errors='ignore')
    except Exception as e:
        result['source_error'] = str(e)
    
    return result

# %% ../nbs/00_read.ipynb 48
def _gh_ssh_from_gh_url(gh_repo_address:str):
    "Given a GH URL or SSH remote address, returns a GH URL or None"
    pattern = r'https://github\.com/([^/]+)/([^/]+)(?:/.*)?'
    if gh_repo_address.startswith("git@github.com:"): return gh_repo_address
    elif match := re.match(pattern, gh_repo_address):
        user, repo = match.groups()
        return f'git@github.com:{user}/{repo}.git'
    # Not a GitHub URL or a GitHub SSH remote address
    else: return None

def _get_default_branch(repo_path:str):
    "master or main"
    try:
        result = subprocess.run(['git', 'symbolic-ref', 'refs/remotes/origin/HEAD'], 
                                cwd=repo_path, capture_output=True, text=True, check=True)
        return result.stdout.strip().split('/')[-1]
    except subprocess.CalledProcessError:
        return 'main'  # Default to 'main' if we can't determine the branch

def _get_git_repo(gh_ssh:str):
    "Fetchs from a GH SSH address, returns a path"
    repo_name = gh_ssh.split('/')[-1].replace('.git', '')
    cache_dir = Path(os.environ.get('XDG_CACHE_HOME', Path.home() / '.cache')) / 'contextkit_git_clones'
    cache_dir.mkdir(parents=True, exist_ok=True)
    repo_dir = cache_dir / repo_name

    if repo_dir.exists():
        try:
            subprocess.run(['git', 'fetch'], cwd=repo_dir, check=True, capture_output=True)
            default_branch = _get_default_branch(repo_dir)
            subprocess.run(['git', 'reset', '--hard', f'origin/{default_branch}'], 
                           cwd=repo_dir, check=True, capture_output=True)
            return str(repo_dir)
        except subprocess.CalledProcessError:
            shutil.rmtree(repo_dir)  # Remove the cached directory if update fails

    with tempfile.TemporaryDirectory() as temp_dir:
        try:
            print("Cloning repo.")
            subprocess.run(['git', 'clone', gh_ssh], cwd=temp_dir, check=True, capture_output=False)
            cloned_dir = Path(temp_dir) / repo_name
            shutil.move(str(cloned_dir), str(repo_dir))
            return str(repo_dir)
        except subprocess.CalledProcessError as e:
            print(f"Error cloning repo from cwd {temp_dir} with error {e}")
            return None

# %% ../nbs/00_read.ipynb 49
def read_gh_repo(path_or_url:str,    # Repo's GitHub URL, or GH SSH address, or file path
                 as_dict:bool=True,  # if True, will return repo contents {path,content} dict
                 verbose:bool=False  # if True, will log paths of files being read
                ):
    "Repo contents from path, GH URL, or GH SSH address"
    gh_ssh = _gh_ssh_from_gh_url(path_or_url)
    path = path_or_url if not gh_ssh else _get_git_repo(gh_ssh)
    return read_dir(path,verbose=verbose,as_dict=as_dict)
