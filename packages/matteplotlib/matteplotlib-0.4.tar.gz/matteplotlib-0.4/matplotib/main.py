# main.py

def hello():
    print("Hello test!")

def ml_index():
    print(
        "1. Data Pre-processing and Exploration \n"
        "1a. Load a CSV dataset. Handle missing values, inconsistent formatting, and outliers.  \n"
        "1b. Load a dataset, calculate descriptive summary statistics, create visualizations using different graphs, and identify potential features and target variables Note: Explore Univariate and Bivariate graphs (Matplotlib) and Seaborn for visualization.  \n"
        "1c. Create or Explore datasets to use all pre-processing routines like label encoding, scaling and binerization.  \n"
        "1d. Design a simple machine learning model to train the training instances and test the same. \n\n"

        "2. Testing Hypothesis \n"
        "2a. Implement and demonstrate the find-s algorithm for finding the most specific hypothesis based on given set of training data samples. Read the training data from a. CSV file and generate the final specific hypothesis (Create your dataset). \n\n"

        "3. Linear Models \n"
        "3a. Simple Linear Regression: Fit a linear regression model on a dataset. Interpret coefficients, make predictions, and evaluate performance using metrics like R-squared and MSE.  \n"
        "3b. Multiple Linear Regression: Extend linear regression to multiple features. Handle feature selection and potential multi collinearity.  \n"
        "3c. Regularized Linear Models (Ridge, Lasso, ElasticNet): Implement regression variants like LASSO aid Ridge on any generated dataset. \n\n"

        "4. Discriminative Models \n"
        "4a. Logistic Regression: Perform binary classification using logistic regression. Calculate accuracy, precision, recall, and understand the ROC curve.  \n"
        "4b. k-nearest Neighbor: Implement and demonstrate k-nearest Neighbor algorithm. Read the training data from .CSV file and build a model to classify the test sample. Print both correct and wrong predictions.  \n"
        "4c. Decision Tree: Build decision tree classifier or regressor. Control hyperparameters like tree depth to avoid overfitting. Visualize the tree.  \n"
        "4d. Support Vector Machine: Implement a Support Vector Machine for any relevant dataset.  \n"
        "4e. Random Forest ensemble: Train the random forest ensemble. Experiment with the number of trees and feature sampling. Compare performance to a single decision tree.  \n"
        "4f. Gradient Boosting machine: Implement a gradient boosting machine. Tune hyper parameters and explore feature importance. \n\n"

        "5. Generative Models \n"
        "5a. Implement and demonstrate the working of a Na√Øve Bayesian classifier using a sample data set. Build the model to classify a test sample.  \n"
        "5b. Implement Hidden Markov Models using hmmlearn. \n\n"

        "6. Probabilistic Models \n"
        "6a. Implement Bayesian Linear Regression to explore prior and posterior distribution.  \n"
        "6b. Implement Gaussian Mixture Models for density estimation and unsupervised clustering. \n\n"

        "7. Model Evaluation and Hyperparameter Tuning \n"
        "7a. Implement cross-validation techniques (k-fold, stratified, etc.) for robust model evaluation.  \n"
        "7b. Systematically explore combinations of hyperparameters to optimize model performance. (use grid and randomized search). \n\n"

        "8. Bayesian learning \n"
        "8a. Implement Bayesian Learning using inferences. \n\n"

        "9. Deep Generative Models \n"
        "9a. Set up a generator network to produce samples and a discriminator network to distinguish between real and generated data. (Use a simple dataset). \n\n"

        "10. Model Deployment \n"
        "10a. Develop an API to deploy your model and perform predictions. \n\n"
        )
    