#!/usr/bin/env python3
"""
Script de validaci√≥n de producci√≥n
IA Agent para Generaci√≥n de Pruebas Unitarias .NET
"""

import sys
import time
import logging
from pathlib import Path
from typing import Dict, List, Any

# Agregar src al path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from config.environment import environment_manager
from memory.memory_manager import memory_manager
from monitoring.performance_optimizer import performance_optimizer
from utils.error_handler import error_handler
from agents.analysis_agent import analysis_agent
from tools.file_tools import file_manager
from tools.dotnet_tools import dotnet_manager


class ProductionValidator:
    """Validador de producci√≥n"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.config = environment_manager.get_config()
        self.validation_results = {}
        self.start_time = time.time()
    
    def validate_environment(self) -> Dict[str, Any]:
        """Validar configuraci√≥n de entorno"""
        self.logger.info("üîç Validando configuraci√≥n de entorno...")
        
        results = {
            "environment_info": environment_manager.get_environment_info(),
            "is_production": environment_manager.is_production(),
            "is_development": environment_manager.is_development(),
            "config_valid": True,
            "issues": []
        }
        
        # Verificar API key
        if not self.config.openai_api_key:
            results["issues"].append("OPENAI_API_KEY no configurado")
            results["config_valid"] = False
        
        # Verificar directorios
        required_dirs = [
            self.config.chromadb_persist_directory,
            self.config.temp_directory,
            self.config.output_directory
        ]
        
        for directory in required_dirs:
            if not Path(directory).exists():
                results["issues"].append(f"Directorio no existe: {directory}")
                results["config_valid"] = False
        
        self.logger.info(f"‚úÖ Configuraci√≥n de entorno: {'OK' if results['config_valid'] else 'ISSUES'}")
        return results
    
    def validate_agents(self) -> Dict[str, Any]:
        """Validar agentes de IA"""
        self.logger.info("ü§ñ Validando agentes de IA...")
        
        results = {
            "agents_available": {},
            "agents_functional": {},
            "total_agents": 0,
            "functional_agents": 0
        }
        
        agents = {
            "analysis_agent": analysis_agent,
            # Agregar otros agentes cuando est√©n disponibles
        }
        
        for agent_name, agent in agents.items():
            try:
                # Verificar que el agente existe
                results["agents_available"][agent_name] = agent is not None
                results["total_agents"] += 1
                
                # Verificar funcionalidad b√°sica
                if agent and hasattr(agent, 'name'):
                    results["agents_functional"][agent_name] = True
                    results["functional_agents"] += 1
                else:
                    results["agents_functional"][agent_name] = False
                
            except Exception as e:
                self.logger.error(f"Error validando agente {agent_name}: {e}")
                results["agents_functional"][agent_name] = False
        
        success_rate = (results["functional_agents"] / results["total_agents"] * 100) if results["total_agents"] > 0 else 0
        self.logger.info(f"‚úÖ Agentes: {results['functional_agents']}/{results['total_agents']} ({success_rate:.1f}%)")
        
        return results
    
    def validate_tools(self) -> Dict[str, Any]:
        """Validar herramientas"""
        self.logger.info("üîß Validando herramientas...")
        
        results = {
            "file_manager": False,
            "dotnet_manager": False,
            "dotnet_version": None,
            "tools_functional": 0,
            "total_tools": 2
        }
        
        # Validar file manager
        try:
            test_file = "test_validation.tmp"
            file_manager.write_file(test_file, "test content")
            content = file_manager.read_file(test_file)
            file_manager.delete_file(test_file)
            
            if content == "test content":
                results["file_manager"] = True
                results["tools_functional"] += 1
        except Exception as e:
            self.logger.error(f"Error validando file manager: {e}")
        
        # Validar dotnet manager
        try:
            version = dotnet_manager.get_dotnet_version()
            if version and "Error" not in version:
                results["dotnet_manager"] = True
                results["dotnet_version"] = version
                results["tools_functional"] += 1
        except Exception as e:
            self.logger.error(f"Error validando dotnet manager: {e}")
        
        success_rate = (results["tools_functional"] / results["total_tools"] * 100)
        self.logger.info(f"‚úÖ Herramientas: {results['tools_functional']}/{results['total_tools']} ({success_rate:.1f}%)")
        
        return results
    
    def validate_memory_system(self) -> Dict[str, Any]:
        """Validar sistema de memoria"""
        self.logger.info("üß† Validando sistema de memoria...")
        
        results = {
            "memory_manager": False,
            "cache_functional": False,
            "shared_memory": False,
            "memory_stats": {}
        }
        
        try:
            # Verificar memory manager
            if memory_manager:
                results["memory_manager"] = True
                
                # Verificar cache
                test_key = "validation_test"
                test_value = "test_data"
                memory_manager.add_to_cache(test_key, test_value)
                retrieved_value = memory_manager.get_from_cache(test_key)
                
                if retrieved_value == test_value:
                    results["cache_functional"] = True
                
                # Verificar memoria compartida
                shared_memory = memory_manager.get_shared_memory()
                results["shared_memory"] = shared_memory is not None
                
                # Obtener estad√≠sticas
                results["memory_stats"] = memory_manager.get_memory_stats()
                
        except Exception as e:
            self.logger.error(f"Error validando sistema de memoria: {e}")
        
        functional_components = sum([
            results["memory_manager"],
            results["cache_functional"],
            results["shared_memory"]
        ])
        
        self.logger.info(f"‚úÖ Sistema de memoria: {functional_components}/3 componentes funcionales")
        
        return results
    
    def validate_performance(self) -> Dict[str, Any]:
        """Validar rendimiento del sistema"""
        self.logger.info("‚ö° Validando rendimiento del sistema...")
        
        results = {
            "performance_optimizer": False,
            "system_resources": {},
            "metrics_collection": False,
            "optimization_available": False
        }
        
        try:
            # Verificar performance optimizer
            if performance_optimizer:
                results["performance_optimizer"] = True
                
                # Obtener recursos del sistema
                resources = performance_optimizer.get_system_resources()
                if resources:
                    results["system_resources"] = resources.__dict__
                
                # Verificar recolecci√≥n de m√©tricas
                performance_optimizer.add_metric("validation_test", 100.0)
                metrics = performance_optimizer.get_all_metrics()
                if "validation_test" in metrics:
                    results["metrics_collection"] = True
                
                # Verificar optimizaci√≥n
                results["optimization_available"] = len(performance_optimizer.optimization_callbacks) > 0
                
        except Exception as e:
            self.logger.error(f"Error validando rendimiento: {e}")
        
        self.logger.info(f"‚úÖ Rendimiento: {'OK' if results['performance_optimizer'] else 'ISSUES'}")
        
        return results
    
    def validate_error_handling(self) -> Dict[str, Any]:
        """Validar manejo de errores"""
        self.logger.info("üõ°Ô∏è Validando manejo de errores...")
        
        results = {
            "error_handler": False,
            "error_categories": 0,
            "error_callbacks": 0,
            "error_stats": {}
        }
        
        try:
            # Verificar error handler
            if error_handler:
                results["error_handler"] = True
                
                # Contar categor√≠as de error
                results["error_categories"] = len(error_handler.error_callbacks)
                
                # Contar callbacks
                total_callbacks = sum(len(callbacks) for callbacks in error_handler.error_callbacks.values())
                results["error_callbacks"] = total_callbacks
                
                # Obtener estad√≠sticas
                results["error_stats"] = error_handler.get_error_stats()
                
        except Exception as e:
            self.logger.error(f"Error validando manejo de errores: {e}")
        
        self.logger.info(f"‚úÖ Manejo de errores: {'OK' if results['error_handler'] else 'ISSUES'}")
        
        return results
    
    def run_full_validation(self) -> Dict[str, Any]:
        """Ejecutar validaci√≥n completa"""
        self.logger.info("üöÄ Iniciando validaci√≥n completa de producci√≥n...")
        
        validation_results = {
            "timestamp": time.time(),
            "duration": 0,
            "overall_status": "PENDING",
            "validations": {}
        }
        
        try:
            # Ejecutar todas las validaciones
            validation_results["validations"]["environment"] = self.validate_environment()
            validation_results["validations"]["agents"] = self.validate_agents()
            validation_results["validations"]["tools"] = self.validate_tools()
            validation_results["validations"]["memory"] = self.validate_memory_system()
            validation_results["validations"]["performance"] = self.validate_performance()
            validation_results["validations"]["error_handling"] = self.validate_error_handling()
            
            # Calcular estado general
            validation_results["duration"] = time.time() - self.start_time
            
            # Determinar estado general
            all_validations = validation_results["validations"]
            critical_issues = 0
            
            # Verificar problemas cr√≠ticos
            if not all_validations["environment"]["config_valid"]:
                critical_issues += 1
            
            if all_validations["agents"]["functional_agents"] == 0:
                critical_issues += 1
            
            if all_validations["tools"]["tools_functional"] == 0:
                critical_issues += 1
            
            # Determinar estado
            if critical_issues == 0:
                validation_results["overall_status"] = "PASS"
            elif critical_issues <= 2:
                validation_results["overall_status"] = "WARNING"
            else:
                validation_results["overall_status"] = "FAIL"
            
            self.logger.info(f"üéØ Validaci√≥n completada: {validation_results['overall_status']}")
            self.logger.info(f"‚è±Ô∏è Duraci√≥n: {validation_results['duration']:.2f} segundos")
            
        except Exception as e:
            self.logger.error(f"Error en validaci√≥n completa: {e}")
            validation_results["overall_status"] = "ERROR"
            validation_results["error"] = str(e)
        
        return validation_results
    
    def print_summary(self, results: Dict[str, Any]):
        """Imprimir resumen de validaci√≥n"""
        print("\n" + "="*80)
        print("üìä RESUMEN DE VALIDACI√ìN DE PRODUCCI√ìN")
        print("="*80)
        
        print(f"Estado General: {results['overall_status']}")
        print(f"Duraci√≥n: {results['duration']:.2f} segundos")
        print(f"Timestamp: {time.ctime(results['timestamp'])}")
        
        print("\nüìã DETALLES POR COMPONENTE:")
        print("-" * 40)
        
        for component, validation in results["validations"].items():
            status = "‚úÖ" if validation.get("config_valid", True) else "‚ùå"
            print(f"{status} {component.upper()}")
            
            if component == "environment":
                print(f"   - Producci√≥n: {validation['is_production']}")
                print(f"   - Configuraci√≥n v√°lida: {validation['config_valid']}")
                if validation["issues"]:
                    print(f"   - Problemas: {len(validation['issues'])}")
            
            elif component == "agents":
                print(f"   - Agentes funcionales: {validation['functional_agents']}/{validation['total_agents']}")
            
            elif component == "tools":
                print(f"   - Herramientas funcionales: {validation['tools_functional']}/{validation['total_tools']}")
                if validation.get("dotnet_version"):
                    print(f"   - Versi√≥n .NET: {validation['dotnet_version']}")
            
            elif component == "memory":
                functional = sum([
                    validation["memory_manager"],
                    validation["cache_functional"],
                    validation["shared_memory"]
                ])
                print(f"   - Componentes funcionales: {functional}/3")
            
            elif component == "performance":
                print(f"   - Optimizador activo: {validation['performance_optimizer']}")
                print(f"   - M√©tricas recolectadas: {validation['metrics_collection']}")
            
            elif component == "error_handling":
                print(f"   - Manejador activo: {validation['error_handler']}")
                print(f"   - Categor√≠as de error: {validation['error_categories']}")
        
        print("\n" + "="*80)


def main():
    """Funci√≥n principal"""
    # Configurar logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s | %(levelname)-8s | %(message)s'
    )
    
    validator = ProductionValidator()
    results = validator.run_full_validation()
    validator.print_summary(results)
    
    # Exit code basado en el resultado
    if results["overall_status"] == "PASS":
        sys.exit(0)
    elif results["overall_status"] == "WARNING":
        sys.exit(1)
    else:
        sys.exit(2)


if __name__ == "__main__":
    main()
