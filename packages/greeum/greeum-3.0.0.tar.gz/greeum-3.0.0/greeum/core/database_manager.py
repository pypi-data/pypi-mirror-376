import os
import sqlite3
import json
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import logging

logger = logging.getLogger(__name__)

class DatabaseManager:
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self, connection_string=None, db_type='sqlite'):
        """
        ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ì ì´ˆê¸°í™”
        
        Args:
            connection_string: ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë¬¸ìì—´ (ê¸°ë³¸ê°’: data/memory.db)
            db_type: ë°ì´í„°ë² ì´ìŠ¤ íƒ€ì… (sqlite, postgres ë“±)
        """
        self.db_type = db_type
        
        # Smart Database Path Detection (ì˜µì…˜ 3)
        if connection_string:
            self.connection_string = connection_string
        else:
            self.connection_string = self._get_smart_db_path()
        self._ensure_data_dir()
        self._setup_connection()
        self._create_schemas()
        logger.info(f"DatabaseManager initialization complete: {self.connection_string} (type: {self.db_type})")
    
    def _get_smart_db_path(self) -> str:
        """
        ì§€ëŠ¥í˜• ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ ê°ì§€
        
        ìš°ì„ ìˆœìœ„:
        1. GREEUM_DATA_DIR í™˜ê²½ë³€ìˆ˜ (ëª…ì‹œì  ì„¤ì •)
        2. í˜„ì¬ ë””ë ‰í† ë¦¬ì˜ data/memory.db (í”„ë¡œì íŠ¸ ë¡œì»¬)
        3. ~/greeum-global/data/memory.db (ê¸€ë¡œë²Œ í´ë°±)
        
        Returns:
            str: ìµœì ì˜ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ê²½ë¡œ
        """
        # 1. í™˜ê²½ë³€ìˆ˜ ìš°ì„  (ëª…ì‹œì  ì„¤ì •)
        if 'GREEUM_DATA_DIR' in os.environ:
            env_path = os.path.join(os.environ['GREEUM_DATA_DIR'], 'data', 'memory.db')
            logger.info(f"ğŸ“ Using environment variable path: {env_path}")
            return env_path
        
        # 2. í˜„ì¬ ë””ë ‰í† ë¦¬ì— ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ì¡´ì¬í•˜ë©´ ì‚¬ìš© (í”„ë¡œì íŠ¸ ë¡œì»¬)
        current_dir = os.getcwd()
        local_db_path = os.path.join(current_dir, 'data', 'memory.db')
        
        if os.path.exists(local_db_path):
            logger.info(f"[DB] Found existing local database: {local_db_path}")
            return local_db_path
        
        # 3. í˜„ì¬ ë””ë ‰í† ë¦¬ì— data í´ë”ê°€ ìˆìœ¼ë©´ ì‚¬ìš© (ìƒˆ í”„ë¡œì íŠ¸)
        data_dir_path = os.path.join(current_dir, 'data')
        if os.path.exists(data_dir_path) and os.path.isdir(data_dir_path):
            new_local_path = os.path.join(data_dir_path, 'memory.db')
            logger.info(f"ğŸ“ Using local data directory: {new_local_path}")
            return new_local_path
        
        # 4. ê¸€ë¡œë²Œ ë””ë ‰í† ë¦¬ í´ë°±
        home_dir = os.path.expanduser('~')
        global_db_path = os.path.join(home_dir, 'greeum-global', 'data', 'memory.db')
        logger.info(f"ğŸŒ Using global fallback path: {global_db_path}")
        return global_db_path
    
    def _ensure_data_dir(self):
        """ë°ì´í„° ë””ë ‰í† ë¦¬ ì¡´ì¬ í™•ì¸"""
        data_dir = os.path.dirname(self.connection_string)
        if data_dir:
            os.makedirs(data_dir, exist_ok=True)
    
    def _setup_connection(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •"""
        if self.db_type == 'sqlite':
            self.conn = sqlite3.connect(self.connection_string)
            self.conn.row_factory = sqlite3.Row
        elif self.db_type == 'postgres':
            try:
                import psycopg2
                from psycopg2.extras import RealDictCursor
                self.conn = psycopg2.connect(self.connection_string)
                self.conn.cursor_factory = RealDictCursor
            except ImportError:
                raise ImportError("PostgreSQL ì§€ì›ì„ ìœ„í•´ psycopg2ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ë² ì´ìŠ¤ íƒ€ì…: {self.db_type}")
    
    def _create_schemas(self):
        """í•„ìš”í•œ í…Œì´ë¸” ìƒì„±"""
        cursor = self.conn.cursor()
        
        # Create v3.0.0 tables if needed
        self._create_v3_tables(cursor)
        
        # ë¸”ë¡ í…Œì´ë¸”
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS blocks (
            block_index INTEGER PRIMARY KEY,
            timestamp TEXT NOT NULL,
            context TEXT NOT NULL,
            importance REAL NOT NULL,
            hash TEXT NOT NULL,
            prev_hash TEXT NOT NULL
        )
        ''')
        
        # í‚¤ì›Œë“œ í…Œì´ë¸” (M:N ê´€ê³„)
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS block_keywords (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            block_index INTEGER NOT NULL,
            keyword TEXT NOT NULL,
            FOREIGN KEY (block_index) REFERENCES blocks(block_index),
            UNIQUE(block_index, keyword)
        )
        ''')
        
        # íƒœê·¸ í…Œì´ë¸” (M:N ê´€ê³„)
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS block_tags (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            block_index INTEGER NOT NULL,
            tag TEXT NOT NULL,
            FOREIGN KEY (block_index) REFERENCES blocks(block_index),
            UNIQUE(block_index, tag)
        )
        ''')
        
        # ë©”íƒ€ë°ì´í„° í…Œì´ë¸” (JSON ì €ì¥)
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS block_metadata (
            block_index INTEGER PRIMARY KEY,
            metadata TEXT NOT NULL,
            FOREIGN KEY (block_index) REFERENCES blocks(block_index)
        )
        ''')
        
        # ì„ë² ë”© í…Œì´ë¸”
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS block_embeddings (
            block_index INTEGER PRIMARY KEY,
            embedding BLOB NOT NULL,
            embedding_model TEXT,
            embedding_dim INTEGER,
            FOREIGN KEY (block_index) REFERENCES blocks(block_index)
        )
        ''')
        
        # ë‹¨ê¸° ê¸°ì–µ í…Œì´ë¸”
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS short_term_memories (
            id TEXT PRIMARY KEY,
            timestamp TEXT NOT NULL,
            content TEXT NOT NULL,
            speaker TEXT,
            metadata TEXT
        )
        ''')
        
        # ì¸ë±ìŠ¤ ìƒì„±
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_blocks_timestamp ON blocks(timestamp)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_block_keywords ON block_keywords(keyword)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_block_tags ON block_tags(tag)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_stm_timestamp ON short_term_memories(timestamp)')
        
        self.conn.commit()
    
    def _create_v3_tables(self, cursor):
        """Create v3.0.0 association-based memory tables"""
        # Memory nodes table (v3.0.0)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS memory_nodes (
                node_id TEXT PRIMARY KEY,
                memory_id INTEGER,
                node_type TEXT,
                content TEXT,
                embedding TEXT,
                activation_level REAL DEFAULT 0.0,
                last_activated TEXT,
                metadata TEXT,
                created_at TEXT,
                FOREIGN KEY (memory_id) REFERENCES blocks(block_index)
            )
        ''')
        
        # Associations table (v3.0.0)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS associations (
                association_id TEXT PRIMARY KEY,
                source_node_id TEXT,
                target_node_id TEXT,
                association_type TEXT,
                strength REAL DEFAULT 0.5,
                weight REAL DEFAULT 1.0,
                created_at TEXT,
                last_activated TEXT,
                activation_count INTEGER DEFAULT 0,
                metadata TEXT,
                FOREIGN KEY (source_node_id) REFERENCES memory_nodes(node_id),
                FOREIGN KEY (target_node_id) REFERENCES memory_nodes(node_id)
            )
        ''')
        
        # Activation history (v3.0.0)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS activation_history (
                history_id INTEGER PRIMARY KEY AUTOINCREMENT,
                node_id TEXT,
                activation_level REAL,
                trigger_type TEXT,
                trigger_source TEXT,
                timestamp TEXT,
                session_id TEXT,
                FOREIGN KEY (node_id) REFERENCES memory_nodes(node_id)
            )
        ''')
        
        # Context sessions (v3.0.0)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS context_sessions (
                session_id TEXT PRIMARY KEY,
                active_nodes TEXT,
                activation_snapshot TEXT,
                created_at TEXT,
                last_updated TEXT,
                metadata TEXT
            )
        ''')
        
        # Create indexes for v3 tables
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_nodes_memory ON memory_nodes(memory_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_nodes_activation ON memory_nodes(activation_level)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_associations_source ON associations(source_node_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_associations_target ON associations(target_node_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_associations_strength ON associations(strength)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_activation_history_node ON activation_history(node_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_activation_history_session ON activation_history(session_id)')
        
        # Actant model tables (v3.0.0)
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS memory_actants (
                actant_id TEXT PRIMARY KEY,
                memory_id INTEGER,
                
                -- Primary Actants (required)
                subject_raw TEXT,
                subject_hash TEXT,
                action_raw TEXT,
                action_hash TEXT,
                object_raw TEXT,
                object_hash TEXT,
                
                -- Secondary Actants (optional)
                sender_raw TEXT,
                sender_hash TEXT,
                receiver_raw TEXT,
                receiver_hash TEXT,
                helper_raw TEXT,
                helper_hash TEXT,
                opponent_raw TEXT,
                opponent_hash TEXT,
                
                -- Metadata
                confidence REAL DEFAULT 0.5,
                parser_version TEXT,
                parsed_at TEXT,
                metadata TEXT,
                
                FOREIGN KEY (memory_id) REFERENCES blocks(block_index)
            )
        ''')
        
        # Entity normalization table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS actant_entities (
                entity_hash TEXT PRIMARY KEY,
                entity_type TEXT,
                canonical_form TEXT,
                variations TEXT,
                first_seen TEXT,
                last_seen TEXT,
                occurrence_count INTEGER DEFAULT 1,
                metadata TEXT
            )
        ''')
        
        # Action normalization table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS actant_actions (
                action_hash TEXT PRIMARY KEY,
                action_type TEXT,
                canonical_form TEXT,
                variations TEXT,
                tense TEXT,
                aspect TEXT,
                first_seen TEXT,
                last_seen TEXT,
                occurrence_count INTEGER DEFAULT 1,
                metadata TEXT
            )
        ''')
        
        # Actant relations table
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS actant_relations (
                relation_id TEXT PRIMARY KEY,
                source_actant_id TEXT,
                target_actant_id TEXT,
                relation_type TEXT,
                strength REAL DEFAULT 0.5,
                evidence_count INTEGER DEFAULT 1,
                created_at TEXT,
                last_updated TEXT,
                metadata TEXT,
                
                FOREIGN KEY (source_actant_id) REFERENCES memory_actants(actant_id),
                FOREIGN KEY (target_actant_id) REFERENCES memory_actants(actant_id)
            )
        ''')
        
        # Create indexes for actant tables
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_actants_memory ON memory_actants(memory_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_actants_subject ON memory_actants(subject_hash)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_actants_action ON memory_actants(action_hash)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_actants_object ON memory_actants(object_hash)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_entities_type ON actant_entities(entity_type)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_actions_type ON actant_actions(action_type)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_relations_source ON actant_relations(source_actant_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_relations_target ON actant_relations(target_actant_id)')
    
    def add_block(self, block_data: Dict[str, Any]) -> int:
        """
        ìƒˆ ë¸”ë¡ ì¶”ê°€
        
        Args:
            block_data: ë¸”ë¡ ë°ì´í„°
            
        Returns:
            ì¶”ê°€ëœ ë¸”ë¡ì˜ ì¸ë±ìŠ¤
        """
        cursor = self.conn.cursor()
        logger.debug(f"ìƒˆ ë¸”ë¡ ì¶”ê°€ ì‹œë„: index={block_data.get('block_index')}")
        
        # 1. ë¸”ë¡ ê¸°ë³¸ ì •ë³´ ì‚½ì…
        cursor.execute('''
        INSERT INTO blocks (block_index, timestamp, context, importance, hash, prev_hash)
        VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            block_data.get('block_index'),
            block_data.get('timestamp'),
            block_data.get('context'),
            block_data.get('importance', 0.0),
            block_data.get('hash'),
            block_data.get('prev_hash', '')
        ))
        
        block_index = block_data.get('block_index')
        
        # 2. í‚¤ì›Œë“œ ì‚½ì…
        keywords = block_data.get('keywords', [])
        for keyword in keywords:
            cursor.execute('''
            INSERT OR IGNORE INTO block_keywords (block_index, keyword)
            VALUES (?, ?)
            ''', (block_index, keyword))
        
        # 3. íƒœê·¸ ì‚½ì…
        tags = block_data.get('tags', [])
        for tag in tags:
            cursor.execute('''
            INSERT OR IGNORE INTO block_tags (block_index, tag)
            VALUES (?, ?)
            ''', (block_index, tag))
        
        # 4. ë©”íƒ€ë°ì´í„° ì‚½ì…
        metadata = block_data.get('metadata', {})
        if metadata:
            cursor.execute('''
            INSERT INTO block_metadata (block_index, metadata)
            VALUES (?, ?)
            ''', (block_index, json.dumps(metadata)))
        
        # 5. ì„ë² ë”© ì €ì¥
        embedding = block_data.get('embedding')
        if embedding:
            # NumPy ë°°ì—´ë¡œ ë³€í™˜ í›„ ë°”ì´ë„ˆë¦¬ë¡œ ì €ì¥
            if isinstance(embedding, list):
                embedding_array = np.array(embedding, dtype=np.float32)
            else:
                embedding_array = embedding
                
            cursor.execute('''
            INSERT INTO block_embeddings (block_index, embedding, embedding_model, embedding_dim)
            VALUES (?, ?, ?, ?)
            ''', (
                block_index,
                embedding_array.tobytes(),
                block_data.get('embedding_model', 'default'),
                len(embedding_array)
            ))
        
        self.conn.commit()
        logger.info(f"Block added successfully: index={block_index}")
        return block_index
    
    def get_block(self, block_index: int) -> Optional[Dict[str, Any]]:
        """
        ë¸”ë¡ ì¡°íšŒ
        
        Args:
            block_index: ë¸”ë¡ ì¸ë±ìŠ¤
            
        Returns:
            ë¸”ë¡ ë°ì´í„° (ì—†ìœ¼ë©´ None)
        """
        cursor = self.conn.cursor()
        logger.debug(f"Attempting to retrieve block: index={block_index}")
        
        # 1. ê¸°ë³¸ ë¸”ë¡ ë°ì´í„° ì¡°íšŒ
        cursor.execute('''
        SELECT * FROM blocks WHERE block_index = ?
        ''', (block_index,))
        
        row = cursor.fetchone()
        if not row:
            logger.warning(f"Block retrieval failed: index={block_index} not found")
            return None
            
        # dictë¡œ ë³€í™˜
        if self.db_type == 'sqlite':
            block = dict(row)
        else:
            block = row
        
        # 2. í‚¤ì›Œë“œ ì¡°íšŒ
        cursor.execute('''
        SELECT keyword FROM block_keywords WHERE block_index = ?
        ''', (block_index,))
        keywords = [row[0] for row in cursor.fetchall()]
        block['keywords'] = keywords
        
        # 3. íƒœê·¸ ì¡°íšŒ
        cursor.execute('''
        SELECT tag FROM block_tags WHERE block_index = ?
        ''', (block_index,))
        tags = [row[0] for row in cursor.fetchall()]
        block['tags'] = tags
        
        # 4. ë©”íƒ€ë°ì´í„° ì¡°íšŒ
        cursor.execute('''
        SELECT metadata FROM block_metadata WHERE block_index = ?
        ''', (block_index,))
        row = cursor.fetchone()
        if row:
            block['metadata'] = json.loads(row[0])
        else:
            block['metadata'] = {}
        
        # 5. ì„ë² ë”© ì¡°íšŒ
        cursor.execute('''
        SELECT embedding, embedding_dim, embedding_model FROM block_embeddings WHERE block_index = ?
        ''', (block_index,))
        row = cursor.fetchone()
        if row:
            embedding_bytes = row[0]
            embedding_dim = row[1]
            embedding_model = row[2]
            
            # ë°”ì´ë„ˆë¦¬ì—ì„œ NumPy ë°°ì—´ë¡œ ë³€í™˜
            embedding_array = np.frombuffer(embedding_bytes, dtype=np.float32)
            if embedding_dim:
                embedding_array = embedding_array[:embedding_dim]
                
            block['embedding'] = embedding_array.tolist()
            block['embedding_model'] = embedding_model
        
        logger.debug(f"ë¸”ë¡ ì¡°íšŒ ì„±ê³µ: index={block_index}")
        return block
    
    def get_blocks(self, start_idx: Optional[int] = None, end_idx: Optional[int] = None,
                  limit: int = 100, offset: int = 0,
                  sort_by: str = 'block_index', order: str = 'asc') -> List[Dict[str, Any]]:
        """
        ë¸”ë¡ ëª©ë¡ ì¡°íšŒ
        
        Args:
            start_idx: ì‹œì‘ ì¸ë±ìŠ¤
            end_idx: ì¢…ë£Œ ì¸ë±ìŠ¤
            limit: ìµœëŒ€ ë°˜í™˜ ê°œìˆ˜
            offset: ì‹œì‘ ì˜¤í”„ì…‹
            sort_by: ì •ë ¬ ê¸°ì¤€ í•„ë“œ (ì˜ˆ: 'block_index', 'timestamp', 'importance')
            order: ì •ë ¬ ìˆœì„œ ('asc' ë˜ëŠ” 'desc')
            
        Returns:
            ë¸”ë¡ ëª©ë¡
        """
        cursor = self.conn.cursor()
        
        # ìœ íš¨í•œ ì •ë ¬ í•„ë“œ ë° ìˆœì„œì¸ì§€ í™•ì¸ (SQL Injection ë°©ì§€)
        valid_sort_fields = ['block_index', 'timestamp', 'importance']
        if sort_by not in valid_sort_fields:
            sort_by = 'block_index' # ê¸°ë³¸ê°’
        if order.lower() not in ['asc', 'desc']:
            order = 'asc' # ê¸°ë³¸ê°’

        if sort_by == 'importance':
            # JOIN ì—†ì´ importanceë¡œ ì •ë ¬ëœ block_indexë¥¼ ê°€ì ¸ì˜¤ë ¤ë©´ blocks í…Œì´ë¸”ì— ì§ì ‘ ì ‘ê·¼
            query = "SELECT block_index FROM blocks"
            params_build = [] # ì„ì‹œ íŒŒë¼ë¯¸í„° ë¦¬ìŠ¤íŠ¸
            conditions = []
            if start_idx is not None:
                conditions.append("block_index >= ?")
                params_build.append(start_idx)
            if end_idx is not None:
                conditions.append("block_index <= ?")
                params_build.append(end_idx)
            
            if conditions:
                query += " WHERE " + " AND ".join(conditions)
            
            query += f" ORDER BY importance {order.upper()} LIMIT ? OFFSET ?"
            params_build.extend([limit, offset])
            params = params_build

        else:
            query = "SELECT block_index FROM blocks"
            params = [] # params ì´ˆê¸°í™” ìœ„ì¹˜ ë³€ê²½
            if start_idx is not None or end_idx is not None:
                conditions = []
                if start_idx is not None:
                    conditions.append("block_index >= ?")
                    params.append(start_idx)
                if end_idx is not None:
                    conditions.append("block_index <= ?")
                    params.append(end_idx)
                if conditions:
                    query += " WHERE " + " AND ".join(conditions)
            query += f" ORDER BY {sort_by} {order.upper()} LIMIT ? OFFSET ?"
            params.extend([limit, offset])
        
        cursor.execute(query, tuple(params))
        
        blocks = []
        block_indices = [row[0] for row in cursor.fetchall()]
        for block_index in block_indices:
            block = self.get_block(block_index)
            if block:
                blocks.append(block)
        return blocks
    
    def search_blocks_by_keyword(self, keywords: List[str], limit: int = 100) -> List[Dict[str, Any]]:
        """
        í‚¤ì›Œë“œë¡œ ë¸”ë¡ ê²€ìƒ‰
        
        Args:
            keywords: ê²€ìƒ‰í•  í‚¤ì›Œë“œ ëª©ë¡
            limit: ìµœëŒ€ ë°˜í™˜ ê°œìˆ˜
            
        Returns:
            ë§¤ì¹­ëœ ë¸”ë¡ ëª©ë¡
        """
        if not keywords:
            return []
            
        cursor = self.conn.cursor()
        
        # ê° í‚¤ì›Œë“œë§ˆë‹¤ ë¶€ë¶„ ì¼ì¹˜ ê²€ìƒ‰
        block_indices = set()
        for keyword in keywords:
            kw_lower = keyword.lower()
            
            # í‚¤ì›Œë“œ í…Œì´ë¸”ì—ì„œ ê²€ìƒ‰
            cursor.execute('''
            SELECT DISTINCT block_index FROM block_keywords 
            WHERE lower(keyword) LIKE ?
            ''', (f'%{kw_lower}%',))
            
            for row in cursor.fetchall():
                block_indices.add(row[0])
            
            # ì»¨í…ìŠ¤íŠ¸ì—ì„œë„ ê²€ìƒ‰
            cursor.execute('''
            SELECT block_index FROM blocks 
            WHERE lower(context) LIKE ?
            LIMIT ?
            ''', (f'%{kw_lower}%', limit))
            
            for row in cursor.fetchall():
                block_indices.add(row[0])
        
        # ê²°ê³¼ ë¸”ë¡ ì¡°íšŒ
        blocks = []
        for block_index in block_indices:
            block = self.get_block(block_index)
            if block:
                blocks.append(block)
                
        # ë„ˆë¬´ ë§ì€ ê²½ìš° ì œí•œ
        return blocks[:limit]
    
    def search_blocks_by_embedding(self, query_embedding: List[float], top_k: int = 5) -> List[Dict[str, Any]]:
        """
        ì„ë² ë”© ìœ ì‚¬ë„ë¡œ ë¸”ë¡ ê²€ìƒ‰
        
        Args:
            query_embedding: ì¿¼ë¦¬ ì„ë² ë”©
            top_k: ìƒìœ„ kê°œ ê²°ê³¼ ë°˜í™˜
            
        Returns:
            ìœ ì‚¬ë„ ë†’ì€ ë¸”ë¡ ëª©ë¡
        """
        cursor = self.conn.cursor()
        
        # ëª¨ë“  ì„ë² ë”© ê°€ì ¸ì˜¤ê¸°
        cursor.execute('''
        SELECT block_index, embedding, embedding_dim FROM block_embeddings
        ''')
        
        query_embedding = np.array(query_embedding, dtype=np.float32)
        blocks_with_similarity = []
        
        for row in cursor.fetchall():
            block_index = row[0]
            embedding_bytes = row[1]
            embedding_dim = row[2]
            
            # ë°”ì´ë„ˆë¦¬ì—ì„œ NumPy ë°°ì—´ë¡œ ë³€í™˜
            block_embedding = np.frombuffer(embedding_bytes, dtype=np.float32)
            if embedding_dim:
                block_embedding = block_embedding[:embedding_dim]
            
            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
            similarity = np.dot(query_embedding, block_embedding) / (
                np.linalg.norm(query_embedding) * np.linalg.norm(block_embedding)
            )
            
            blocks_with_similarity.append((block_index, similarity))
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        blocks_with_similarity.sort(key=lambda x: x[1], reverse=True)
        
        # ìƒìœ„ kê°œ ë¸”ë¡ ì¡°íšŒ
        result_blocks = []
        for block_index, similarity in blocks_with_similarity[:top_k]:
            block = self.get_block(block_index)
            if block:
                block['similarity'] = float(similarity)
                result_blocks.append(block)
        
        return result_blocks
    
    def search_blocks_by_date_range(self, start_date, end_date, limit: int = 100) -> List[Dict[str, Any]]:
        """
        ë‚ ì§œ ë²”ìœ„ë¡œ ë¸”ë¡ ê²€ìƒ‰
        
        Args:
            start_date: ì‹œì‘ ë‚ ì§œ (ISO í˜•ì‹ ë¬¸ìì—´ ë˜ëŠ” datetime ê°ì²´)
            end_date: ì¢…ë£Œ ë‚ ì§œ (ISO í˜•ì‹ ë¬¸ìì—´ ë˜ëŠ” datetime ê°ì²´)
            limit: ìµœëŒ€ ë°˜í™˜ ê°œìˆ˜
            
        Returns:
            ë‚ ì§œ ë²”ìœ„ ë‚´ ë¸”ë¡ ëª©ë¡
        """
        # datetime ê°ì²´ë¥¼ ISO ë¬¸ìì—´ë¡œ ë³€í™˜
        if hasattr(start_date, 'isoformat'):
            start_date = start_date.isoformat()
        if hasattr(end_date, 'isoformat'):
            end_date = end_date.isoformat()
            
        cursor = self.conn.cursor()
        
        cursor.execute('''
        SELECT block_index FROM blocks
        WHERE timestamp >= ? AND timestamp <= ?
        ORDER BY timestamp DESC
        LIMIT ?
        ''', (start_date, end_date, limit))
        
        blocks = []
        for row in cursor.fetchall():
            block_index = row[0]
            block = self.get_block(block_index)
            if block:
                blocks.append(block)
                
        return blocks
    
    def add_short_term_memory(self, memory_data: Dict[str, Any]) -> str:
        """
        ë‹¨ê¸° ê¸°ì–µ ì¶”ê°€
        
        Args:
            memory_data: ê¸°ì–µ ë°ì´í„° (id, timestamp, content, speaker, metadata í¬í•¨)
            
        Returns:
            ì¶”ê°€ëœ ê¸°ì–µì˜ ID
        """
        cursor = self.conn.cursor()
        
        memory_id = memory_data.get('id')
        timestamp = memory_data.get('timestamp')
        content = memory_data.get('content')
        speaker = memory_data.get('speaker')
        metadata = memory_data.get('metadata', {})
        
        cursor.execute('''
        INSERT OR REPLACE INTO short_term_memories (id, timestamp, content, speaker, metadata)
        VALUES (?, ?, ?, ?, ?)
        ''', (
            memory_id,
            timestamp,
            content,
            speaker,
            json.dumps(metadata) if metadata else '{}'
        ))
        
        self.conn.commit()
        return memory_id
    
    def get_recent_short_term_memories(self, count: int = 10) -> List[Dict[str, Any]]:
        """
        ìµœê·¼ ë‹¨ê¸° ê¸°ì–µ ì¡°íšŒ
        
        Args:
            count: ë°˜í™˜í•  ê¸°ì–µ ê°œìˆ˜
            
        Returns:
            ìµœê·¼ ë‹¨ê¸° ê¸°ì–µ ëª©ë¡
        """
        cursor = self.conn.cursor()
        
        cursor.execute('''
        SELECT id, timestamp, content, speaker, metadata
        FROM short_term_memories
        ORDER BY timestamp DESC
        LIMIT ?
        ''', (count,))
        
        memories = []
        for row in cursor.fetchall():
            if self.db_type == 'sqlite':
                memory = dict(row)
            else:
                memory = row
                
            # ë©”íƒ€ë°ì´í„° JSON íŒŒì‹±
            if 'metadata' in memory and memory['metadata']:
                memory['metadata'] = json.loads(memory['metadata'])
                
            memories.append(memory)
            
        return memories
    
    def delete_expired_short_term_memories(self, ttl_seconds: int) -> int:
        """
        ë§Œë£Œëœ ë‹¨ê¸° ê¸°ì–µ ì‚­ì œ
        
        Args:
            ttl_seconds: ìœ íš¨ ê¸°ê°„ (ì´ˆ)
            
        Returns:
            ì‚­ì œëœ ê¸°ì–µ ê°œìˆ˜
        """
        import datetime
        
        # í˜„ì¬ ì‹œê°„ì—ì„œ TTLì„ ëº€ ê°’ë³´ë‹¤ ì´ì „ íƒ€ì„ìŠ¤íƒ¬í”„ ì‚­ì œ
        cutoff_time = (datetime.datetime.now() - 
                      datetime.timedelta(seconds=ttl_seconds)).isoformat()
        
        cursor = self.conn.cursor()
        
        cursor.execute('''
        DELETE FROM short_term_memories
        WHERE timestamp < ?
        ''', (cutoff_time,))
        
        deleted_count = cursor.rowcount
        self.conn.commit()
        
        return deleted_count
    
    def clear_short_term_memories(self) -> int:
        """
        ëª¨ë“  ë‹¨ê¸° ê¸°ì–µ ì‚­ì œ
        
        Returns:
            ì‚­ì œëœ ê¸°ì–µ ê°œìˆ˜
        """
        cursor = self.conn.cursor()
        
        cursor.execute('DELETE FROM short_term_memories')
        
        deleted_count = cursor.rowcount
        self.conn.commit()
        
        return deleted_count
    
    def migrate_from_jsonl(self, block_file_path: str) -> int:
        """
        JSONL íŒŒì¼ì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ë¡œ ê¸°ì¡´ ë¸”ë¡ ë°ì´í„° ì´ì „
        
        Args:
            block_file_path: ë¸”ë¡ JSONL íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ì´ì „ëœ ë¸”ë¡ ê°œìˆ˜
        """
        import json
        
        if not os.path.exists(block_file_path):
            logger.warning(f"JSONL ë§ˆì´ê·¸ë ˆì´ì…˜ ê±´ë„ˆëœ€: íŒŒì¼ ì—†ìŒ - {block_file_path}")
            return 0
        logger.info(f"JSONL íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘: {block_file_path}")
            
        migrated_count = 0
        with open(block_file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                    
                try:
                    block_data = json.loads(line)
                    self.add_block(block_data)
                    migrated_count += 1
                except json.JSONDecodeError:
                    continue
                    
        logger.info(f"JSONL íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ: {migrated_count}ê°œ ë¸”ë¡ ì´ì „ë¨")
        return migrated_count
    
    def close(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ"""
        if self.conn:
            self.conn.close()
            logger.info(f"Database connection closed: {self.connection_string}")

    def get_short_term_memory_by_id(self, memory_id: str) -> Optional[Dict[str, Any]]:
        """
        IDë¡œ ë‹¨ê¸° ê¸°ì–µ ì¡°íšŒ

        Args:
            memory_id: ì¡°íšŒí•  ë‹¨ê¸° ê¸°ì–µì˜ ID

        Returns:
            ë‹¨ê¸° ê¸°ì–µ ë°ì´í„° (ì—†ìœ¼ë©´ None)
        """
        cursor = self.conn.cursor()
        cursor.execute("""
        SELECT id, timestamp, content, speaker, metadata 
        FROM short_term_memories 
        WHERE id = ?
        """, (memory_id,))

        row = cursor.fetchone()
        if not row:
            return None
        
        memory = dict(row)
        if 'metadata' in memory and memory['metadata']:
            try:
                memory['metadata'] = json.loads(memory['metadata'])
            except json.JSONDecodeError:
                memory['metadata'] = {} # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¹ˆ ê°ì²´
        return memory

    def get_last_block_info(self) -> Optional[Dict[str, Any]]:
        """
        ê°€ì¥ ë§ˆì§€ë§‰ìœ¼ë¡œ ì¶”ê°€ëœ ë¸”ë¡ì˜ ì¸ë±ìŠ¤ì™€ í•´ì‹œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        ë¸”ë¡ì´ ì—†ì„ ê²½ìš° Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        cursor = self.conn.cursor()
        cursor.execute("""
        SELECT block_index, hash FROM blocks 
        ORDER BY block_index DESC 
        LIMIT 1
        """)
        row = cursor.fetchone()
        if row:
            return dict(row) # {'block_index': ..., 'hash': ...}
        return None

    def filter_blocks_by_importance(self, threshold: float, limit: int = 100, 
                                   sort_by: str = 'importance', order: str = 'desc') -> List[Dict[str, Any]]:
        """
        ì¤‘ìš”ë„ ê¸°ì¤€ìœ¼ë¡œ ë¸”ë¡ í•„í„°ë§ ë° ì •ë ¬

        Args:
            threshold: ì¤‘ìš”ë„ ìµœì†Œê°’
            limit: ë°˜í™˜í•  ìµœëŒ€ ë¸”ë¡ ìˆ˜
            sort_by: ì •ë ¬ ê¸°ì¤€ í•„ë“œ
            order: ì •ë ¬ ìˆœì„œ

        Returns:
            í•„í„°ë§ ë° ì •ë ¬ëœ ë¸”ë¡ ëª©ë¡
        """
        cursor = self.conn.cursor()

        valid_sort_fields = ['block_index', 'timestamp', 'importance']
        if sort_by not in valid_sort_fields:
            sort_by = 'importance'
        if order.lower() not in ['asc', 'desc']:
            order = 'desc'

        # importance í•„ë“œë¡œ í•„í„°ë§í•˜ê³ , ì§€ì •ëœ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ block_index ëª©ë¡ì„ ê°€ì ¸ì˜´
        query = f"""
            SELECT block_index 
            FROM blocks 
            WHERE importance >= ? 
            ORDER BY {sort_by} {order.upper()} 
            LIMIT ?
        """
        params = (threshold, limit)
        
        cursor.execute(query, params)
        block_indices = [row[0] for row in cursor.fetchall()]
        
        blocks = []
        for block_index in block_indices:
            block = self.get_block(block_index) # N+1 ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ
            if block:
                blocks.append(block)
        
        return blocks
    
    def count_blocks(self) -> int:
        """
        ì „ì²´ ë¸”ë¡ ê°œìˆ˜ ì¡°íšŒ
        
        Returns:
            int: ì „ì²´ ë¸”ë¡ ê°œìˆ˜
        """
        try:
            cursor = self.conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM blocks")
            result = cursor.fetchone()
            return result[0] if result else 0
        except Exception as e:
            logger.error(f"Failed to count blocks: {e}")
            return 0
    
    def get_recent_blocks(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        ìµœê·¼ ë¸”ë¡ë“¤ ì¡°íšŒ
        
        Args:
            limit: ì¡°íšŒí•  ë¸”ë¡ ê°œìˆ˜ (ê¸°ë³¸ê°’: 10)
            
        Returns:
            List[Dict[str, Any]]: ìµœê·¼ ë¸”ë¡ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        """
        try:
            cursor = self.conn.cursor()
            cursor.execute("""
                SELECT block_index, timestamp, context, importance, hash, prev_hash
                FROM blocks 
                ORDER BY block_index DESC 
                LIMIT ?
            """, (limit,))
            
            rows = cursor.fetchall()
            blocks = []
            
            for row in rows:
                block = {
                    'block_index': row[0],
                    'timestamp': row[1],
                    'context': row[2],
                    'keywords': [],  # ìŠ¤í‚¤ë§ˆì— ì—†ìœ¼ë¯€ë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸
                    'tags': [],      # ìŠ¤í‚¤ë§ˆì— ì—†ìœ¼ë¯€ë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸
                    'embedding': [], # ìŠ¤í‚¤ë§ˆì— ì—†ìœ¼ë¯€ë¡œ ë¹ˆ ë¦¬ìŠ¤íŠ¸
                    'importance': row[3],
                    'hash': row[4],
                    'prev_hash': row[5]
                }
                blocks.append(block)
            
            return blocks
        except Exception as e:
            logger.error(f"Failed to get recent blocks: {e}")
            return []

    def health_check(self) -> bool:
        """
        ë°ì´í„°ë² ì´ìŠ¤ ìƒíƒœ ë° ë¬´ê²°ì„± ê²€ì‚¬
        
        Returns:
            bool: ë°ì´í„°ë² ì´ìŠ¤ê°€ ì •ìƒ ìƒíƒœì´ë©´ True
        """
        import time
        
        try:
            cursor = self.conn.cursor()
            
            # 1. ê¸°ë³¸ ì—°ê²° í…ŒìŠ¤íŠ¸
            cursor.execute("SELECT 1")
            
            # 2. í•„ìˆ˜ í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            required_tables = ['blocks', 'block_keywords', 'block_tags', 'block_metadata']
            for table in required_tables:
                cursor.execute("""
                    SELECT name FROM sqlite_master 
                    WHERE type='table' AND name=?
                """, (table,))
                if not cursor.fetchone():
                    logger.error(f"Required table '{table}' not found")
                    return False
            
            # 3. í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ê²€ì¦ (blocks í…Œì´ë¸”)
            cursor.execute("PRAGMA table_info(blocks)")
            columns = {row[1] for row in cursor.fetchall()}
            required_columns = {
                'block_index', 'timestamp', 'context', 
                'importance', 'hash', 'prev_hash'
            }
            if not required_columns.issubset(columns):
                logger.error("Blocks table missing required columns")
                return False
            
            # 4. ê¸°ë³¸ ë¬´ê²°ì„± í…ŒìŠ¤íŠ¸
            cursor.execute("PRAGMA integrity_check(1)")
            result = cursor.fetchone()
            if result[0] != 'ok':
                logger.error(f"Database integrity check failed: {result[0]}")
                return False
            
            # 5. ì½ê¸°/ì“°ê¸° ê¶Œí•œ í…ŒìŠ¤íŠ¸
            test_table = f"health_check_test_{int(time.time())}"
            cursor.execute(f"CREATE TEMP TABLE {test_table} (id INTEGER)")
            cursor.execute(f"INSERT INTO {test_table} VALUES (1)")
            cursor.execute(f"SELECT id FROM {test_table}")
            if cursor.fetchone()[0] != 1:
                return False
            cursor.execute(f"DROP TABLE {test_table}")
            
            self.conn.commit()
            logger.info("Database health check passed")
            return True
        
        except Exception as e:
            logger.error(f"Database health check failed: {e}")
            return False
    
    def update_block_metadata(self, block_index: int, metadata: Dict[str, Any]) -> bool:
        """
        Update metadata for a specific block (M2 Implementation).
        
        Args:
            block_index: Block index to update
            metadata: New metadata dictionary
            
        Returns:
            bool: True if update successful
        """
        try:
            cursor = self.conn.cursor()
            
            # Update blocks table metadata column if it exists
            cursor.execute("PRAGMA table_info(blocks)")
            columns = {row[1] for row in cursor.fetchall()}
            
            if 'metadata' in columns:
                # Update metadata column in blocks table
                cursor.execute('''
                UPDATE blocks SET metadata = ? WHERE block_index = ?
                ''', (json.dumps(metadata), block_index))
            
            # Update/insert into block_metadata table (using existing schema)
            cursor.execute('''
            INSERT OR REPLACE INTO block_metadata (block_index, metadata)
            VALUES (?, ?)
            ''', (block_index, json.dumps(metadata)))
            
            self.conn.commit()
            logger.debug(f"Updated metadata for block {block_index}")
            return True
            
        except Exception as e:
            logger.error(f"Failed to update metadata for block {block_index}: {e}")
            return False
    
    def get_block_by_index(self, block_index: int) -> Optional[Dict[str, Any]]:
        """
        Get block by index (alias for get_block for compatibility).
        """
        return self.get_block(block_index)
    
    def get_block_embedding(self, block_index: int) -> Optional[Dict[str, Any]]:
        """
        Get embedding data for a specific block.
        
        Args:
            block_index: Block index
            
        Returns:
            Dict with embedding data or None if not found
        """
        try:
            cursor = self.conn.cursor()
            cursor.execute('''
            SELECT embedding, embedding_model, embedding_dim 
            FROM block_embeddings 
            WHERE block_index = ?
            ''', (block_index,))
            
            row = cursor.fetchone()
            if not row:
                return None
            
            # Convert binary embedding back to numpy array
            embedding_bytes = row[0]
            embedding_array = np.frombuffer(embedding_bytes, dtype=np.float32)
            
            return {
                'embedding': embedding_array.tolist(),
                'embedding_model': row[1],
                'embedding_dim': row[2]
            }
            
        except Exception as e:
            logger.debug(f"Failed to get embedding for block {block_index}: {e}")
            return None 