"""
Raw Data Backup Layer for Greeum v2.6.3
STM Architecture Reimagining - STMì„ Raw Data Backupìœ¼ë¡œ í™•ì¥

Claude Code auto-compact ëŒ€ì‘ ì»¨í…ìŠ¤íŠ¸ ë³´ì¡´ ì‹œìŠ¤í…œ
"""

import os
import time
import json
import uuid
import sqlite3
from typing import List, Dict, Any, Optional, Set
from datetime import datetime, timedelta
from collections import defaultdict, deque
from concurrent.futures import ThreadPoolExecutor

try:
    from ..utils.console_utils import console
except ImportError:
    # Fallback for safe console output
    class SafeConsole:
        def print(self, msg, file=None):
            try:
                print(msg, file=file)
            except UnicodeEncodeError:
                # Windows fallback
                safe_msg = msg.encode('ascii', errors='replace').decode('ascii')
                print(safe_msg, file=file)
    console = SafeConsole()

from .stm_layer import STMLayer
from .context_backup import (
    ContextBackupItem, ContextType, ProcessingStatus, 
    BackupStrategy, RetentionPriority, create_context_backup
)
from .auto_compact_monitor import AutoCompactMonitor, CompactEvent
from .claude_code_detector import get_claude_code_detector
from .database_manager import DatabaseManager


class ProcessingQueue:
    """ë°±ì—… í•­ëª© ì²˜ë¦¬ ëŒ€ê¸°ì—´"""
    
    def __init__(self):
        self.priority_queue: deque = deque()  # ìš°ì„ ìˆœìœ„ í•­ëª©
        self.normal_queue: deque = deque()   # ì¼ë°˜ í•­ëª©
        
    def add_priority_item(self, backup_id: str):
        """ìš°ì„ ìˆœìœ„ í•­ëª© ì¶”ê°€"""
        self.priority_queue.append(backup_id)
    
    def add_normal_item(self, backup_id: str):
        """ì¼ë°˜ í•­ëª© ì¶”ê°€"""
        self.normal_queue.append(backup_id)
    
    def get_next(self) -> Optional[str]:
        """ë‹¤ìŒ ì²˜ë¦¬í•  í•­ëª© ë°˜í™˜"""
        if self.priority_queue:
            return self.priority_queue.popleft()
        elif self.normal_queue:
            return self.normal_queue.popleft()
        return None
    
    def is_empty(self) -> bool:
        """ëŒ€ê¸°ì—´ì´ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸"""
        return len(self.priority_queue) == 0 and len(self.normal_queue) == 0
    
    def size(self) -> Dict[str, int]:
        """ëŒ€ê¸°ì—´ í¬ê¸°"""
        return {
            'priority': len(self.priority_queue),
            'normal': len(self.normal_queue),
            'total': len(self.priority_queue) + len(self.normal_queue)
        }


class ContextRecoveryManager:
    """Context ë³µêµ¬ ê´€ë¦¬ì"""
    
    def __init__(self, backup_layer: 'RawDataBackupLayer'):
        self.backup_layer = backup_layer
    
    def recover_lost_context(self, session_id: str) -> Dict[str, Any]:
        """ì†ì‹¤ëœ ì»¨í…ìŠ¤íŠ¸ ë³µêµ¬"""
        recovery_results = {
            "recovered_items": 0,
            "reconstructed_conversations": 0,
            "ltm_transfers": 0,
            "failed_items": 0
        }
        
        try:
            # 1. LOST ìƒíƒœ ë°±ì—…ë“¤ ì‹ë³„
            lost_backups = self._find_lost_backups(session_id)
            
            if not lost_backups:
                return recovery_results
            
            # 2. ë³µêµ¬ ìš°ì„ ìˆœìœ„ ê²°ì •
            priority_queue = self._prioritize_recovery(lost_backups)
            
            # 3. Context ì¬êµ¬ì„± ë° ë³µêµ¬
            for backup_item in priority_queue:
                try:
                    # Context ì¬êµ¬ì„±
                    reconstructed = self._reconstruct_context(backup_item)
                    
                    if reconstructed:
                        # ìƒíƒœ ì—…ë°ì´íŠ¸
                        backup_item.processing_status = ProcessingStatus.RECOVERED
                        self.backup_layer._update_backup_in_db(backup_item)
                        
                        # ì¦‰ì‹œ LTM ì „ì†¡ì´ í•„ìš”í•œ ê²½ìš°
                        if self._should_immediate_transfer(backup_item):
                            success = self.backup_layer._transfer_to_ltm(backup_item)
                            if success:
                                recovery_results["ltm_transfers"] += 1
                        
                        recovery_results["recovered_items"] += 1
                        
                        # ëŒ€í™” ì¬êµ¬ì„± ì¹´ìš´íŠ¸ (conversation_turn ê¸°ì¤€)
                        if backup_item.context_type == ContextType.CONVERSATION_TURN:
                            recovery_results["reconstructed_conversations"] += 1
                    else:
                        recovery_results["failed_items"] += 1
                        
                except Exception as e:
                    print(f"ê°œë³„ ì•„ì´í…œ ë³µêµ¬ ì‹¤íŒ¨ {backup_item.id}: {e}")
                    recovery_results["failed_items"] += 1
            
            return recovery_results
            
        except Exception as e:
            print(f"Context ë³µêµ¬ ì¤‘ ì˜¤ë¥˜: {e}")
            recovery_results["failed_items"] = len(lost_backups) if 'lost_backups' in locals() else 0
            return recovery_results
    
    def _find_lost_backups(self, session_id: str) -> List[ContextBackupItem]:
        """LOST ìƒíƒœì˜ ë°±ì—…ë“¤ ì°¾ê¸°"""
        lost_backups = []
        
        for backup_id, backup_item in self.backup_layer.backup_cache.items():
            if (backup_item.session_id == session_id and 
                backup_item.processing_status == ProcessingStatus.LOST):
                lost_backups.append(backup_item)
        
        return lost_backups
    
    def _prioritize_recovery(self, lost_backups: List[ContextBackupItem]) -> List[ContextBackupItem]:
        """ë³µêµ¬ ìš°ì„ ìˆœìœ„ ê²°ì •"""
        # ìš°ì„ ìˆœìœ„ ê¸°ì¤€: 1) retention_priority, 2) auto_compact_risk_score, 3) timestamp
        return sorted(lost_backups, key=lambda x: (
            -x.retention_priority.value if hasattr(x.retention_priority, 'value') else 0,
            -x.auto_compact_risk_score,
            x.timestamp
        ))
    
    def _reconstruct_context(self, backup_item: ContextBackupItem) -> bool:
        """ê°œë³„ ì»¨í…ìŠ¤íŠ¸ ì¬êµ¬ì„±"""
        try:
            # ì´ë¯¸ semantic ì •ë³´ê°€ ìˆë‹¤ë©´ ì¬ì‚¬ìš©
            if (backup_item.extracted_intents and 
                backup_item.semantic_chunks and
                backup_item.processing_status != ProcessingStatus.FAILED):
                return True
            
            # ìƒˆë¡œ ì¶”ì¶œ ì‹œë„
            return backup_item.extract_semantic_info()
            
        except Exception as e:
            print(f"Context ì¬êµ¬ì„± ì‹¤íŒ¨ {backup_item.id}: {e}")
            return False
    
    def _should_immediate_transfer(self, backup_item: ContextBackupItem) -> bool:
        """ì¦‰ì‹œ LTM ì „ì†¡í•´ì•¼ í•˜ëŠ”ì§€ íŒë‹¨"""
        return (
            backup_item.retention_priority in [RetentionPriority.CRITICAL, RetentionPriority.HIGH] or
            backup_item.auto_compact_risk_score > 0.8 or
            backup_item.is_ready_for_ltm()
        )


class RawDataBackupLayer(STMLayer):
    """STMì„ Raw Data Backupìœ¼ë¡œ í™•ì¥í•œ ê³„ì¸µ"""
    
    def __init__(self, 
                 db_manager: DatabaseManager = None,
                 max_backup_size: int = 10000,
                 critical_retention_days: int = 30,
                 auto_compact_threshold: float = 0.8,
                 batch_processing_interval: int = 300):  # 5ë¶„
        
        # ê¸°ë³¸ STM ì´ˆê¸°í™”
        super().__init__(db_manager, default_ttl=86400)  # 24ì‹œê°„ ê¸°ë³¸ TTL
        
        # Raw Data Backup ì „ìš© ì„¤ì •
        self.max_backup_size = max_backup_size
        self.critical_retention_days = critical_retention_days
        self.batch_processing_interval = batch_processing_interval
        
        # ë°±ì—… ìºì‹œ (ContextBackupItem ì €ì¥)
        self.backup_cache: Dict[str, ContextBackupItem] = {}
        
        # ì„¸ì…˜ë³„ ì‹œí€€ìŠ¤ ë²ˆí˜¸ ê´€ë¦¬
        self.session_sequence: Dict[str, int] = defaultdict(int)
        
        # Auto-compact ëª¨ë‹ˆí„°ë§
        self.auto_compact_monitor = AutoCompactMonitor(threshold=auto_compact_threshold)
        
        # Processing Pipeline
        self.processing_queue = ProcessingQueue()
        self.recovery_manager = ContextRecoveryManager(self)
        
        # Claude Code ê°ì§€ê¸°
        self.claude_detector = get_claude_code_detector()
        
        # í†µê³„
        self.backup_stats = {
            "total_backups_created": 0,
            "auto_compact_events_detected": 0,
            "recovery_operations": 0,
            "ltm_transfers": 0,
            "processing_errors": 0
        }
        
        # ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ìš© ìŠ¤ë ˆë“œ í’€
        self.thread_pool = ThreadPoolExecutor(max_workers=2)
        
        console.print(f"[INIT] RawDataBackupLayer initialization complete")
        console.print(f"   Claude Code environment: {'[OK]' if self.claude_detector.is_claude_code_host else '[NO]'}")
        console.print(f"   Max backup size: {max_backup_size:,}")
    
    def initialize(self) -> bool:
        """í™•ì¥ ì´ˆê¸°í™”"""
        if not super().initialize():
            return False
        
        try:
            # Context Backup í…Œì´ë¸” ìƒì„±
            self._create_backup_tables()
            
            # ê¸°ì¡´ ë°±ì—… ë°ì´í„° ë¡œë“œ
            self._load_existing_backups()
            
            # Auto-compact ëª¨ë‹ˆí„°ë§ ë°ì´í„° ë¡œë“œ
            monitor_file = self._get_monitor_data_path()
            self.auto_compact_monitor.load_monitoring_data(monitor_file)
            
            return True
            
        except Exception as e:
            print(f"RawDataBackupLayer ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            return False
    
    def _create_backup_tables(self):
        """ë°±ì—… ì „ìš© í…Œì´ë¸” ìƒì„±"""
        cursor = self.db_manager.conn.cursor()
        
        # Context Backup í…Œì´ë¸”
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS context_backups (
                id TEXT PRIMARY KEY,
                session_id TEXT NOT NULL,
                timestamp TEXT NOT NULL,
                raw_content TEXT NOT NULL,
                context_type TEXT NOT NULL,
                sequence_number INTEGER NOT NULL,
                parent_context_id TEXT,
                conversation_turn INTEGER,
                original_length INTEGER,
                auto_compact_risk_score REAL,
                processing_status TEXT DEFAULT 'raw',
                backup_strategy TEXT DEFAULT 'immediate',
                retention_priority TEXT DEFAULT 'normal',
                extracted_intents TEXT, -- JSON array
                key_entities TEXT,      -- JSON array
                semantic_chunks TEXT,   -- JSON array
                recovery_metadata TEXT, -- JSON object
                claudecode_context_id TEXT,
                tool_usage_context TEXT, -- JSON array
                created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                processed_at TEXT,
                expires_at TEXT
            )
        """)
        
        # ì¸ë±ìŠ¤ ìƒì„±
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_session_sequence ON context_backups(session_id, sequence_number)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_processing_status ON context_backups(processing_status)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_retention_priority ON context_backups(retention_priority)")
        cursor.execute("CREATE INDEX IF NOT EXISTS idx_compact_risk ON context_backups(auto_compact_risk_score)")
        
        # Auto-compact ì´ë²¤íŠ¸ ë¡œê·¸ í…Œì´ë¸”
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS auto_compact_events (
                id TEXT PRIMARY KEY,
                session_id TEXT,
                detected_at TEXT,
                detected_method TEXT,
                context_length_before INTEGER,
                context_length_after INTEGER,
                affected_backup_count INTEGER,
                recovery_initiated BOOLEAN DEFAULT FALSE,
                event_data TEXT -- JSON
            )
        """)
        
        self.db_manager.conn.commit()
    
    def _load_existing_backups(self):
        """ê¸°ì¡´ ë°±ì—… ë°ì´í„° ë¡œë“œ"""
        try:
            cursor = self.db_manager.conn.cursor()
            cursor.execute("SELECT * FROM context_backups WHERE processing_status != 'processed'")
            
            for row in cursor.fetchall():
                backup_item = self._backup_from_db_row(dict(row))
                if backup_item and not backup_item.is_expired():
                    self.backup_cache[backup_item.id] = backup_item
            
            console.print(f"[CACHE] Loaded {len(self.backup_cache)} existing backups")
            
        except Exception as e:
            print(f"ê¸°ì¡´ ë°±ì—… ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    def backup_context_immediately(self, 
                                  content: str,
                                  context_type: ContextType,
                                  session_id: str = None,
                                  retention_priority: RetentionPriority = None) -> str:
        """ì‹¤ì‹œê°„ ì»¨í…ìŠ¤íŠ¸ ë°±ì—…"""
        start_time = time.time()
        
        try:
            # ì„¸ì…˜ ID ì„¤ì •
            session_id = session_id or self.claude_detector.claude_session_id or self.current_session_id
            
            # ì‹œí€€ìŠ¤ ë²ˆí˜¸ í• ë‹¹
            sequence = self.session_sequence[session_id]
            self.session_sequence[session_id] += 1
            
            # Auto-compact ìœ„í—˜ë„ ê³„ì‚°
            risk_score = self.auto_compact_monitor.calculate_risk(
                content_length=len(content),
                session_history=self.get_session_backup_count(session_id),
                tool_usage_count=self._count_tool_usage_in_session(session_id),
                conversation_turns=self._count_conversation_turns(session_id)
            )
            
            # ë³´ì¡´ ìš°ì„ ìˆœìœ„ ê²°ì •
            if retention_priority is None:
                retention_priority = self._calculate_retention_priority(content, risk_score)
            
            # ë°±ì—… ì•„ì´í…œ ìƒì„±
            backup_item = ContextBackupItem(
                session_id=session_id,
                raw_content=content,
                context_type=context_type,
                sequence_number=sequence,
                auto_compact_risk_score=risk_score,
                processing_status=ProcessingStatus.RAW,
                backup_strategy=BackupStrategy.IMMEDIATE,
                retention_priority=retention_priority
            )
            
            # ì¦‰ì‹œ ìºì‹œ ë° DB ì €ì¥
            self.backup_cache[backup_item.id] = backup_item
            self._store_backup_to_db(backup_item)
            
            # ê³ ìœ„í—˜ í•­ëª© ìš°ì„  ì²˜ë¦¬ ëŒ€ê¸°ì—´ ì¶”ê°€
            if risk_score > 0.7 or retention_priority == RetentionPriority.CRITICAL:
                self.processing_queue.add_priority_item(backup_item.id)
            else:
                self.processing_queue.add_normal_item(backup_item.id)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.backup_stats["total_backups_created"] += 1
            
            # Auto-compact ëª¨ë‹ˆí„°ë§ì— ë©”íŠ¸ë¦­ ê¸°ë¡
            self.auto_compact_monitor.record_context_metrics(
                content_length=len(content),
                session_history_count=self.get_session_backup_count(session_id),
                tool_usage_count=self._count_tool_usage_in_session(session_id),
                conversation_turns=self._count_conversation_turns(session_id)
            )
            
            # ì²˜ë¦¬ ì‹œê°„ ì²´í¬ (ì„±ëŠ¥ ëª©í‘œ: <1ms)
            processing_time = time.time() - start_time
            if processing_time > 0.001:  # 1ms ì´ˆê³¼ ì‹œ ê²½ê³ 
                print(f"âš ï¸ ë°±ì—… ì²˜ë¦¬ ì‹œê°„ {processing_time*1000:.1f}ms (ëª©í‘œ: <1ms)")
            
            return backup_item.id
            
        except Exception as e:
            print(f"ì¦‰ì‹œ ë°±ì—… ì‹¤íŒ¨: {e}")
            self.backup_stats["processing_errors"] += 1
            return ""
    
    def process_backup_queue(self) -> Dict[str, Any]:
        """ë°±ì—… í•­ëª© ë°°ì¹˜ ì²˜ë¦¬"""
        start_time = time.time()
        
        results = {
            "processed_count": 0,
            "ltm_transferred": 0,
            "failed_count": 0,
            "processing_time": 0.0
        }
        
        processed_count = 0
        max_batch_size = 100  # ë°°ì¹˜ë‹¹ ìµœëŒ€ ì²˜ë¦¬ ê°œìˆ˜
        
        try:
            while not self.processing_queue.is_empty() and processed_count < max_batch_size:
                backup_id = self.processing_queue.get_next()
                backup_item = self.backup_cache.get(backup_id)
                
                if not backup_item:
                    continue
                
                try:
                    # ë§Œë£Œëœ í•­ëª© ê±´ë„ˆë›°ê¸°
                    if backup_item.is_expired():
                        self._cleanup_expired_backup(backup_id)
                        continue
                    
                    # 1. ì •ë³´ ì¶”ì¶œ
                    backup_item.processing_status = ProcessingStatus.EXTRACTING
                    extraction_success = backup_item.extract_semantic_info()
                    
                    if extraction_success:
                        # 2. LTM ì „ì†¡ ì¤€ë¹„
                        backup_item.processing_status = ProcessingStatus.READY_FOR_LTM
                        
                        # 3. LTM ì „ì†¡ ì‹œë„
                        if self._should_transfer_to_ltm(backup_item):
                            success = self._transfer_to_ltm(backup_item)
                            if success:
                                backup_item.processing_status = ProcessingStatus.PROCESSED
                                results["ltm_transferred"] += 1
                            else:
                                results["failed_count"] += 1
                        
                        # DB ì—…ë°ì´íŠ¸
                        self._update_backup_in_db(backup_item)
                        results["processed_count"] += 1
                    else:
                        results["failed_count"] += 1
                    
                    processed_count += 1
                    
                except Exception as e:
                    print(f"ë°±ì—… ì²˜ë¦¬ ì‹¤íŒ¨ {backup_id}: {e}")
                    results["failed_count"] += 1
            
            results["processing_time"] = time.time() - start_time
            
            # ì„±ëŠ¥ ê²€ì¦ (ëª©í‘œ: >100 items/sec)
            if results["processing_time"] > 0:
                throughput = results["processed_count"] / results["processing_time"]
                if throughput < 100:
                    print(f"âš ï¸ ë°°ì¹˜ ì²˜ë¦¬ ì„±ëŠ¥ {throughput:.1f} items/sec (ëª©í‘œ: >100)")
            
            return results
            
        except Exception as e:
            print(f"ë°°ì¹˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            results["processing_time"] = time.time() - start_time
            self.backup_stats["processing_errors"] += 1
            return results
    
    def detect_and_handle_auto_compact(self) -> Optional[CompactEvent]:
        """Auto-compact ê°ì§€ ë° ì²˜ë¦¬"""
        compact_event = self.auto_compact_monitor.detect_auto_compact_event()
        
        if compact_event:
            print(f"[ALERT] Auto-compact ê°ì§€ë¨: {compact_event.event_id}")
            
            # ì˜í–¥ë°›ì€ ë°±ì—… í•­ëª©ë“¤ LOST ìƒíƒœë¡œ ë³€ê²½
            affected_count = self._mark_affected_backups_as_lost(compact_event)
            compact_event.affected_items = affected_count
            
            # ì´ë²¤íŠ¸ DB ì €ì¥
            self._store_compact_event(compact_event)
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self.backup_stats["auto_compact_events_detected"] += 1
            
            # ìë™ ë³µêµ¬ ì‹œì‘ (ë°±ê·¸ë¼ìš´ë“œ)
            if affected_count > 0:
                self.thread_pool.submit(self._auto_recovery_process, compact_event)
            
            return compact_event
        
        return None
    
    def _mark_affected_backups_as_lost(self, compact_event: CompactEvent) -> int:
        """ì˜í–¥ë°›ì€ ë°±ì—…ë“¤ì„ LOST ìƒíƒœë¡œ ë³€ê²½"""
        affected_count = 0
        
        # ì²˜ë¦¬ ì¤‘ì´ë˜ í•­ëª©ë“¤ì„ LOSTë¡œ ë³€ê²½
        for backup_id, backup_item in self.backup_cache.items():
            if backup_item.processing_status in [ProcessingStatus.RAW, ProcessingStatus.EXTRACTING]:
                backup_item.processing_status = ProcessingStatus.LOST
                backup_item.recovery_metadata['lost_event_id'] = compact_event.event_id
                backup_item.recovery_metadata['lost_timestamp'] = compact_event.timestamp.isoformat()
                self._update_backup_in_db(backup_item)
                affected_count += 1
        
        return affected_count
    
    def _auto_recovery_process(self, compact_event: CompactEvent):
        """ìë™ ë³µêµ¬ í”„ë¡œì„¸ìŠ¤ (ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰)"""
        try:
            print(f"[PROCESS] ìë™ ë³µêµ¬ í”„ë¡œì„¸ìŠ¤ ì‹œì‘: {compact_event.event_id}")
            
            # ì„¸ì…˜ë³„ ë³µêµ¬ ì‹¤í–‰
            sessions_to_recover = set()
            for backup_item in self.backup_cache.values():
                if backup_item.processing_status == ProcessingStatus.LOST:
                    sessions_to_recover.add(backup_item.session_id)
            
            total_recovered = 0
            for session_id in sessions_to_recover:
                recovery_results = self.recovery_manager.recover_lost_context(session_id)
                total_recovered += recovery_results["recovered_items"]
            
            # ì´ë²¤íŠ¸ ì—…ë°ì´íŠ¸
            compact_event.recovery_initiated = True
            
            console.print(f"[OK] Auto recovery completed: {total_recovered} items recovered")
            self.backup_stats["recovery_operations"] += 1
            
        except Exception as e:
            print(f"ìë™ ë³µêµ¬ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
    
    def get_session_backup_count(self, session_id: str) -> int:
        """ì„¸ì…˜ì˜ ë°±ì—… ê°œìˆ˜"""
        return sum(1 for item in self.backup_cache.values() 
                  if item.session_id == session_id)
    
    def _count_tool_usage_in_session(self, session_id: str) -> int:
        """ì„¸ì…˜ì˜ ë„êµ¬ ì‚¬ìš© íšŸìˆ˜"""
        return sum(len(item.tool_usage_context) for item in self.backup_cache.values()
                  if item.session_id == session_id)
    
    def _count_conversation_turns(self, session_id: str) -> int:
        """ì„¸ì…˜ì˜ ëŒ€í™” í„´ ìˆ˜"""
        turns = set()
        for item in self.backup_cache.values():
            if item.session_id == session_id:
                turns.add(item.conversation_turn)
        return len(turns)
    
    def _calculate_retention_priority(self, content: str, risk_score: float) -> RetentionPriority:
        """ë³´ì¡´ ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        # ìœ„í—˜ë„ ê¸°ë°˜
        if risk_score > 0.9:
            return RetentionPriority.CRITICAL
        elif risk_score > 0.7:
            return RetentionPriority.HIGH
        
        # ë‚´ìš© ê¸°ë°˜ ë¶„ì„
        content_lower = content.lower()
        
        # ì¤‘ìš”í•œ í‚¤ì›Œë“œ í¬í•¨ ì‹œ
        critical_keywords = ["ì˜¤ë¥˜", "error", "ì‹¤íŒ¨", "fail", "ì¤‘ìš”", "important", "ê¸´ê¸‰", "urgent"]
        if any(keyword in content_lower for keyword in critical_keywords):
            return RetentionPriority.HIGH
        
        # ì½”ë“œ ê´€ë ¨ ë‚´ìš©
        code_keywords = ["def ", "class ", "function", "import", "from", "```"]
        if any(keyword in content_lower for keyword in code_keywords):
            return RetentionPriority.NORMAL
        
        return RetentionPriority.NORMAL
    
    def _should_transfer_to_ltm(self, backup_item: ContextBackupItem) -> bool:
        """LTM ì „ì†¡ ì—¬ë¶€ íŒë‹¨"""
        return (
            backup_item.is_ready_for_ltm() and
            (backup_item.retention_priority in [RetentionPriority.CRITICAL, RetentionPriority.HIGH] or
             backup_item.auto_compact_risk_score > 0.6)
        )
    
    def _transfer_to_ltm(self, backup_item: ContextBackupItem) -> bool:
        """LTMìœ¼ë¡œ ì „ì†¡"""
        try:
            from .memory_layer import create_memory_item, MemoryLayerType
            from .hierarchical_memory import HierarchicalMemorySystem
            
            # MemoryItemìœ¼ë¡œ ë³€í™˜
            memory_item = create_memory_item(
                content=backup_item.raw_content,
                layer=MemoryLayerType.LTM,
                keywords=backup_item.key_entities,
                tags=backup_item.extracted_intents,
                importance=min(backup_item.auto_compact_risk_score + 0.3, 1.0),
                metadata={
                    'backup_source': 'raw_data_backup',
                    'original_backup_id': backup_item.id,
                    'session_id': backup_item.session_id,
                    'context_type': backup_item.context_type.value,
                    'semantic_chunks': backup_item.semantic_chunks
                }
            )
            
            # HierarchicalMemorySystemì„ í†µí•´ LTMì— ì €ì¥
            hierarchical_system = HierarchicalMemorySystem(self.db_manager)
            success = hierarchical_system.ltm_layer.add_memory(memory_item)
            
            if success:
                self.backup_stats["ltm_transfers"] += 1
                return True
            
            return False
            
        except Exception as e:
            print(f"LTM ì „ì†¡ ì‹¤íŒ¨ {backup_item.id}: {e}")
            return False
    
    def _store_backup_to_db(self, backup_item: ContextBackupItem):
        """ë°±ì—… í•­ëª©ì„ DBì— ì €ì¥"""
        cursor = self.db_manager.conn.cursor()
        
        cursor.execute("""
            INSERT OR REPLACE INTO context_backups 
            (id, session_id, timestamp, raw_content, context_type, sequence_number,
             parent_context_id, conversation_turn, original_length, auto_compact_risk_score,
             processing_status, backup_strategy, retention_priority, extracted_intents,
             key_entities, semantic_chunks, recovery_metadata, claudecode_context_id,
             tool_usage_context, created_at, processed_at, expires_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            backup_item.id,
            backup_item.session_id,
            backup_item.timestamp.isoformat(),
            backup_item.raw_content,
            backup_item.context_type.value,
            backup_item.sequence_number,
            backup_item.parent_context_id,
            backup_item.conversation_turn,
            backup_item.original_length,
            backup_item.auto_compact_risk_score,
            backup_item.processing_status.value,
            backup_item.backup_strategy.value,
            backup_item.retention_priority.value,
            json.dumps(backup_item.extracted_intents),
            json.dumps(backup_item.key_entities),
            json.dumps(backup_item.semantic_chunks),
            json.dumps(backup_item.recovery_metadata),
            backup_item.claudecode_context_id,
            json.dumps(backup_item.tool_usage_context),
            backup_item.created_at.isoformat(),
            backup_item.processed_at.isoformat() if backup_item.processed_at else None,
            backup_item.expires_at.isoformat() if backup_item.expires_at else None
        ))
        
        self.db_manager.conn.commit()
    
    def _update_backup_in_db(self, backup_item: ContextBackupItem):
        """ê¸°ì¡´ ë°±ì—… í•­ëª© ì—…ë°ì´íŠ¸"""
        self._store_backup_to_db(backup_item)  # INSERT OR REPLACE ì‚¬ìš©
    
    def _backup_from_db_row(self, row: Dict[str, Any]) -> Optional[ContextBackupItem]:
        """DB í–‰ì—ì„œ ContextBackupItem ìƒì„±"""
        try:
            return ContextBackupItem(
                id=row['id'],
                session_id=row['session_id'],
                timestamp=datetime.fromisoformat(row['timestamp']),
                raw_content=row['raw_content'],
                context_type=ContextType(row['context_type']),
                sequence_number=row['sequence_number'],
                parent_context_id=row['parent_context_id'],
                conversation_turn=row['conversation_turn'],
                original_length=row['original_length'],
                auto_compact_risk_score=row['auto_compact_risk_score'],
                processing_status=ProcessingStatus(row['processing_status']),
                backup_strategy=BackupStrategy(row['backup_strategy']),
                retention_priority=RetentionPriority(row['retention_priority']),
                extracted_intents=json.loads(row['extracted_intents'] or '[]'),
                key_entities=json.loads(row['key_entities'] or '[]'),
                semantic_chunks=json.loads(row['semantic_chunks'] or '[]'),
                recovery_metadata=json.loads(row['recovery_metadata'] or '{}'),
                claudecode_context_id=row['claudecode_context_id'],
                tool_usage_context=json.loads(row['tool_usage_context'] or '[]'),
                created_at=datetime.fromisoformat(row['created_at']),
                processed_at=datetime.fromisoformat(row['processed_at']) if row['processed_at'] else None,
                expires_at=datetime.fromisoformat(row['expires_at']) if row['expires_at'] else None
            )
        except Exception as e:
            print(f"DB í–‰ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return None
    
    def _store_compact_event(self, event: CompactEvent):
        """Compact ì´ë²¤íŠ¸ DB ì €ì¥"""
        cursor = self.db_manager.conn.cursor()
        
        cursor.execute("""
            INSERT INTO auto_compact_events
            (id, session_id, detected_at, detected_method, context_length_before,
             context_length_after, affected_backup_count, recovery_initiated, event_data)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            event.event_id,
            getattr(event, 'session_id', ''),
            event.timestamp.isoformat(),
            event.detected_method,
            event.context_length_before,
            event.context_length_after,
            event.affected_items,
            event.recovery_initiated,
            json.dumps({'event_id': event.event_id})
        ))
        
        self.db_manager.conn.commit()
    
    def _cleanup_expired_backup(self, backup_id: str):
        """ë§Œë£Œëœ ë°±ì—… ì •ë¦¬"""
        if backup_id in self.backup_cache:
            del self.backup_cache[backup_id]
        
        cursor = self.db_manager.conn.cursor()
        cursor.execute("DELETE FROM context_backups WHERE id = ?", (backup_id,))
        self.db_manager.conn.commit()
    
    def _get_monitor_data_path(self) -> str:
        """ëª¨ë‹ˆí„°ë§ ë°ì´í„° íŒŒì¼ ê²½ë¡œ"""
        data_dir = os.path.expanduser("~/.greeum")
        os.makedirs(data_dir, exist_ok=True)
        return os.path.join(data_dir, "auto_compact_monitor.json")
    
    def get_backup_statistics(self) -> Dict[str, Any]:
        """ë°±ì—… ì‹œìŠ¤í…œ í†µê³„"""
        stats = self.backup_stats.copy()
        
        # í˜„ì¬ ìƒíƒœ
        stats.update({
            "cache_size": len(self.backup_cache),
            "queue_size": self.processing_queue.size(),
            "sessions_count": len(self.session_sequence),
            "auto_compact_risk": self.auto_compact_monitor.get_current_risk_status(),
            "monitor_stats": self.auto_compact_monitor.get_statistics()
        })
        
        # ìƒíƒœë³„ ë¶„í¬
        status_counts = defaultdict(int)
        priority_counts = defaultdict(int)
        
        for backup_item in self.backup_cache.values():
            status_counts[backup_item.processing_status.value] += 1
            priority_counts[backup_item.retention_priority.value] += 1
        
        stats.update({
            "status_distribution": dict(status_counts),
            "priority_distribution": dict(priority_counts)
        })
        
        return stats
    
    def print_status_report(self):
        """ìƒíƒœ ë¦¬í¬íŠ¸ ì¶œë ¥"""
        print("=" * 70)
        print("[PROCESS] Raw Data Backup Layer Status Report")
        print("=" * 70)
        
        stats = self.get_backup_statistics()
        
        console.print(f"[CACHE] Backup Cache: {stats['cache_size']:,} items")
        print(f"â³ Processing Queue: {stats['queue_size']['total']} items "
              f"(Priority: {stats['queue_size']['priority']}, Normal: {stats['queue_size']['normal']})")
        print(f"ğŸ¯ Sessions: {stats['sessions_count']}")
        
        print(f"\nğŸ“Š Lifetime Statistics:")
        print(f"  Created: {stats['total_backups_created']:,}")
        print(f"  LTM Transfers: {stats['ltm_transfers']:,}")
        print(f"  Auto-compact Events: {stats['auto_compact_events_detected']:,}")
        print(f"  Recovery Operations: {stats['recovery_operations']:,}")
        print(f"  Errors: {stats['processing_errors']:,}")
        
        # ìœ„í—˜ë„ ìƒíƒœ
        risk_status = stats['auto_compact_risk']
        print(f"\nğŸ¯ Auto-compact Risk: {risk_status['risk_level'].upper()} ({risk_status['risk_score']:.1%})")
        
        # ìƒíƒœ ë¶„í¬
        print(f"\n[IMPROVE] Status Distribution:")
        for status, count in stats['status_distribution'].items():
            print(f"  {status}: {count}")
        
        print("=" * 70)
    
    def __del__(self):
        """ì†Œë©¸ì - ì •ë¦¬ ì‘ì—…"""
        try:
            # ëª¨ë‹ˆí„°ë§ ë°ì´í„° ì €ì¥
            monitor_file = self._get_monitor_data_path()
            self.auto_compact_monitor.save_monitoring_data(monitor_file)
            
            # ìŠ¤ë ˆë“œ í’€ ì¢…ë£Œ
            if hasattr(self, 'thread_pool'):
                self.thread_pool.shutdown(wait=False)
        except:
            pass


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª RawDataBackupLayer í…ŒìŠ¤íŠ¸")
    
    backup_layer = RawDataBackupLayer()
    backup_layer.initialize()
    
    # í…ŒìŠ¤íŠ¸ ë°±ì—… ìƒì„±
    test_contents = [
        "Claude Codeì—ì„œ STM ì„±ëŠ¥ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ì‹¶ì–´ìš”.",
        "def calculate_risk(content_length, session_history): return 0.5",
        "[ALERT] Auto-compactì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤! ì¤‘ìš”í•œ ë°ì´í„°ë¥¼ ë°±ì—…í•˜ì„¸ìš”.",
        "ì¼ë°˜ì ì¸ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤."
    ]
    
    backup_ids = []
    for i, content in enumerate(test_contents):
        context_type = [
            ContextType.USER_MESSAGE,
            ContextType.ASSISTANT_RESPONSE, 
            ContextType.SYSTEM_STATE,
            ContextType.CONVERSATION_TURN
        ][i]
        
        backup_id = backup_layer.backup_context_immediately(content, context_type)
        backup_ids.append(backup_id)
        console.print(f"[OK] Backup created: {backup_id[:8]}...")
    
    # ë°°ì¹˜ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    print("\n[PROCESS] ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰...")
    results = backup_layer.process_backup_queue()
    print(f"  ì²˜ë¦¬: {results['processed_count']}")
    print(f"  LTM ì „ì†¡: {results['ltm_transferred']}")
    print(f"  ì‹¤íŒ¨: {results['failed_count']}")
    
    # Auto-compact ì‹œë®¬ë ˆì´ì…˜
    print("\n[ALERT] Auto-compact ì‹œë®¬ë ˆì´ì…˜...")
    backup_layer.auto_compact_monitor.context_length_history.append((datetime.now() - timedelta(minutes=5), 45000))
    backup_layer.auto_compact_monitor.context_length_history.append((datetime.now(), 15000))
    
    compact_event = backup_layer.detect_and_handle_auto_compact()
    if compact_event:
        print(f"  ê°ì§€ë¨: {compact_event.event_id}")
        print(f"  ì˜í–¥ë°›ì€ í•­ëª©: {compact_event.affected_items}")
    
    # ìƒíƒœ ë¦¬í¬íŠ¸
    backup_layer.print_status_report()