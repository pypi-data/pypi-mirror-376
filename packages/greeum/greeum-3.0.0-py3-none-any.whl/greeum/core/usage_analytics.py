#!/usr/bin/env python3
"""
Usage Analytics & Monitoring System for Greeum v2.0.5
- Tracks memory usage patterns and system performance
- Provides insights for optimization and user behavior analysis
- Integrates with MCP server for real-time monitoring
"""

import json
import logging
import sqlite3
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from collections import defaultdict, Counter
from pathlib import Path
import threading

logger = logging.getLogger(__name__)

class UsageAnalytics:
    """ì‚¬ìš© íŒ¨í„´ ë¶„ì„ ë° ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self, db_manager=None, analytics_db_path: Optional[str] = None):
        """
        Analytics ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        
        Args:
            db_manager: ê¸°ì¡´ DatabaseManager (ë©”ëª¨ë¦¬ ë°ì´í„° ì ‘ê·¼ìš©)
            analytics_db_path: Analytics ì „ìš© DB ê²½ë¡œ
        """
        self.db_manager = db_manager
        # Validate and sanitize database path
        default_path = str(Path.home() / ".greeum" / "analytics.db")
        self.analytics_db_path = self._validate_db_path(analytics_db_path or default_path)
        self.lock = threading.Lock()
        
        # Analytics DB ì´ˆê¸°í™”
        self._init_analytics_db()
        
        # ë©”íŠ¸ë¦­ ì¹´í…Œê³ ë¦¬ ì •ì˜ (v2.5.1 í™•ì¥)
        self.metric_categories = {
            'memory_operations': ['add_memory', 'search_memory', 'delete_memory'],
            'stm_operations': ['stm_add', 'stm_promote', 'stm_cleanup'],
            'system_operations': ['get_memory_stats', 'ltm_analyze', 'ltm_verify', 'ltm_export'],
            'quality_metrics': ['quality_validation', 'duplicate_detection'],
            'user_behavior': ['session_start', 'session_end', 'tool_usage'],
            # v2.5.1 AI Context Slots ì „ìš© ë©”íŠ¸ë¦­
            'slots_operations': ['slots_set', 'slots_clear', 'slots_status', 'slots_search'],
            'ai_decisions': ['intent_analysis', 'slot_allocation', 'context_switching'],
            'performance_comparison': ['search_speed_comparison', 'hit_rate_tracking']
        }
    
    def _init_analytics_db(self):
        """Analytics ì „ìš© ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        try:
            Path(self.analytics_db_path).parent.mkdir(parents=True, exist_ok=True)
            
            with sqlite3.connect(self.analytics_db_path) as conn:
                # ì‚¬ìš© ì´ë²¤íŠ¸ í…Œì´ë¸”
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS usage_events (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                        event_type TEXT NOT NULL,
                        tool_name TEXT,
                        user_id TEXT,
                        session_id TEXT,
                        metadata TEXT,
                        duration_ms INTEGER,
                        success BOOLEAN,
                        error_message TEXT
                    )
                """)
                
                # í’ˆì§ˆ ë©”íŠ¸ë¦­ í…Œì´ë¸”
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS quality_metrics (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                        content_length INTEGER,
                        quality_score REAL,
                        quality_level TEXT,
                        importance REAL,
                        adjusted_importance REAL,
                        is_duplicate BOOLEAN,
                        duplicate_similarity REAL,
                        suggestions_count INTEGER
                    )
                """)
                
                # ì‹œìŠ¤í…œ ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…Œì´ë¸”
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS performance_metrics (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                        metric_type TEXT NOT NULL,
                        metric_name TEXT NOT NULL,
                        metric_value REAL,
                        unit TEXT,
                        metadata TEXT
                    )
                """)
                
                # v2.5.1 AI Context Slots ë©”íŠ¸ë¦­ í…Œì´ë¸”
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS slots_metrics (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                        operation TEXT NOT NULL,
                        slot_type TEXT,
                        content_length INTEGER,
                        ai_intent TEXT,
                        ai_confidence REAL,
                        slot_allocation TEXT,
                        ltm_anchor_block INTEGER,
                        search_radius INTEGER,
                        response_time_ms REAL,
                        success BOOLEAN,
                        error_message TEXT
                    )
                """)
                
                # v2.5.1 ê²€ìƒ‰ ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸”
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS search_comparison (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                        query_text TEXT NOT NULL,
                        query_length INTEGER,
                        slots_enabled BOOLEAN,
                        response_time_ms REAL,
                        results_count INTEGER,
                        slot_hits INTEGER,
                        ltm_hits INTEGER,
                        top_result_source TEXT,
                        user_clicked_rank INTEGER,
                        session_id TEXT
                    )
                """)
                
                # AI ì˜ë„ ë¶„ì„ ì •í™•ë„ í…Œì´ë¸”
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS ai_intent_tracking (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                        input_content TEXT NOT NULL,
                        predicted_intent TEXT,
                        predicted_slot TEXT,
                        actual_slot_used TEXT,
                        user_feedback INTEGER,  -- -1: í‹€ë¦¼, 0: ëª¨ë¦„, 1: ë§ìŒ
                        keywords_detected TEXT,
                        importance_score REAL,
                        context_metadata TEXT
                    )
                """)

                # ì‚¬ìš©ì ì„¸ì…˜ í…Œì´ë¸”
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS user_sessions (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        session_id TEXT UNIQUE NOT NULL,
                        start_time DATETIME DEFAULT CURRENT_TIMESTAMP,
                        end_time DATETIME,
                        total_operations INTEGER DEFAULT 0,
                        memory_added INTEGER DEFAULT 0,
                        searches_performed INTEGER DEFAULT 0,
                        avg_quality_score REAL,
                        user_agent TEXT,
                        client_type TEXT
                    )
                """)
                
                conn.commit()
                logger.info(f"Analytics database initialized: {self.analytics_db_path}")
                
        except Exception as e:
            logger.error(f"Failed to initialize analytics database: {e}")
    
    def _sanitize_metadata(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """Sanitize metadata to prevent injection attacks"""
        if not metadata:
            return {}
        
        sanitized = {}
        for key, value in metadata.items():
            # Limit key/value lengths and types
            if isinstance(key, str) and len(key) <= 100:
                if isinstance(value, (str, int, float, bool)):
                    # Truncate string values to prevent DoS
                    if isinstance(value, str):
                        value = value[:1000]
                    sanitized[str(key)[:100]] = value
        return sanitized
    
    def _validate_db_path(self, path: str) -> str:
        """Validate and sanitize database path"""
        resolved_path = Path(path).resolve()
        
        # Allow safe directories including temp directories for testing
        allowed_dirs = [
            Path.home() / ".greeum",
            Path("/tmp"),
            Path("/var/tmp"),
            Path.cwd(),  # Allow current working directory
        ]
        
        # For testing, allow any path that's not obviously dangerous
        if not str(resolved_path).startswith(('/etc', '/bin', '/usr', '/System')):
            return str(resolved_path)
            
        # Fallback to allowed directories check
        if any(str(resolved_path).startswith(str(allowed_dir)) for allowed_dir in allowed_dirs):
            return str(resolved_path)
            
        raise ValueError(f"Database path not allowed: {resolved_path}")

    def log_event(self, event_type: str, tool_name: str = None, metadata: Dict[str, Any] = None,
                  duration_ms: int = None, success: bool = True, error_message: str = None,
                  user_id: str = "anonymous", session_id: str = None) -> bool:
        """
        ì‚¬ìš© ì´ë²¤íŠ¸ ë¡œê¹…
        
        Args:
            event_type: ì´ë²¤íŠ¸ íƒ€ì… (tool_usage, session_start, error ë“±)
            tool_name: ì‚¬ìš©ëœ ë„êµ¬ ì´ë¦„
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            duration_ms: ì‹¤í–‰ ì‹œê°„ (ë°€ë¦¬ì´ˆ)
            success: ì„±ê³µ ì—¬ë¶€
            error_message: ì˜¤ë¥˜ ë©”ì‹œì§€ (ì‹¤íŒ¨ì‹œ)
            user_id: ì‚¬ìš©ì ID
            session_id: ì„¸ì…˜ ID
        """
        try:
            # Sanitize inputs
            event_type = str(event_type)[:50] if event_type else "unknown"
            tool_name = str(tool_name)[:50] if tool_name else None
            user_id = str(user_id)[:50] if user_id else "anonymous"
            session_id = str(session_id)[:100] if session_id else None
            error_message = str(error_message)[:500] if error_message else None
            metadata = self._sanitize_metadata(metadata) if metadata else None
            
            # Validate duration
            if duration_ms is not None and (duration_ms < 0 or duration_ms > 3600000):  # Max 1 hour
                duration_ms = None
            
            with self.lock:
                with sqlite3.connect(self.analytics_db_path) as conn:
                    conn.execute("""
                        INSERT INTO usage_events 
                        (event_type, tool_name, user_id, session_id, metadata, 
                         duration_ms, success, error_message)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        event_type, tool_name, user_id, session_id,
                        json.dumps(metadata) if metadata else None,
                        duration_ms, success, error_message
                    ))
                    conn.commit()
            return True
        except Exception as e:
            logger.error(f"Failed to log event: {e}")
            return False
    
    def log_quality_metrics(self, content_length: int, quality_score: float, 
                          quality_level: str, importance: float, 
                          adjusted_importance: float, is_duplicate: bool = False,
                          duplicate_similarity: float = 0.0, suggestions_count: int = 0) -> bool:
        """í’ˆì§ˆ ë©”íŠ¸ë¦­ ë¡œê¹…"""
        try:
            with self.lock:
                with sqlite3.connect(self.analytics_db_path) as conn:
                    conn.execute("""
                        INSERT INTO quality_metrics 
                        (content_length, quality_score, quality_level, importance, 
                         adjusted_importance, is_duplicate, duplicate_similarity, suggestions_count)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        content_length, quality_score, quality_level, importance,
                        adjusted_importance, is_duplicate, duplicate_similarity, suggestions_count
                    ))
                    conn.commit()
            return True
        except Exception as e:
            logger.error(f"Failed to log quality metrics: {e}")
            return False
    
    def log_performance_metric(self, metric_type: str, metric_name: str, 
                             metric_value: float, unit: str = None, 
                             metadata: Dict[str, Any] = None) -> bool:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¡œê¹…"""
        try:
            with self.lock:
                with sqlite3.connect(self.analytics_db_path) as conn:
                    conn.execute("""
                        INSERT INTO performance_metrics 
                        (metric_type, metric_name, metric_value, unit, metadata)
                        VALUES (?, ?, ?, ?, ?)
                    """, (
                        metric_type, metric_name, metric_value, unit,
                        json.dumps(metadata) if metadata else None
                    ))
                    conn.commit()
            return True
        except Exception as e:
            logger.error(f"Failed to log performance metric: {e}")
            return False
    
    def start_session(self, session_id: str, user_agent: str = None, 
                     client_type: str = "mcp_client") -> bool:
        """ì‚¬ìš©ì ì„¸ì…˜ ì‹œì‘"""
        try:
            with self.lock:
                with sqlite3.connect(self.analytics_db_path) as conn:
                    conn.execute("""
                        INSERT OR REPLACE INTO user_sessions 
                        (session_id, start_time, user_agent, client_type)
                        VALUES (?, ?, ?, ?)
                    """, (session_id, datetime.now().isoformat(), user_agent, client_type))
                    conn.commit()
                    
            # ì„¸ì…˜ ì‹œì‘ ì´ë²¤íŠ¸ ë¡œê¹…
            self.log_event("session_start", session_id=session_id, 
                          metadata={"user_agent": user_agent, "client_type": client_type})
            return True
        except Exception as e:
            logger.error(f"Failed to start session: {e}")
            return False
    
    def end_session(self, session_id: str) -> bool:
        """ì‚¬ìš©ì ì„¸ì…˜ ì¢…ë£Œ"""
        try:
            with self.lock:
                with sqlite3.connect(self.analytics_db_path) as conn:
                    # ì„¸ì…˜ í†µê³„ ê³„ì‚°
                    cursor = conn.execute("""
                        SELECT COUNT(*) as total_ops,
                               SUM(CASE WHEN tool_name = 'add_memory' THEN 1 ELSE 0 END) as memory_added,
                               SUM(CASE WHEN tool_name = 'search_memory' THEN 1 ELSE 0 END) as searches
                        FROM usage_events 
                        WHERE session_id = ? AND event_type = 'tool_usage'
                    """, (session_id,))
                    
                    stats = cursor.fetchone()
                    total_ops, memory_added, searches = stats if stats else (0, 0, 0)
                    
                    # í‰ê·  í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                    cursor = conn.execute("""
                        SELECT AVG(quality_score) 
                        FROM quality_metrics 
                        WHERE timestamp >= (
                            SELECT start_time FROM user_sessions WHERE session_id = ?
                        )
                    """, (session_id,))
                    
                    avg_quality = cursor.fetchone()[0] or 0.0
                    
                    # ì„¸ì…˜ ì¢…ë£Œ ì—…ë°ì´íŠ¸
                    conn.execute("""
                        UPDATE user_sessions 
                        SET end_time = ?, total_operations = ?, memory_added = ?, 
                            searches_performed = ?, avg_quality_score = ?
                        WHERE session_id = ?
                    """, (
                        datetime.now().isoformat(), total_ops, memory_added, 
                        searches, avg_quality, session_id
                    ))
                    conn.commit()
                    
            # ì„¸ì…˜ ì¢…ë£Œ ì´ë²¤íŠ¸ ë¡œê¹…
            self.log_event("session_end", session_id=session_id,
                          metadata={"total_operations": total_ops, "avg_quality": avg_quality})
            return True
        except Exception as e:
            logger.error(f"Failed to end session: {e}")
            return False
    
    def get_usage_statistics(self, days: int = 7) -> Dict[str, Any]:
        """ì‚¬ìš© í†µê³„ ìƒì„±"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days)
            
            with sqlite3.connect(self.analytics_db_path) as conn:
                # ê¸°ë³¸ ì‚¬ìš© í†µê³„
                cursor = conn.execute("""
                    SELECT 
                        COUNT(*) as total_events,
                        COUNT(DISTINCT session_id) as unique_sessions,
                        COUNT(CASE WHEN success = 1 THEN 1 END) as successful_events,
                        AVG(duration_ms) as avg_duration_ms
                    FROM usage_events 
                    WHERE timestamp >= ?
                """, (cutoff_date.isoformat(),))
                
                basic_stats = cursor.fetchone()
                
                # ë„êµ¬ë³„ ì‚¬ìš© ë¹ˆë„
                cursor = conn.execute("""
                    SELECT tool_name, COUNT(*) as usage_count
                    FROM usage_events 
                    WHERE timestamp >= ? AND tool_name IS NOT NULL
                    GROUP BY tool_name
                    ORDER BY usage_count DESC
                """, (cutoff_date.isoformat(),))
                
                tool_usage = dict(cursor.fetchall())
                
                # í’ˆì§ˆ ë©”íŠ¸ë¦­ í†µê³„
                cursor = conn.execute("""
                    SELECT 
                        AVG(quality_score) as avg_quality,
                        AVG(content_length) as avg_content_length,
                        COUNT(CASE WHEN is_duplicate = 1 THEN 1 END) as duplicate_count,
                        COUNT(*) as total_quality_checks
                    FROM quality_metrics 
                    WHERE timestamp >= ?
                """, (cutoff_date.isoformat(),))
                
                quality_stats = cursor.fetchone()
                
                # ì‹œê°„ëŒ€ë³„ ì‚¬ìš© íŒ¨í„´
                cursor = conn.execute("""
                    SELECT 
                        strftime('%H', timestamp) as hour,
                        COUNT(*) as event_count
                    FROM usage_events 
                    WHERE timestamp >= ?
                    GROUP BY strftime('%H', timestamp)
                    ORDER BY hour
                """, (cutoff_date.isoformat(),))
                
                hourly_usage = dict(cursor.fetchall())
                
                return {
                    "period_days": days,
                    "basic_stats": {
                        "total_events": basic_stats[0] if basic_stats else 0,
                        "unique_sessions": basic_stats[1] if basic_stats else 0,
                        "successful_events": basic_stats[2] if basic_stats else 0,
                        "success_rate": (basic_stats[2] / basic_stats[0]) if basic_stats and basic_stats[0] > 0 else 0.0,
                        "avg_duration_ms": basic_stats[3] if basic_stats else 0.0
                    },
                    "tool_usage": tool_usage,
                    "quality_stats": {
                        "avg_quality_score": quality_stats[0] if quality_stats else 0.0,
                        "avg_content_length": quality_stats[1] if quality_stats else 0.0,
                        "duplicate_rate": (quality_stats[2] / quality_stats[3]) if quality_stats and quality_stats[3] > 0 else 0.0,
                        "total_quality_checks": quality_stats[3] if quality_stats else 0
                    },
                    "hourly_usage": hourly_usage,
                    "generated_at": datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"Failed to generate usage statistics: {e}")
            return {"error": str(e)}
    
    def get_quality_trends(self, days: int = 30) -> Dict[str, Any]:
        """í’ˆì§ˆ íŠ¸ë Œë“œ ë¶„ì„"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days)
            
            with sqlite3.connect(self.analytics_db_path) as conn:
                # ì¼ë³„ í’ˆì§ˆ íŠ¸ë Œë“œ
                cursor = conn.execute("""
                    SELECT 
                        DATE(timestamp) as date,
                        AVG(quality_score) as avg_quality,
                        COUNT(*) as count,
                        AVG(content_length) as avg_length
                    FROM quality_metrics 
                    WHERE timestamp >= ?
                    GROUP BY DATE(timestamp)
                    ORDER BY date
                """, (cutoff_date.isoformat(),))
                
                daily_trends = [
                    {
                        "date": row[0],
                        "avg_quality": row[1],
                        "count": row[2],
                        "avg_length": row[3]
                    }
                    for row in cursor.fetchall()
                ]
                
                # í’ˆì§ˆ ë“±ê¸‰ë³„ ë¶„í¬
                cursor = conn.execute("""
                    SELECT quality_level, COUNT(*) as count
                    FROM quality_metrics 
                    WHERE timestamp >= ?
                    GROUP BY quality_level
                """, (cutoff_date.isoformat(),))
                
                quality_distribution = dict(cursor.fetchall())
                
                # ì¤‘ë³µ íŒ¨í„´ ë¶„ì„
                cursor = conn.execute("""
                    SELECT 
                        DATE(timestamp) as date,
                        COUNT(CASE WHEN is_duplicate = 1 THEN 1 END) as duplicates,
                        COUNT(*) as total,
                        AVG(duplicate_similarity) as avg_similarity
                    FROM quality_metrics 
                    WHERE timestamp >= ?
                    GROUP BY DATE(timestamp)
                    ORDER BY date
                """, (cutoff_date.isoformat(),))
                
                duplicate_trends = [
                    {
                        "date": row[0],
                        "duplicate_count": row[1],
                        "total_count": row[2],
                        "duplicate_rate": (row[1] / row[2]) if row[2] > 0 else 0.0,
                        "avg_similarity": row[3] or 0.0
                    }
                    for row in cursor.fetchall()
                ]
                
                return {
                    "period_days": days,
                    "daily_trends": daily_trends,
                    "quality_distribution": quality_distribution,
                    "duplicate_trends": duplicate_trends,
                    "generated_at": datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"Failed to analyze quality trends: {e}")
            return {"error": str(e)}
    
    def get_performance_insights(self, days: int = 7) -> Dict[str, Any]:
        """ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days)
            
            with sqlite3.connect(self.analytics_db_path) as conn:
                # ëŠë¦° ì‘ì—… ì‹ë³„
                cursor = conn.execute("""
                    SELECT 
                        tool_name,
                        AVG(duration_ms) as avg_duration,
                        MAX(duration_ms) as max_duration,
                        COUNT(*) as operation_count
                    FROM usage_events 
                    WHERE timestamp >= ? AND duration_ms IS NOT NULL AND tool_name IS NOT NULL
                    GROUP BY tool_name
                    ORDER BY avg_duration DESC
                """, (cutoff_date.isoformat(),))
                
                performance_by_tool = [
                    {
                        "tool_name": row[0],
                        "avg_duration_ms": row[1],
                        "max_duration_ms": row[2],
                        "operation_count": row[3]
                    }
                    for row in cursor.fetchall()
                ]
                
                # ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„
                cursor = conn.execute("""
                    SELECT 
                        tool_name,
                        error_message,
                        COUNT(*) as error_count
                    FROM usage_events 
                    WHERE timestamp >= ? AND success = 0 AND tool_name IS NOT NULL
                    GROUP BY tool_name, error_message
                    ORDER BY error_count DESC
                    LIMIT 10
                """, (cutoff_date.isoformat(),))
                
                error_patterns = [
                    {
                        "tool_name": row[0],
                        "error_message": row[1],
                        "error_count": row[2]
                    }
                    for row in cursor.fetchall()
                ]
                
                # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­
                cursor = conn.execute("""
                    SELECT 
                        metric_name,
                        AVG(metric_value) as avg_value,
                        MAX(metric_value) as max_value,
                        unit
                    FROM performance_metrics 
                    WHERE timestamp >= ?
                    GROUP BY metric_name, unit
                """, (cutoff_date.isoformat(),))
                
                resource_metrics = [
                    {
                        "metric_name": row[0],
                        "avg_value": row[1],
                        "max_value": row[2],
                        "unit": row[3]
                    }
                    for row in cursor.fetchall()
                ]
                
                # ê¶Œì¥ì‚¬í•­ ìƒì„±
                recommendations = self._generate_performance_recommendations(
                    performance_by_tool, error_patterns
                )
                
                return {
                    "period_days": days,
                    "performance_by_tool": performance_by_tool,
                    "error_patterns": error_patterns,
                    "resource_metrics": resource_metrics,
                    "recommendations": recommendations,
                    "generated_at": datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"Failed to generate performance insights: {e}")
            return {"error": str(e)}
    
    def _generate_performance_recommendations(self, performance_data: List[Dict], 
                                           error_data: List[Dict]) -> List[str]:
        """ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ëŠë¦° ì‘ì—… ê¶Œì¥ì‚¬í•­
        if performance_data:
            slowest_tool = performance_data[0]
            if slowest_tool["avg_duration_ms"] > 1000:  # 1ì´ˆ ì´ìƒ
                recommendations.append(
                    f"ğŸŒ '{slowest_tool['tool_name']}' operation is slow "
                    f"(avg: {slowest_tool['avg_duration_ms']:.0f}ms). Consider optimization."
                )
        
        # ì˜¤ë¥˜ íŒ¨í„´ ê¶Œì¥ì‚¬í•­
        if error_data:
            top_error = error_data[0]
            if top_error["error_count"] > 5:
                recommendations.append(
                    f"ğŸš« Frequent errors in '{top_error['tool_name']}': "
                    f"{top_error['error_message']} ({top_error['error_count']} times)"
                )
        
        # ì¼ë°˜ ê¶Œì¥ì‚¬í•­
        if not recommendations:
            recommendations.append("âœ… System performance looks healthy!")
        
        return recommendations
    
    def cleanup_old_data(self, days_to_keep: int = 90) -> Dict[str, int]:
        """ì˜¤ë˜ëœ ë¶„ì„ ë°ì´í„° ì •ë¦¬"""
        try:
            cutoff_date = datetime.now() - timedelta(days=days_to_keep)
            
            with self.lock:
                with sqlite3.connect(self.analytics_db_path) as conn:
                    # ê° í…Œì´ë¸”ì—ì„œ ì˜¤ë˜ëœ ë°ì´í„° ì‚­ì œ
                    tables = ['usage_events', 'quality_metrics', 'performance_metrics']
                    deleted_counts = {}
                    
                    for table in tables:
                        cursor = conn.execute(f"""
                            DELETE FROM {table} 
                            WHERE timestamp < ?
                        """, (cutoff_date.isoformat(),))
                        deleted_counts[table] = cursor.rowcount
                    
                    # ì™„ë£Œëœ ì„¸ì…˜ ì¤‘ ì˜¤ë˜ëœ ê²ƒë“¤ ì‚­ì œ
                    cursor = conn.execute("""
                        DELETE FROM user_sessions 
                        WHERE end_time IS NOT NULL AND end_time < ?
                    """, (cutoff_date.isoformat(),))
                    deleted_counts['user_sessions'] = cursor.rowcount
                    
                    conn.commit()
                    
            logger.info(f"Cleaned up analytics data older than {days_to_keep} days: {deleted_counts}")
            return deleted_counts
            
        except Exception as e:
            logger.error(f"Failed to cleanup old analytics data: {e}")
            return {"error": str(e)}
    
    def _get_hourly_activity(self, days: int = 7) -> Dict[int, int]:
        """ì‹œê°„ëŒ€ë³„ í™œë™ íŒ¨í„´ ë¶„ì„"""
        with self.lock:
            conn = sqlite3.connect(self.analytics_db_path)
            cursor = conn.cursor()
            
            try:
                since_date = datetime.now() - timedelta(days=days)
                cursor.execute("""
                    SELECT 
                        CAST(strftime('%H', timestamp) AS INTEGER) as hour,
                        COUNT(*) as count
                    FROM usage_events 
                    WHERE timestamp >= ?
                    GROUP BY hour
                    ORDER BY hour
                """, [since_date.isoformat()])
                
                return dict(cursor.fetchall())
                
            finally:
                conn.close()
    
    def _get_error_analysis(self, days: int = 7) -> Dict[str, Any]:
        """ì˜¤ë¥˜ íŒ¨í„´ ë¶„ì„"""
        with self.lock:
            conn = sqlite3.connect(self.analytics_db_path)
            cursor = conn.cursor()
            
            try:
                since_date = datetime.now() - timedelta(days=days)
                
                # ì´ ì˜¤ë¥˜ ìˆ˜
                cursor.execute("""
                    SELECT COUNT(*) FROM usage_events 
                    WHERE timestamp >= ? AND success = 0
                """, [since_date.isoformat()])
                
                total_errors = cursor.fetchone()[0] or 0
                
                # ì£¼ìš” ì˜¤ë¥˜ ìœ í˜•
                cursor.execute("""
                    SELECT error_message, COUNT(*) as count
                    FROM usage_events 
                    WHERE timestamp >= ? AND success = 0 AND error_message IS NOT NULL
                    GROUP BY error_message
                    ORDER BY count DESC
                    LIMIT 5
                """, [since_date.isoformat()])
                
                common_errors = cursor.fetchall()
                
                return {
                    'total_errors': total_errors,
                    'common_errors': common_errors
                }
                
            finally:
                conn.close()
    
    def _analyze_stm_promotion_candidates(self) -> int:
        """STM ìŠ¹ê²© í›„ë³´ ë¶„ì„ (ëª¨ì˜ êµ¬í˜„)"""
        # ì‹¤ì œë¡œëŠ” STMManagerì™€ ì—°ë™í•˜ì—¬ ë¶„ì„
        # í˜„ì¬ëŠ” ì„ì˜ì˜ ê°’ ë°˜í™˜
        import random
        return random.randint(0, 5)
    
    def get_usage_report(self, days: int = 7, report_type: str = "usage") -> Dict[str, Any]:
        """
        í†µí•© ì‚¬ìš© ë¦¬í¬íŠ¸ ìƒì„± (MCP ì„œë²„ í˜¸í™˜ì„±ìš©)
        
        Args:
            days: ë¶„ì„ ê¸°ê°„ (ì¼)
            report_type: ë¦¬í¬íŠ¸ ìœ í˜• ("usage", "quality", "performance", "all")
            
        Returns:
            ë¦¬í¬íŠ¸ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        """
        try:
            if report_type == "usage":
                return self.get_usage_statistics(days=days)
            elif report_type == "quality":
                return self.get_quality_trends(days=days)
            elif report_type == "performance":
                return self.get_performance_insights(days=days)
            elif report_type == "all":
                # ëª¨ë“  ë¦¬í¬íŠ¸ í†µí•©
                usage_stats = self.get_usage_statistics(days=days)
                quality_trends = self.get_quality_trends(days=days)
                performance_insights = self.get_performance_insights(days=days)
                
                return {
                    "report_type": "comprehensive",
                    "period_days": days,
                    "usage_statistics": usage_stats,
                    "quality_trends": quality_trends,
                    "performance_insights": performance_insights,
                    "generated_at": datetime.now().isoformat()
                }
            else:
                # ê¸°ë³¸ì ìœ¼ë¡œ usage ë¦¬í¬íŠ¸ ë°˜í™˜
                return self.get_usage_statistics(days=days)
                
        except Exception as e:
            logger.error(f"get_usage_report failed: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë°ì´í„° ë°˜í™˜
            return {
                "total_operations": 0,
                "add_operations": 0,
                "search_operations": 0,
                "avg_quality_score": 0,
                "high_quality_rate": 0,
                "avg_response_time": 0,
                "success_rate": 0,
                "error": str(e)
            }
    
    # ===== v2.5.1 AI Context Slots ì „ìš© ë©”íŠ¸ë¦­ ë©”ì„œë“œë“¤ =====
    
    def track_slots_operation(self, operation: str, slot_type: Optional[str] = None, 
                             content: Optional[str] = None, ai_intent: Optional[str] = None,
                             ai_confidence: Optional[float] = None, slot_allocation: Optional[str] = None,
                             ltm_anchor_block: Optional[int] = None, search_radius: Optional[int] = None,
                             response_time_ms: Optional[float] = None, success: bool = True, 
                             error_message: Optional[str] = None):
        """
        v2.5.1 AI Context Slots ì‘ì—… ì¶”ì 
        
        Args:
            operation: ì‘ì—… ìœ í˜• ('set', 'clear', 'status', 'search')
            slot_type: í• ë‹¹ëœ ìŠ¬ë¡¯ íƒ€ì… ('active', 'anchor', 'buffer')
            content: ì…ë ¥ ë‚´ìš©
            ai_intent: AIê°€ ë¶„ì„í•œ ì˜ë„
            ai_confidence: AI ë¶„ì„ ì‹ ë¢°ë„ (0.0-1.0)
            slot_allocation: ìµœì¢… í• ë‹¹ëœ ìŠ¬ë¡¯
            ltm_anchor_block: LTM ì•µì»¤ ë¸”ë¡ ID
            search_radius: ê²€ìƒ‰ ë°˜ê²½
            response_time_ms: ì‘ë‹µ ì‹œê°„ (ë°€ë¦¬ì´ˆ)
            success: ì„±ê³µ ì—¬ë¶€
            error_message: ì—ëŸ¬ ë©”ì‹œì§€
        """
        try:
            with sqlite3.connect(self.analytics_db_path) as conn:
                conn.execute("""
                    INSERT INTO slots_metrics 
                    (operation, slot_type, content_length, ai_intent, ai_confidence, 
                     slot_allocation, ltm_anchor_block, search_radius, response_time_ms, 
                     success, error_message)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    operation, slot_type, len(content) if content else 0, ai_intent, ai_confidence,
                    slot_allocation, ltm_anchor_block, search_radius, response_time_ms,
                    success, error_message
                ))
                conn.commit()
        except Exception as e:
            logger.error(f"Failed to track slots operation: {e}")
    
    def track_search_comparison(self, query_text: str, slots_enabled: bool, 
                              response_time_ms: float, results_count: int,
                              slot_hits: int = 0, ltm_hits: int = 0, 
                              top_result_source: Optional[str] = None,
                              user_clicked_rank: Optional[int] = None,
                              session_id: Optional[str] = None):
        """
        v2.5.1 ê²€ìƒ‰ ì„±ëŠ¥ ë¹„êµ ì¶”ì  (ìŠ¬ë¡¯ vs ê¸°ì¡´ ê²€ìƒ‰)
        
        Args:
            query_text: ê²€ìƒ‰ ì¿¼ë¦¬
            slots_enabled: ìŠ¬ë¡¯ í†µí•© ê²€ìƒ‰ í™œì„±í™” ì—¬ë¶€
            response_time_ms: ì‘ë‹µ ì‹œê°„
            results_count: ê²°ê³¼ ìˆ˜
            slot_hits: ìŠ¬ë¡¯ì—ì„œ ì°¾ì€ ê²°ê³¼ ìˆ˜
            ltm_hits: LTMì—ì„œ ì°¾ì€ ê²°ê³¼ ìˆ˜
            top_result_source: ìµœìƒìœ„ ê²°ê³¼ ì¶œì²˜ ('working_memory' or 'long_term_memory')
            user_clicked_rank: ì‚¬ìš©ìê°€ í´ë¦­í•œ ê²°ê³¼ ìˆœìœ„ (í”¼ë“œë°±ìš©)
            session_id: ì„¸ì…˜ ID
        """
        try:
            with sqlite3.connect(self.analytics_db_path) as conn:
                conn.execute("""
                    INSERT INTO search_comparison 
                    (query_text, query_length, slots_enabled, response_time_ms, 
                     results_count, slot_hits, ltm_hits, top_result_source, 
                     user_clicked_rank, session_id)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    query_text, len(query_text), slots_enabled, response_time_ms,
                    results_count, slot_hits, ltm_hits, top_result_source,
                    user_clicked_rank, session_id
                ))
                conn.commit()
        except Exception as e:
            logger.error(f"Failed to track search comparison: {e}")
    
    def track_ai_intent(self, input_content: str, predicted_intent: str, 
                       predicted_slot: str, actual_slot_used: str,
                       user_feedback: Optional[int] = 0, keywords_detected: Optional[str] = None,
                       importance_score: Optional[float] = None, context_metadata: Optional[dict] = None):
        """
        AI ì˜ë„ ë¶„ì„ ì •í™•ë„ ì¶”ì 
        
        Args:
            input_content: ì‚¬ìš©ì ì…ë ¥
            predicted_intent: AIê°€ ì˜ˆì¸¡í•œ ì˜ë„
            predicted_slot: AIê°€ ì˜ˆì¸¡í•œ ìŠ¬ë¡¯
            actual_slot_used: ì‹¤ì œ ì‚¬ìš©ëœ ìŠ¬ë¡¯
            user_feedback: ì‚¬ìš©ì í”¼ë“œë°± (-1: í‹€ë¦¼, 0: ëª¨ë¦„, 1: ë§ìŒ)
            keywords_detected: ê°ì§€ëœ í‚¤ì›Œë“œë“¤
            importance_score: ì¤‘ìš”ë„ ì ìˆ˜
            context_metadata: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„°
        """
        try:
            with sqlite3.connect(self.analytics_db_path) as conn:
                conn.execute("""
                    INSERT INTO ai_intent_tracking 
                    (input_content, predicted_intent, predicted_slot, actual_slot_used,
                     user_feedback, keywords_detected, importance_score, context_metadata)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    input_content, predicted_intent, predicted_slot, actual_slot_used,
                    user_feedback, keywords_detected, importance_score, 
                    json.dumps(context_metadata) if context_metadata else None
                ))
                conn.commit()
        except Exception as e:
            logger.error(f"Failed to track AI intent: {e}")
    
    def get_slots_performance_report(self, days: int = 7) -> Dict[str, Any]:
        """
        v2.5.1 AI Context Slots ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
        
        Returns:
            ì‹¤ì œ ì¸¡ì •ëœ KPIë“¤ì„ í¬í•¨í•œ ë¦¬í¬íŠ¸
        """
        try:
            cutoff_date = datetime.now() - timedelta(days=days)
            
            with sqlite3.connect(self.analytics_db_path) as conn:
                # ìŠ¬ë¡¯ ì‚¬ìš© í†µê³„
                slots_stats = conn.execute("""
                    SELECT 
                        operation,
                        slot_allocation,
                        COUNT(*) as count,
                        AVG(response_time_ms) as avg_response_time,
                        AVG(CASE WHEN success THEN 1.0 ELSE 0.0 END) as success_rate,
                        AVG(content_length) as avg_content_length
                    FROM slots_metrics 
                    WHERE timestamp > ?
                    GROUP BY operation, slot_allocation
                """, (cutoff_date,)).fetchall()
                
                # ê²€ìƒ‰ ì„±ëŠ¥ ë¹„êµ
                search_comparison = conn.execute("""
                    SELECT 
                        slots_enabled,
                        COUNT(*) as search_count,
                        AVG(response_time_ms) as avg_response_time,
                        AVG(results_count) as avg_results_count,
                        AVG(slot_hits) as avg_slot_hits,
                        AVG(ltm_hits) as avg_ltm_hits,
                        AVG(CASE WHEN slot_hits > 0 THEN 1.0 ELSE 0.0 END) as slot_hit_rate
                    FROM search_comparison 
                    WHERE timestamp > ?
                    GROUP BY slots_enabled
                """, (cutoff_date,)).fetchall()
                
                # AI ì˜ë„ ë¶„ì„ ì •í™•ë„
                ai_accuracy = conn.execute("""
                    SELECT 
                        predicted_intent,
                        COUNT(*) as total_predictions,
                        AVG(CASE WHEN predicted_slot = actual_slot_used THEN 1.0 ELSE 0.0 END) as accuracy,
                        AVG(CASE WHEN user_feedback = 1 THEN 1.0 
                                 WHEN user_feedback = -1 THEN 0.0 
                                 ELSE NULL END) as user_satisfaction
                    FROM ai_intent_tracking 
                    WHERE timestamp > ?
                    GROUP BY predicted_intent
                """, (cutoff_date,)).fetchall()
                
                return {
                    "period_days": days,
                    "slots_usage_stats": [dict(zip([col[0] for col in slots_stats], row)) for row in slots_stats] if slots_stats else [],
                    "search_performance_comparison": [dict(zip([col[0] for col in search_comparison], row)) for row in search_comparison] if search_comparison else [],
                    "ai_intent_accuracy": [dict(zip([col[0] for col in ai_accuracy], row)) for row in ai_accuracy] if ai_accuracy else [],
                    "generated_at": datetime.now().isoformat()
                }
                
        except Exception as e:
            logger.error(f"Failed to generate slots performance report: {e}")
            return {
                "error": str(e),
                "period_days": days,
                "generated_at": datetime.now().isoformat()
            }

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    analytics = UsageAnalytics()
    
    print("âœ… UsageAnalytics module loaded successfully")
    print("ğŸ“Š Key features:")
    print("  - Event logging and session tracking")
    print("  - Quality metrics analysis")
    print("  - Performance monitoring")
    print("  - Usage pattern insights")
    print("  - Automatic data cleanup")
    
    # ê¸°ë³¸ í…ŒìŠ¤íŠ¸
    session_id = "test_session_001"
    analytics.start_session(session_id, "test_client", "unit_test")
    analytics.log_event("tool_usage", "add_memory", {"test": True}, 150, True, session_id=session_id)
    analytics.log_quality_metrics(100, 0.8, "good", 0.5, 0.6, False, 0.0, 2)
    
    stats = analytics.get_usage_statistics(1)
    print(f"\n[IMPROVE] Test statistics: {stats['basic_stats']['total_events']} events logged")
    
    analytics.end_session(session_id)
    print("ğŸ”š Test session completed")