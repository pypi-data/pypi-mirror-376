#!/usr/bin/env python3
"""
Intelligent Context Processor for Greeum

AI ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„, ì••ì¶•, ë° ìµœì í™”ë¥¼ ìˆ˜í–‰í•˜ëŠ” 
ì§€ëŠ¥í˜• ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

Author: Greeum Development Team  
Version: 2.6.4
"""

import re
import json
import logging
import hashlib
from typing import Dict, Any, List, Tuple, Optional, Set
from datetime import datetime, timedelta
from dataclasses import dataclass
from collections import Counter, defaultdict

from .context_backup import ContextBackupItem


@dataclass
class ContextAnalysisResult:
    """ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ê²°ê³¼"""
    content: str
    importance_score: float
    content_type: str
    key_concepts: List[str]
    redundancy_level: float
    compression_ratio: float
    semantic_density: float
    

@dataclass
class CompressionResult:
    """ì••ì¶• ê²°ê³¼"""
    original_content: str
    compressed_content: str
    compression_ratio: float
    preserved_concepts: List[str]
    removed_redundancy: List[str]
    quality_score: float


class IntelligentContextProcessor:
    """
    AI ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ ë° ìµœì í™” í”„ë¡œì„¸ì„œ
    
    ëŒ€ëŸ‰ì˜ ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ë¶„ì„í•˜ê³ ,
    ì¤‘ìš”ë„ì— ë”°ë¼ ì••ì¶• ë° ìš°ì„ ìˆœìœ„ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        
        # ë¶„ì„ ì„¤ì •
        self.min_importance_threshold = 0.3
        self.max_compression_ratio = 0.7
        self.semantic_density_target = 0.8
        
        # ì»¨í…ìŠ¤íŠ¸ íŒ¨í„´ ì •ì˜
        self.importance_patterns = {
            'error_patterns': [r'error|exception|fail|bug|issue', 0.9],
            'decision_patterns': [r'decision|choose|select|determine', 0.8], 
            'code_patterns': [r'def |class |function|import|const|let|var', 0.7],
            'user_intent': [r'ì‚¬ìš©ì|user|ìš”ì²­|request|ì§ˆë¬¸|question', 0.8],
            'result_patterns': [r'ê²°ê³¼|result|output|ì™„ë£Œ|complete', 0.7],
            'context_transition': [r'ë‹¤ìŒ|next|then|ì´í›„|after', 0.6]
        }
        
        # ì¤‘ë³µì„± íƒì§€ íŒ¨í„´
        self.redundancy_patterns = [
            r'ë°˜ë³µ|repeat|again|ë‹¤ì‹œ|same|ë™ì¼',
            r'ì´ë¯¸|already|previously|ì•ì„œ|before',
            r'ë§ˆì°¬ê°€ì§€|similarly|likewise|ê°™ì€|identical'
        ]
        
        # í•µì‹¬ ê°œë… ì¶”ì¶œ íŒ¨í„´
        self.concept_extractors = [
            r'(?:implement|êµ¬í˜„|ê°œë°œ)\s+(\w+)',
            r'(?:class|í´ë˜ìŠ¤)\s+(\w+)', 
            r'(?:function|í•¨ìˆ˜|ê¸°ëŠ¥)\s+(\w+)',
            r'(?:error|ì˜¤ë¥˜|ì—ëŸ¬)\s+(\w+)',
            r'(?:test|í…ŒìŠ¤íŠ¸|ê²€ì¦)\s+(\w+)'
        ]
        
        # ìºì‹œëœ ë¶„ì„ ê²°ê³¼
        self.analysis_cache: Dict[str, ContextAnalysisResult] = {}
        self.compression_cache: Dict[str, CompressionResult] = {}
        
    def analyze_context_importance(self, content: str) -> float:
        """
        ì»¨í…ìŠ¤íŠ¸ ë‚´ìš©ì˜ ì¤‘ìš”ë„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
        
        Args:
            content: ë¶„ì„í•  ì»¨í…ìŠ¤íŠ¸ ë‚´ìš©
            
        Returns:
            ì¤‘ìš”ë„ ìŠ¤ì½”ì–´ (0.0-1.0)
        """
        try:
            if not content or not content.strip():
                return 0.0
            
            # ìºì‹œ í™•ì¸
            content_hash = self._get_content_hash(content)
            if content_hash in self.analysis_cache:
                return self.analysis_cache[content_hash].importance_score
            
            importance_score = 0.5  # ê¸°ë³¸ê°’
            content_lower = content.lower()
            
            # íŒ¨í„´ ê¸°ë°˜ ì¤‘ìš”ë„ ë¶„ì„
            pattern_scores = []
            for pattern_name, (pattern, weight) in self.importance_patterns.items():
                matches = len(re.findall(pattern, content_lower, re.IGNORECASE))
                if matches > 0:
                    pattern_score = min(matches * weight * 0.1, weight)
                    pattern_scores.append(pattern_score)
                    self.logger.debug(f"Pattern {pattern_name}: {matches} matches, score: {pattern_score}")
            
            # íŒ¨í„´ ìŠ¤ì½”ì–´ë“¤ì˜ ê°€ì¤‘í‰ê· 
            if pattern_scores:
                importance_score += sum(pattern_scores) / len(pattern_scores)
            
            # ì»¨í…ì¸  ê¸¸ì´ ê¸°ë°˜ ì¡°ì •
            length_factor = self._calculate_length_importance(content)
            importance_score *= length_factor
            
            # êµ¬ì¡°ì  ë³µì¡ì„± í‰ê°€
            structural_complexity = self._evaluate_structural_complexity(content)
            importance_score += structural_complexity * 0.2
            
            # ì‹œë§¨í‹± ë°€ë„ í‰ê°€
            semantic_density = self._calculate_semantic_density(content)
            importance_score += semantic_density * 0.15
            
            # ìµœì¢… ìŠ¤ì½”ì–´ ì •ê·œí™”
            final_score = min(max(importance_score, 0.0), 1.0)
            
            # ê²°ê³¼ ìºì‹±
            analysis_result = ContextAnalysisResult(
                content=content,
                importance_score=final_score,
                content_type=self._classify_content_type(content),
                key_concepts=self._extract_key_concepts(content),
                redundancy_level=self._calculate_redundancy_level(content),
                compression_ratio=0.0,  # ì•„ì§ ì••ì¶•ë˜ì§€ ì•ŠìŒ
                semantic_density=semantic_density
            )
            self.analysis_cache[content_hash] = analysis_result
            
            return final_score
            
        except Exception as e:
            self.logger.error(f"Failed to analyze context importance: {e}")
            return 0.5  # ê¸°ë³¸ê°’ ë°˜í™˜
    
    def compress_redundant_context(self, contexts: List[str]) -> str:
        """
        ì¤‘ë³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì••ì¶•í•©ë‹ˆë‹¤.
        
        Args:
            contexts: ì••ì¶•í•  ì»¨í…ìŠ¤íŠ¸ ëª©ë¡
            
        Returns:
            ì••ì¶•ëœ ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸
        """
        try:
            if not contexts:
                return ""
            
            if len(contexts) == 1:
                return contexts[0]
            
            # ì „ì²´ ì»¨í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
            combined_content = "\n\n".join(contexts)
            content_hash = self._get_content_hash(combined_content)
            
            # ìºì‹œ í™•ì¸
            if content_hash in self.compression_cache:
                return self.compression_cache[content_hash].compressed_content
            
            # ì¤‘ë³µ ì œê±° ë° ì••ì¶• ìˆ˜í–‰
            compressed = self._perform_intelligent_compression(contexts)
            
            # ì••ì¶• ê²°ê³¼ ìºì‹±
            compression_result = CompressionResult(
                original_content=combined_content,
                compressed_content=compressed,
                compression_ratio=len(compressed) / max(len(combined_content), 1),
                preserved_concepts=self._extract_key_concepts(compressed),
                removed_redundancy=self._identify_removed_content(combined_content, compressed),
                quality_score=self._evaluate_compression_quality(combined_content, compressed)
            )
            self.compression_cache[content_hash] = compression_result
            
            return compressed
            
        except Exception as e:
            self.logger.error(f"Failed to compress redundant context: {e}")
            return "\n\n".join(contexts)  # ì••ì¶• ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
    
    def prioritize_context_segments(self, segments: List[str]) -> List[Tuple[str, float]]:
        """
        ì»¨í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ë“¤ì„ ìš°ì„ ìˆœìœ„ë¡œ ì •ë ¬í•©ë‹ˆë‹¤.
        
        Args:
            segments: ìš°ì„ ìˆœìœ„ë¥¼ ì§€ì •í•  ì„¸ê·¸ë¨¼íŠ¸ ëª©ë¡
            
        Returns:
            (ì„¸ê·¸ë¨¼íŠ¸, ìš°ì„ ìˆœìœ„_ìŠ¤ì½”ì–´) íŠœí”Œì˜ ì •ë ¬ëœ ëª©ë¡
        """
        try:
            prioritized_segments = []
            
            for segment in segments:
                # ê° ì„¸ê·¸ë¨¼íŠ¸ì˜ ì¤‘ìš”ë„ ë¶„ì„
                importance = self.analyze_context_importance(segment)
                
                # ì„¸ê·¸ë¨¼íŠ¸ë³„ ì¶”ê°€ ë¶„ì„
                segment_priority = self._calculate_segment_priority(segment, importance)
                
                prioritized_segments.append((segment, segment_priority))
            
            # ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì •ë ¬ (ë†’ì€ ìˆœ)
            sorted_segments = sorted(prioritized_segments, key=lambda x: x[1], reverse=True)
            
            self.logger.info(f"Prioritized {len(segments)} segments, top priority: {sorted_segments[0][1]:.3f}")
            return sorted_segments
            
        except Exception as e:
            self.logger.error(f"Failed to prioritize context segments: {e}")
            return [(segment, 0.5) for segment in segments]
    
    def optimize_context_flow(self, contexts: List[str]) -> List[str]:
        """
        ì»¨í…ìŠ¤íŠ¸ í”Œë¡œìš°ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.
        
        Args:
            contexts: ìµœì í™”í•  ì»¨í…ìŠ¤íŠ¸ ëª©ë¡
            
        Returns:
            ìµœì í™”ëœ ì»¨í…ìŠ¤íŠ¸ ëª©ë¡
        """
        try:
            if not contexts:
                return []
            
            # 1. ê° ì»¨í…ìŠ¤íŠ¸ ë¶„ì„
            analyzed_contexts = []
            for i, context in enumerate(contexts):
                importance = self.analyze_context_importance(context)
                concepts = self._extract_key_concepts(context)
                
                analyzed_contexts.append({
                    'index': i,
                    'content': context,
                    'importance': importance,
                    'concepts': concepts,
                    'length': len(context)
                })
            
            # 2. ì¤‘ìš”ë„ ê¸°ë°˜ í•„í„°ë§
            important_contexts = [
                ctx for ctx in analyzed_contexts 
                if ctx['importance'] >= self.min_importance_threshold
            ]
            
            # 3. ê°œë… ê¸°ë°˜ ì¤‘ë³µ ì œê±°
            deduplicated = self._remove_conceptual_duplicates(important_contexts)
            
            # 4. í”Œë¡œìš° ìµœì í™” (ë…¼ë¦¬ì  ìˆœì„œ ì¬ë°°ì¹˜)
            optimized_flow = self._optimize_logical_flow(deduplicated)
            
            # 5. ìµœì í™”ëœ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜
            result = [ctx['content'] for ctx in optimized_flow]
            
            self.logger.info(f"Optimized context flow: {len(contexts)} -> {len(result)} contexts")
            return result
            
        except Exception as e:
            self.logger.error(f"Failed to optimize context flow: {e}")
            return contexts
    
    def get_processing_statistics(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return {
            'analysis_cache_size': len(self.analysis_cache),
            'compression_cache_size': len(self.compression_cache),
            'average_importance_score': self._calculate_average_importance(),
            'average_compression_ratio': self._calculate_average_compression(),
            'most_common_content_types': self._get_common_content_types(),
            'processing_efficiency': self._calculate_processing_efficiency()
        }
    
    # ë‚´ë¶€ ë©”ì„œë“œë“¤
    
    def _get_content_hash(self, content: str) -> str:
        """ì»¨í…ì¸ ì˜ í•´ì‹œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def _calculate_length_importance(self, content: str) -> float:
        """ì»¨í…ì¸  ê¸¸ì´ ê¸°ë°˜ ì¤‘ìš”ë„ íŒ©í„°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        length = len(content)
        
        if length < 50:
            return 0.7  # ë„ˆë¬´ ì§§ìŒ
        elif length < 200:
            return 1.0  # ì ë‹¹í•¨
        elif length < 1000:
            return 0.95  # ì•½ê°„ ê¹€
        elif length < 5000:
            return 0.8   # ê¹€
        else:
            return 0.6   # ë„ˆë¬´ ê¹€
    
    def _evaluate_structural_complexity(self, content: str) -> float:
        """êµ¬ì¡°ì  ë³µì¡ì„±ì„ í‰ê°€í•©ë‹ˆë‹¤."""
        complexity_indicators = [
            len(re.findall(r'\n', content)) * 0.01,  # ì¤„ë°”ê¿ˆ ìˆ˜
            len(re.findall(r'```', content)) * 0.1,   # ì½”ë“œ ë¸”ë¡ ìˆ˜
            len(re.findall(r'[{}()\[\]]', content)) * 0.005,  # ê´„í˜¸ ìˆ˜
            len(re.findall(r'[.!?]', content)) * 0.02,  # ë¬¸ì¥ ìˆ˜
        ]
        
        return min(sum(complexity_indicators), 0.3)  # ìµœëŒ€ 0.3
    
    def _calculate_semantic_density(self, content: str) -> float:
        """ì‹œë§¨í‹± ë°€ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        if not content:
            return 0.0
        
        words = content.split()
        if not words:
            return 0.0
        
        # ì˜ë¯¸ìˆëŠ” ë‹¨ì–´ë“¤ (ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ ë“±)
        meaningful_words = [
            word for word in words 
            if len(word) > 2 and not word.lower() in ['the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for']
        ]
        
        # ê³ ìœ  ë‹¨ì–´ ë¹„ìœ¨
        unique_ratio = len(set(meaningful_words)) / max(len(meaningful_words), 1)
        
        # í‰ê·  ë‹¨ì–´ ê¸¸ì´
        avg_word_length = sum(len(word) for word in meaningful_words) / max(len(meaningful_words), 1)
        
        # ì‹œë§¨í‹± ë°€ë„ = ê³ ìœ ì„± * ë‹¨ì–´ê¸¸ì´íŒ©í„°
        density = unique_ratio * min(avg_word_length / 8.0, 1.0)
        
        return min(density, 1.0)
    
    def _classify_content_type(self, content: str) -> str:
        """ì»¨í…ì¸  íƒ€ì…ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
        content_lower = content.lower()
        
        type_patterns = {
            'code': [r'def |class |function|import|const|let|var|\{|\}|;$'],
            'error': [r'error|exception|fail|traceback|stack'],
            'dialogue': [r'ì‚¬ìš©ì:|user:|assistant:|ì§ˆë¬¸:|ë‹µë³€:'],
            'analysis': [r'ë¶„ì„|analysis|result|ê²°ê³¼|í‰ê°€|evaluation'],
            'instruction': [r'ì§€ì‹œ|instruction|command|ìš”ì²­|request']
        }
        
        for content_type, patterns in type_patterns.items():
            for pattern in patterns:
                if re.search(pattern, content_lower, re.IGNORECASE):
                    return content_type
        
        return 'general'
    
    def _extract_key_concepts(self, content: str) -> List[str]:
        """í•µì‹¬ ê°œë…ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        concepts = []
        
        for pattern in self.concept_extractors:
            matches = re.findall(pattern, content, re.IGNORECASE)
            concepts.extend(matches)
        
        # ë‹¨ì–´ ë¹ˆë„ ê¸°ë°˜ í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ
        words = re.findall(r'\b\w{3,}\b', content.lower())
        word_freq = Counter(words)
        frequent_words = [word for word, count in word_freq.most_common(10) if count > 1]
        
        concepts.extend(frequent_words)
        
        return list(set(concepts))[:20]  # ìµœëŒ€ 20ê°œ
    
    def _calculate_redundancy_level(self, content: str) -> float:
        """ì¤‘ë³µì„± ìˆ˜ì¤€ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
        redundancy_score = 0.0
        
        for pattern in self.redundancy_patterns:
            matches = len(re.findall(pattern, content.lower(), re.IGNORECASE))
            redundancy_score += matches * 0.1
        
        # ë°˜ë³µë˜ëŠ” ë¬¸ì¥ íŒ¨í„´ ê°ì§€
        sentences = re.split(r'[.!?]', content)
        if len(sentences) > 1:
            sentence_similarity = self._calculate_sentence_similarity(sentences)
            redundancy_score += sentence_similarity * 0.3
        
        return min(redundancy_score, 1.0)
    
    def _calculate_sentence_similarity(self, sentences: List[str]) -> float:
        """ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        if len(sentences) < 2:
            return 0.0
        
        similarities = []
        for i in range(len(sentences)):
            for j in range(i + 1, len(sentences)):
                sim = self._simple_similarity(sentences[i], sentences[j])
                similarities.append(sim)
        
        return sum(similarities) / len(similarities) if similarities else 0.0
    
    def _simple_similarity(self, text1: str, text2: str) -> float:
        """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1 & words2
        union = words1 | words2
        
        return len(intersection) / len(union) if union else 0.0
    
    def _perform_intelligent_compression(self, contexts: List[str]) -> str:
        """ì§€ëŠ¥í˜• ì••ì¶•ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        if not contexts:
            return ""
        
        # 1. ê° ì»¨í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        key_info_parts = []
        seen_concepts = set()
        
        for context in contexts:
            # ì¤‘ìš”ë„ ë¶„ì„
            importance = self.analyze_context_importance(context)
            
            if importance < self.min_importance_threshold:
                continue  # ì¤‘ìš”ë„ê°€ ë‚®ìœ¼ë©´ ì œì™¸
            
            # í•µì‹¬ ê°œë… ì¶”ì¶œ
            concepts = self._extract_key_concepts(context)
            new_concepts = [c for c in concepts if c not in seen_concepts]
            
            if new_concepts or importance > 0.7:  # ìƒˆë¡œìš´ ê°œë…ì´ ìˆê±°ë‚˜ ì¤‘ìš”ë„ê°€ ë†’ìœ¼ë©´ í¬í•¨
                compressed_context = self._compress_single_context(context, importance)
                key_info_parts.append(compressed_context)
                seen_concepts.update(concepts)
        
        # 2. ì••ì¶•ëœ ë¶€ë¶„ë“¤ì„ ì—°ê²°
        compressed_result = "\n\n".join(key_info_parts)
        
        # 3. ìµœì¢… ì •ë¦¬
        final_compressed = self._final_polish(compressed_result)
        
        return final_compressed
    
    def _compress_single_context(self, context: str, importance: float) -> str:
        """ë‹¨ì¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì••ì¶•í•©ë‹ˆë‹¤."""
        # ì¤‘ìš”ë„ì— ë”°ë¥¸ ì••ì¶• ë¹„ìœ¨ ê²°ì •
        if importance > 0.8:
            return context  # ê±°ì˜ ì••ì¶• ì•ˆí•¨
        elif importance > 0.6:
            return self._moderate_compression(context)
        else:
            return self._aggressive_compression(context)
    
    def _moderate_compression(self, context: str) -> str:
        """ì¤‘ê°„ ì •ë„ ì••ì¶•"""
        lines = context.split('\n')
        important_lines = []
        
        for line in lines:
            if any(re.search(pattern[0], line, re.IGNORECASE) 
                   for pattern in self.importance_patterns.values()):
                important_lines.append(line)
            elif len(line.strip()) > 20:  # ì˜ë¯¸ìˆëŠ” ê¸¸ì´ì˜ ì¤„ë§Œ ìœ ì§€
                important_lines.append(line[:100] + "..." if len(line) > 100 else line)
        
        return '\n'.join(important_lines)
    
    def _aggressive_compression(self, context: str) -> str:
        """ì ê·¹ì  ì••ì¶•"""
        concepts = self._extract_key_concepts(context)
        content_type = self._classify_content_type(context)
        
        summary = f"[{content_type}] {', '.join(concepts[:5])}"
        return summary
    
    def _final_polish(self, compressed: str) -> str:
        """ìµœì¢… ì •ë¦¬ ì‘ì—…"""
        # ë¹ˆ ì¤„ ì •ë¦¬
        cleaned = re.sub(r'\n\s*\n\s*\n', '\n\n', compressed)
        
        # ë„ˆë¬´ ê¸´ ì¤„ ì •ë¦¬
        lines = cleaned.split('\n')
        polished_lines = []
        
        for line in lines:
            if len(line) > 200:
                polished_lines.append(line[:197] + "...")
            else:
                polished_lines.append(line)
        
        return '\n'.join(polished_lines)
    
    def _identify_removed_content(self, original: str, compressed: str) -> List[str]:
        """ì œê±°ëœ ì»¨í…ì¸ ë¥¼ ì‹ë³„í•©ë‹ˆë‹¤."""
        original_lines = set(original.split('\n'))
        compressed_lines = set(compressed.split('\n'))
        removed_lines = original_lines - compressed_lines
        return list(removed_lines)[:10]  # ìµœëŒ€ 10ê°œ
    
    def _evaluate_compression_quality(self, original: str, compressed: str) -> float:
        """ì••ì¶• í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤."""
        if not original or not compressed:
            return 0.0
        
        # ì••ì¶• ë¹„ìœ¨
        compression_ratio = len(compressed) / len(original)
        
        # í•µì‹¬ ê°œë… ë³´ì¡´ìœ¨
        original_concepts = set(self._extract_key_concepts(original))
        compressed_concepts = set(self._extract_key_concepts(compressed))
        
        concept_preservation = len(compressed_concepts & original_concepts) / max(len(original_concepts), 1)
        
        # í’ˆì§ˆ ìŠ¤ì½”ì–´ = ê°œë…ë³´ì¡´ìœ¨ * (1 - ê³¼ì••ì¶•íŒ¨ë„í‹°)
        quality = concept_preservation * (2 - compression_ratio)
        
        return min(max(quality, 0.0), 1.0)
    
    def _calculate_segment_priority(self, segment: str, base_importance: float) -> float:
        """ì„¸ê·¸ë¨¼íŠ¸ë³„ ìš°ì„ ìˆœìœ„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        priority = base_importance
        
        # ê¸¸ì´ ê¸°ë°˜ ì¡°ì •
        length_factor = self._calculate_length_importance(segment)
        priority *= length_factor
        
        # ì»¨í…ì¸  íƒ€ì…ë³„ ê°€ì¤‘ì¹˜
        content_type = self._classify_content_type(segment)
        type_weights = {
            'error': 0.9,
            'code': 0.8, 
            'analysis': 0.7,
            'dialogue': 0.8,
            'instruction': 0.75,
            'general': 0.6
        }
        priority += type_weights.get(content_type, 0.6) * 0.2
        
        return min(priority, 1.0)
    
    def _remove_conceptual_duplicates(self, analyzed_contexts: List[Dict]) -> List[Dict]:
        """ê°œë…ì  ì¤‘ë³µì„ ì œê±°í•©ë‹ˆë‹¤."""
        unique_contexts = []
        seen_concept_sets = []
        
        for ctx in analyzed_contexts:
            ctx_concepts = set(ctx['concepts'])
            
            # ê¸°ì¡´ ì»¨í…ìŠ¤íŠ¸ë“¤ê³¼ ê°œë… ìœ ì‚¬ë„ ê²€ì‚¬
            is_duplicate = False
            for seen_concepts in seen_concept_sets:
                similarity = len(ctx_concepts & seen_concepts) / len(ctx_concepts | seen_concepts) if (ctx_concepts | seen_concepts) else 0
                if similarity > 0.7:  # 70% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                unique_contexts.append(ctx)
                seen_concept_sets.append(ctx_concepts)
        
        return unique_contexts
    
    def _optimize_logical_flow(self, contexts: List[Dict]) -> List[Dict]:
        """ë…¼ë¦¬ì  í”Œë¡œìš°ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤."""
        # ì»¨í…ì¸  íƒ€ì…ë³„ ê·¸ë£¹í™”
        type_groups = defaultdict(list)
        for ctx in contexts:
            content_type = self._classify_content_type(ctx['content'])
            type_groups[content_type].append(ctx)
        
        # ìµœì  ìˆœì„œ: instruction -> dialogue -> code -> analysis -> error -> general
        optimal_order = ['instruction', 'dialogue', 'code', 'analysis', 'error', 'general']
        
        optimized_flow = []
        for content_type in optimal_order:
            if content_type in type_groups:
                # ê° ê·¸ë£¹ ë‚´ì—ì„œëŠ” ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
                sorted_group = sorted(type_groups[content_type], key=lambda x: x['importance'], reverse=True)
                optimized_flow.extend(sorted_group)
        
        return optimized_flow
    
    def _calculate_average_importance(self) -> float:
        """í‰ê·  ì¤‘ìš”ë„ ìŠ¤ì½”ì–´ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        if not self.analysis_cache:
            return 0.0
        
        scores = [result.importance_score for result in self.analysis_cache.values()]
        return sum(scores) / len(scores)
    
    def _calculate_average_compression(self) -> float:
        """í‰ê·  ì••ì¶• ë¹„ìœ¨ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
        if not self.compression_cache:
            return 1.0
        
        ratios = [result.compression_ratio for result in self.compression_cache.values()]
        return sum(ratios) / len(ratios)
    
    def _get_common_content_types(self) -> Dict[str, int]:
        """ì¼ë°˜ì ì¸ ì»¨í…ì¸  íƒ€ì…ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        type_counts = Counter(result.content_type for result in self.analysis_cache.values())
        return dict(type_counts.most_common(5))
    
    def _calculate_processing_efficiency(self) -> float:
        """ì²˜ë¦¬ íš¨ìœ¨ì„±ì„ ê³„ì‚°í•©ë‹ˆë‹¤."""
        if not self.analysis_cache:
            return 0.0
        
        # ìºì‹œ íˆíŠ¸ìœ¨ê³¼ í‰ê·  í’ˆì§ˆ ìŠ¤ì½”ì–´ë¡œ íš¨ìœ¨ì„± ì¸¡ì •
        cache_efficiency = min(len(self.analysis_cache) / 100, 1.0)  # ìºì‹œ í™œìš©ë„
        avg_quality = self._calculate_average_importance()
        
        return (cache_efficiency + avg_quality) / 2


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_context_processor() -> IntelligentContextProcessor:
    """Intelligent Context Processor ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    return IntelligentContextProcessor()


def quick_context_analysis(content: str) -> Dict[str, Any]:
    """ë¹ ë¥¸ ì»¨í…ìŠ¤íŠ¸ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    processor = create_context_processor()
    
    importance = processor.analyze_context_importance(content)
    concepts = processor._extract_key_concepts(content)
    content_type = processor._classify_content_type(content)
    
    return {
        'importance_score': importance,
        'key_concepts': concepts,
        'content_type': content_type,
        'length': len(content),
        'semantic_density': processor._calculate_semantic_density(content)
    }


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš© ì‹¤í–‰
    logging.basicConfig(level=logging.INFO)
    
    print("[MEMORY] Intelligent Context Processor í…ŒìŠ¤íŠ¸")
    
    # í…ŒìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸ë“¤
    test_contexts = [
        "ì‚¬ìš©ìê°€ ìƒˆë¡œìš´ ê¸°ëŠ¥ êµ¬í˜„ì„ ìš”ì²­í–ˆìŠµë‹ˆë‹¤. ì¤‘ìš”í•œ ê²°ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.",
        "def process_data(data): return data.transform()",
        "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: TypeError in line 42",
        "ë¶„ì„ ê²°ê³¼: ì„±ëŠ¥ì´ 20% í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤.",
        "ê°™ì€ ë‚´ìš©ì„ ë‹¤ì‹œ ë°˜ë³µí•´ì„œ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
    ]
    
    processor = create_context_processor()
    
    # ì¤‘ìš”ë„ ë¶„ì„ í…ŒìŠ¤íŠ¸
    print("\nğŸ“Š ì¤‘ìš”ë„ ë¶„ì„:")
    for i, context in enumerate(test_contexts):
        importance = processor.analyze_context_importance(context)
        print(f"  {i+1}. {context[:50]}... â†’ {importance:.3f}")
    
    # ì••ì¶• í…ŒìŠ¤íŠ¸
    print("\nğŸ—œï¸  ì••ì¶• í…ŒìŠ¤íŠ¸:")
    compressed = processor.compress_redundant_context(test_contexts)
    print(f"  ì›ë³¸: {sum(len(c) for c in test_contexts)} ë¬¸ì")
    print(f"  ì••ì¶•: {len(compressed)} ë¬¸ì")
    print(f"  ì••ì¶•ë¥ : {len(compressed) / sum(len(c) for c in test_contexts) * 100:.1f}%")
    
    # ìš°ì„ ìˆœìœ„ í…ŒìŠ¤íŠ¸
    print("\nğŸ¯ ìš°ì„ ìˆœìœ„ í…ŒìŠ¤íŠ¸:")
    prioritized = processor.prioritize_context_segments(test_contexts)
    for i, (segment, priority) in enumerate(prioritized[:3]):
        print(f"  {i+1}. {segment[:50]}... â†’ {priority:.3f}")
    
    print("\nâœ… Intelligent Context Processor ì¤€ë¹„ ì™„ë£Œ")